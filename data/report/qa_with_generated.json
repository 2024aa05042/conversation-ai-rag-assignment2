[
  {
    "question": "What is artificial intelligence (AI)?",
    "answer": "The capability of computational systems to perform tasks associated with human intelligence such as learning, reasoning, problem-solving, perception, and decision-making.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "The capability of computational systems to perform tasks typically associated with human intelligence",
    "response_time": "11.47s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_43",
        "text": "Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by...",
        "full_text": "Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. See also\nArtificial consciousness – Field in cognitive science\nArtificial intelligence and elections – Impact of AI on political elections\nArtificial intelligence content detection – Software to detect AI-generated content\nArtificial intelligence in Wikimedia projects – Use of artificial intelligence to develop Wikipedia and other Wikimedia projects\nAssociation for the Advancement of Artificial Intelligence (AAAI)\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\nBusiness process automation – Automation of business processes\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\nComputational intelligence – Ability of a computer to learn a specific task from data or experimental observation\nDARWIN EU – A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real world evidence (RWE) to support the evaluation and supervision of medicines across the EU\nDigital immortality – Hypothetical concept of storing a personality in digital form\nEmergent algorithm – Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence – List of concepts in artificial intelligence\nIntelligence amplification – Use of information technology to augment human intelligence\nIntelligent agent – Software agent which acts autonomously\nIntelligent automation – Software process that combines robotic process automation and artificial intelligence\nList of artificial intelligence books\nList of artificial intelligence journals\nList of artificial intelligence projects\nMind uploading – Hypothetical process of digitally emulating a brain\nOrganoid intelligence – Use of brain cells and brain organoids for intelligent computing\nPseudorandomness – Appearing random but actually being generated by a deterministic, causal process\nRobotic process automation – Form of business process automation technology\nThe Last Day – 1967 Welsh science fiction novel\nWetware computer – Computer composed of organic material\n\nExplanatory notes\nReferences\nTextbooks\nHistory of AI\nOther sources\nExternal links\n\nHauser, Larry. \"Artificial Intelligence\". In Fieser, James; Dowden, Bradley (eds.). Internet Encyclopedia of Philosophy. ISSN 2161-0002. OCLC 37741658.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 4,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_0",
        "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...",
        "full_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters.",
        "rrf_score": 0.031099324975891997,
        "dense_rank": 1,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_39",
        "text": "Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI\nAI researchers are divide...",
        "full_text": "Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. Machine consciousness, sentience, and mind\nThere is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction. Consciousness\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain.",
        "rrf_score": 0.02967032967032967,
        "dense_rank": 5,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_37",
        "text": "Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence...",
        "full_text": "Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal, political and academic discourses. Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\". There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text. Evaluating approaches to AI\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI and its limits\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "In which year was AI founded as an academic discipline?",
    "answer": "1956.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "1956",
    "response_time": "4.39s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.030798389007344232,
        "dense_rank": 3,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_0",
        "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...",
        "full_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters.",
        "rrf_score": 0.03057889822595705,
        "dense_rank": 8,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_1",
        "text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of ...",
        "full_text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. Reasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. Knowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_33",
        "text": "The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press ...",
        "full_text": "The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s. Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed. In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_35",
        "text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In ...",
        "full_text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. Philosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "What are AI winters?",
    "answer": "Periods of disappointment and loss of funding in AI research.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "periods of disappointment and loss of funding",
    "response_time": "5.67s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_0",
        "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...",
        "full_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_1",
        "text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of ...",
        "full_text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. Reasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. Knowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program.",
        "rrf_score": 0.031746031746031744,
        "dense_rank": 3,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_14",
        "text": "Military\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting i...",
        "full_text": "Military\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous. AI has been used in military operations in Iraq, Syria, Israel and Ukraine. Generative AI\nAgents\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. Web search\nMicrosoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search. For safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content. Google officially pushed its AI Search at its Google I/O event on 20 May 2025. It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.015625,
        "dense_rank": 0,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "Which hardware greatly boosted AI after 2012?",
    "answer": "Graphics processing units (GPUs).",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "unanswerable",
    "response_time": "5.71s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_general_intelligence_9",
        "text": "Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances...",
        "full_text": "Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs. In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert. In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave. In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27. In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.029631255487269532,
        "dense_rank": 8,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_35",
        "text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In ...",
        "full_text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. Philosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Machine_learning_25",
        "text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financi...",
        "full_text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financial incentives\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. Hardware\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. Tensor Processing Units (TPUs)\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_general_intelligence_5",
        "text": "Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI. However, in the early 1970s, it became obvious that researc...",
        "full_text": "Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI. However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\". Narrow AI research\nIn the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years. At the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than halfway, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "What architecture accelerated AI progress after 2017?",
    "answer": "The transformer architecture.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "the transformer architecture",
    "response_time": "5.19s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_1",
        "text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of ...",
        "full_text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. Reasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. Knowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_9",
        "text": "Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances...",
        "full_text": "Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs. In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert. In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave. In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27. In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 1,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.031054405392392875,
        "dense_rank": 7,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.030309988518943745,
        "dense_rank": 5,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_intelligence_35",
        "text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In ...",
        "full_text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. Philosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "What is supervised learning?",
    "answer": "Learning using labeled training data with expected answers.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired output",
    "response_time": "8.71s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_6",
        "text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm...",
        "full_text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm works for all problems. Supervised learning\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_8",
        "text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the...",
        "full_text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Other types\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. Self-learning\nSelf-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.",
        "rrf_score": 0.03125,
        "dense_rank": 4,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_intelligence_4",
        "text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other gui...",
        "full_text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.",
        "rrf_score": 0.031054405392392875,
        "dense_rank": 2,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Machine_learning_10",
        "text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervi...",
        "full_text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary.",
        "rrf_score": 0.030798389007344232,
        "dense_rank": 7,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_3",
        "text": "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of...",
        "full_text": "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. Data compression\nData mining\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). Generalization\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.",
        "rrf_score": 0.02964426877470356,
        "dense_rank": 9,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "What is unsupervised learning?",
    "answer": "Finding patterns in data without labeled guidance.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities and react based on the presence or absence of such commonalities",
    "response_time": "11.93s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_7",
        "text": "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in...",
        "full_text": "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself. Dimensionality reduction\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation. Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data).",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_10",
        "text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervi...",
        "full_text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary.",
        "rrf_score": 0.03125763125763126,
        "dense_rank": 5,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_intelligence_4",
        "text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other gui...",
        "full_text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.",
        "rrf_score": 0.03036576949620428,
        "dense_rank": 3,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Machine_learning_1",
        "text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early...",
        "full_text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think? \", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\". Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms. Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Learning_0",
        "text": "Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, non-human animals, and some machi...",
        "full_text": "Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, non-human animals, and some machines; there is also evidence for some kind of learning in certain plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulate from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be \"lost\" from that which cannot be retrieved. Human learning starts at birth (it might even start before) and continues until death as a consequence of ongoing interactions between people and their environment. The nature and processes involved in learning are studied in many established fields (including educational psychology, neuropsychology, experimental psychology, cognitive sciences, and pedagogy), as well as emerging fields of knowledge (e.g. with a shared interest in the topic of learning from safety events such as incidents/accidents, or in collaborative learning health systems). Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event cannot be avoided or escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development. Play has been approached by several theorists as a form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Learning",
        "title": "Learning"
      }
    ]
  },
  {
    "question": "What is reinforcement learning based on?",
    "answer": "Rewarding good responses and punishing bad ones.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "rewarded for good responses and punished for bad ones",
    "response_time": "6.47s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_4",
        "text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other gui...",
        "full_text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.",
        "rrf_score": 0.03149801587301587,
        "dense_rank": 3,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Learning_3",
        "text": "Active learning is a key characteristic of student-centered learning. Conversely, passive learning and direct instruction are characteristics of teacher-centered learning (or traditional education). A...",
        "full_text": "Active learning is a key characteristic of student-centered learning. Conversely, passive learning and direct instruction are characteristics of teacher-centered learning (or traditional education). Associative learning\nAssociative learning is the process by which a person or animal learns an association between two stimuli or events. In classical conditioning, a previously neutral stimulus is repeatedly paired with a reflex-eliciting stimulus until eventually the neutral stimulus elicits a response on its own. In operant conditioning, a behavior that is reinforced or punished in the presence of a stimulus becomes more or less likely to occur in the presence of that stimulus. Operant conditioning\nOperant conditioning is a way in which behavior can be shaped or modified according to the desires of the trainer or head individual. Operant conditioning uses the thought that living things seek pleasure and avoid pain, and that an animal or human can learn through receiving either reward or punishment at a specific time called trace conditioning. Trace conditioning is the small and ideal period of time between the subject performing the desired behavior, and receiving the positive reinforcement as a result of their performance. The reward needs to be given immediately after the completion of the wanted behavior. Operant conditioning is different from classical conditioning in that it shapes behavior not solely on bodily reflexes that occur naturally to a specific stimulus, but rather focuses on the shaping of wanted behavior that requires conscious thought, and ultimately requires learning. Punishment and reinforcement are the two principal ways in which operant conditioning occurs. Punishment is used to reduce unwanted behavior, and ultimately (from the learner's perspective) leads to avoidance of the punishment, not necessarily avoidance of the unwanted behavior. Punishment is not an appropriate way to increase wanted behavior for animals or humans. Punishment can be divided into two subcategories, positive punishment and negative punishment. Positive punishment is when an aversive aspect of life or thing is added to the subject, for this reason it is called positive punishment. For example, the parent spanking their child would be considered a positive punishment, because a spanking was added to the child. Negative punishment is considered the removal of something loved or desirable from the subject.",
        "rrf_score": 0.02919863597612958,
        "dense_rank": 9,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Learning",
        "title": "Learning"
      },
      {
        "chunk_id": "Artificial_intelligence_3",
        "text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent m...",
        "full_text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. Learning\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Machine_learning_1",
        "text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early...",
        "full_text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think? \", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\". Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms. Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      }
    ]
  },
  {
    "question": "What are hallucinations in GPT models?",
    "answer": "Falsehoods generated by the model.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "generating falsehoods called \"hallucinations",
    "response_time": "6.18s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Religion_13",
        "text": "Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities. Hallucinations and delusion...",
        "full_text": "Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities. Hallucinations and delusions related to religious content occurs in about 60% of people with schizophrenia. While this number varies across cultures, this had led to theories about a number of influential religious phenomena and possible relation to psychotic disorders. A number of prophetic experiences are consistent with psychotic symptoms, although retrospective diagnoses are practically impossible. Schizophrenic episodes are also experienced by people who do not have belief in gods. Religious content is also common in temporal lobe epilepsy, and obsessive-compulsive disorder. Atheistic content is also found to be common with temporal lobe epilepsy. Comparativism\nComparative religion is the branch of the study of religions concerned with the systematic comparison of the doctrines and practices of the world's religions. In general, the comparative study of religion yields a deeper understanding of the fundamental philosophical concerns of religion such as ethics, metaphysics, and the nature and form of salvation. Studying such material is meant to give one a richer and more sophisticated understanding of human beliefs and practices regarding the sacred, numinous, spiritual and divine. In the field of comparative religion, a common geographical classification of the main world religions includes Middle Eastern religions (including Zoroastrianism and Iranian religions), Indian religions, East Asian religions, African religions, American religions, Oceanic religions, and classical Hellenistic religions. Classification\nIn the 19th and 20th centuries, the academic practice of comparative religion divided religious belief into philosophically defined categories called world religions. Some academics studying the subject have divided religions into three broad categories:\n\nWorld religions, a term which refers to transcultural, international religions;\nIndigenous religions, which refers to smaller, culture-specific or nation-specific religious groups; and\nNew religious movements, which refers to recently developed religions.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Religion",
        "title": "Religion"
      },
      {
        "chunk_id": "Artificial_intelligence_12",
        "text": "Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question ans...",
        "full_text": "Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions. Mathematics\nLarge language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Sigmund_Freud_8",
        "text": "Described as Anna O., she was invited to talk about her symptoms while under hypnosis (she would coin the phrase \"talking cure\"). Her symptoms became reduced in severity as she retrieved memories of t...",
        "full_text": "Described as Anna O., she was invited to talk about her symptoms while under hypnosis (she would coin the phrase \"talking cure\"). Her symptoms became reduced in severity as she retrieved memories of traumatic incidents associated with their onset. The inconsistent results of Freud's early clinical work eventually led him to abandon hypnosis, having concluded that more consistent and effective symptom relief could be achieved by encouraging patients to talk freely, without censorship or inhibition, about whatever ideas or memories occurred to them. He called this procedure \"free association\". In conjunction with this, Freud found that patients' dreams could be fruitfully analyzed to reveal the complex structuring of unconscious material and to demonstrate the psychic action of repression which, he had concluded, underlay symptom formation. By 1896, he was using the term \"psychoanalysis\" to refer to his new clinical method and the theories on which it was based. Freud's development of these new theories took place during a period in which he experienced heart irregularities, disturbing dreams, and periods of depression — a \"neurasthenia\" that he linked to the death of his father in 1896 and that prompted a \"self-analysis\" of his own dreams and memories of childhood. His explorations of his feelings of hostility to his father and rivalrous jealousy over his mother's affections led him to fundamentally revise his theory of the origin of the neuroses. Based on his early clinical work, Freud postulated that unconscious memories of sexual molestation in early childhood were a necessary precondition for psychoneuroses (hysteria and obsessional neurosis), a formulation now known as Freud's seduction theory. In the light of his self-analysis, Freud abandoned the theory that every neurosis can be traced back to the effects of infantile sexual abuse, now arguing that infantile sexual scenarios still had a causative function. It did not matter whether they were real or imagined, and in either case, they became pathogenic only when acting as repressed memories.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Sigmund_Freud",
        "title": "Sigmund Freud"
      },
      {
        "chunk_id": "Artificial_intelligence_4",
        "text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other gui...",
        "full_text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "What technique reduces hallucinations?",
    "answer": "Reinforcement learning from human feedback (RLHF).",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "unanswerable",
    "response_time": "5.65s",
    "fused_chunks": [
      {
        "chunk_id": "Psychoanalysis_31",
        "text": "These have sometimes been called interpersonal, intersubjective (cf. Stolorow), relational, or corrective object relations techniques. Ego psychological concepts of deficit in functioning led to refin...",
        "full_text": "These have sometimes been called interpersonal, intersubjective (cf. Stolorow), relational, or corrective object relations techniques. Ego psychological concepts of deficit in functioning led to refinements in supportive therapy. These techniques are particularly applicable to psychotic and near-psychotic (cf. Eric Marcus, \"Psychosis and Near-psychosis\") patients. These supportive therapy techniques include discussions of reality; encouragement to stay alive (including hospitalization); psychotropic medicines to relieve overwhelming depressive affect or overwhelming fantasies (hallucinations and delusions); and advice about the meanings of things (to counter abstraction failures). The notion of the \"silent analyst\" has been criticized. Actually, the analyst listens using Arlow's approach as set out in \"The Genesis of Interpretation\", using active intervention to interpret resistances, defenses, create pathology, and fantasies. Silence is not a technique of psychoanalysis (see also the studies and opinion papers of Owen Renik). \"Analytic neutrality\" is a concept that does not mean the analyst is silent. It refers to the analyst's position of not taking sides in the internal struggles of the patient. For example, if a patient feels guilty, the analyst might explore what the patient has been doing or thinking that causes the guilt, but not reassure the patient not to feel guilty. The analyst might also explore the identifications with parents and others that led to the guilt. Interpersonal–relational psychoanalysts emphasize the notion that it is impossible to be neutral. Sullivan introduced the term participant-observer to indicate that the analyst inevitably interacts with the analysand, and suggested the detailed inquiry as an alternative to interpretation. The detailed inquiry involves noting where the analysand is leaving out important elements of an account and noting when the story is obfuscated, and asking careful questions to open up the dialogue. Group therapy and play therapy\nAlthough single-client sessions remain the norm, psychoanalytic theory has been used to develop other types of psychological treatment. Psychoanalytic group therapy was pioneered by Trigant Burrow, Joseph Pratt, Paul F. Schilder, Samuel R. Slavson, Harry Stack Sullivan, and Wolfe.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 3,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Psychoanalysis",
        "title": "Psychoanalysis"
      },
      {
        "chunk_id": "Psychoanalysis_30",
        "text": "When the patient reclines on a couch with the analyst out of view, the patient tends to remember more experiences, more resistance and transference, and is able to reorganize thoughts after the develo...",
        "full_text": "When the patient reclines on a couch with the analyst out of view, the patient tends to remember more experiences, more resistance and transference, and is able to reorganize thoughts after the development of insight through the interpretive work of the analyst. Although fantasy life can be understood through the examination of dreams, masturbation fantasies are also important. The analyst is interested in how the patient reacts to and avoids such fantasies. Various memories of early life are generally distorted—what Freud called screen memories—and in any case, very early experiences (before age two)—cannot be remembered. Variations in technique\nThere is what is known among psychoanalysts as classical technique, although Freud, throughout his writings, deviated from this considerably, depending on the problems of any given patient. Classical technique was summarized by Allan Compton as comprising:\n\nInstructions: telling the patient to try to say what's on their mind, including interferences;\nExploration: asking questions; and\nClarification: rephrasing and summarizing what the patient has been describing. As well, the analyst can also use confrontation to bring an aspect of functioning, usually a defense, to the patient's attention. The analyst then uses a variety of interpretation methods, such as:\n\nDynamic interpretation: explaining how being too nice guards against guilt (e.g. defense vs. affect);\nGenetic interpretation: explaining how a past event is influencing the present;\nResistance interpretation: showing the patient how they are avoiding their problems;\nTransference interpretation: showing the patient ways old conflicts arise in current relationships, including that with the analyst; or\nDream interpretation: obtaining the patient's thoughts about their dreams and connecting this with their current problems. Analysts can also use reconstruction to estimate what may have happened in the past that created some current issue. These techniques are primarily based on conflict theory. As object relations theory evolved, supplemented by the work of John Bowlby and Mary Ainsworth, techniques with patients who had more severe problems with basic trust (Erikson, 1950) and a history of maternal deprivation (see the works of Augusta Alpert) led to new techniques with adults.",
        "rrf_score": 0.029418126757516764,
        "dense_rank": 9,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Psychoanalysis",
        "title": "Psychoanalysis"
      },
      {
        "chunk_id": "Psychoanalysis_9",
        "text": "All attempts to develop a suitable neuronal treatment failed; in fact, the detailed examinations did not reveal any organic defects. In the monograph written on this cases, Freud documented his differ...",
        "full_text": "All attempts to develop a suitable neuronal treatment failed; in fact, the detailed examinations did not reveal any organic defects. In the monograph written on this cases, Freud documented his differential-diagnostically supported suspicion that neurotic symptoms probably would have psychological causes. Finishing the ineffective hypnosis, the idea of psychoanalysis began to receive serious attention; Freud initially called it free association. His first attempt to explain neurotical symptoms on this path was presented in Studies on Hysteria (1895). Co-authored with Josef Breuer, this is generally seen as the birth of psychoanalysis. The work based on their partly joint treatment of Bertha Pappenheim, referred to by the pseudonym \"Anna O.\" Bertha herself had dubbed the treatment talking cure. Breuer, a distinguished physician, was astonished but remained unspecific; while Freud formulated his hypothesis that Anna's hystera seemed to be caused by distressing but unconscious experiences related to sexuality, basing his assumption on corresponding free associations made by the young women. For example, she sometimes liked to jokingly rename her talking cure as chimney sweeping, an association about the fairy tale through which part of a pregnant woman's house 'the stork' gives birth to the baby – or in Lacan's words: \"The more Anna provided signifers, the more she chattered on, the better it went.\" See also Jokes and Their Relation to the Unconscious (1905). Around the same time, Freud had started to develop a neurological hypothesis about mental phenomena such as memory, but soon abandoned this attempt and left it unpublished. Insights into neuronal-biochemical processes that store experiences in the brain – like engraving the proverbial tabula rasa with some code – belongs to the physiological branch of science and lead in a different direction of research than the psychological question of what the differences between consciousness and unconsciousness are. After some thought about a suitable term, Freud called his new instrument and field of research psychoanalysis, introduced in his essay \"Inheritance and Etiology of Neuroses\", written in 1896.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Psychoanalysis",
        "title": "Psychoanalysis"
      },
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Sigmund_Freud_8",
        "text": "Described as Anna O., she was invited to talk about her symptoms while under hypnosis (she would coin the phrase \"talking cure\"). Her symptoms became reduced in severity as she retrieved memories of t...",
        "full_text": "Described as Anna O., she was invited to talk about her symptoms while under hypnosis (she would coin the phrase \"talking cure\"). Her symptoms became reduced in severity as she retrieved memories of traumatic incidents associated with their onset. The inconsistent results of Freud's early clinical work eventually led him to abandon hypnosis, having concluded that more consistent and effective symptom relief could be achieved by encouraging patients to talk freely, without censorship or inhibition, about whatever ideas or memories occurred to them. He called this procedure \"free association\". In conjunction with this, Freud found that patients' dreams could be fruitfully analyzed to reveal the complex structuring of unconscious material and to demonstrate the psychic action of repression which, he had concluded, underlay symptom formation. By 1896, he was using the term \"psychoanalysis\" to refer to his new clinical method and the theories on which it was based. Freud's development of these new theories took place during a period in which he experienced heart irregularities, disturbing dreams, and periods of depression — a \"neurasthenia\" that he linked to the death of his father in 1896 and that prompted a \"self-analysis\" of his own dreams and memories of childhood. His explorations of his feelings of hostility to his father and rivalrous jealousy over his mother's affections led him to fundamentally revise his theory of the origin of the neuroses. Based on his early clinical work, Freud postulated that unconscious memories of sexual molestation in early childhood were a necessary precondition for psychoneuroses (hysteria and obsessional neurosis), a formulation now known as Freud's seduction theory. In the light of his self-analysis, Freud abandoned the theory that every neurosis can be traced back to the effects of infantile sexual abuse, now arguing that infantile sexual scenarios still had a causative function. It did not matter whether they were real or imagined, and in either case, they became pathogenic only when acting as repressed memories.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Sigmund_Freud",
        "title": "Sigmund Freud"
      }
    ]
  },
  {
    "question": "Which is older as a discipline: AI or machine learning?",
    "answer": "AI; machine learning grew out of AI.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "comparative",
    "generated_answer": "machine learning",
    "response_time": "4.78s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_37",
        "text": "Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence...",
        "full_text": "Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal, political and academic discourses. Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\". There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text. Evaluating approaches to AI\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI and its limits\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning.",
        "rrf_score": 0.03131881575727918,
        "dense_rank": 1,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.030886196246139225,
        "dense_rank": 9,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Machine_learning_2",
        "text": "Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and associati...",
        "full_text": "Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule. Current Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods. In 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques. Relationships to other fields\nArtificial intelligence\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines, including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature.",
        "rrf_score": 0.0304147465437788,
        "dense_rank": 2,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_21",
        "text": "Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine ex...",
        "full_text": "Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes. In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users. Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research itself. Explainability\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. Overfitting\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is. Other limitations and vulnerabilities\nLearners can also be disappointed by \"learning the wrong lesson\".",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_3",
        "text": "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of...",
        "full_text": "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. Data compression\nData mining\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). Generalization\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "Are GPUs or CPUs dominant for large-scale ML training today?",
    "answer": "GPUs.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "comparative",
    "generated_answer": "graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale (commercial and academic) machine learning",
    "response_time": "10.05s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_25",
        "text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financi...",
        "full_text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financial incentives\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. Hardware\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. Tensor Processing Units (TPUs)\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Deep_learning_9",
        "text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision ...",
        "full_text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. Deep learning revolution\nThe deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning. A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "rrf_score": 0.03177805800756621,
        "dense_rank": 1,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_14",
        "text": "This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that...",
        "full_text": "This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting. DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations. Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. Hardware\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months. Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms.",
        "rrf_score": 0.030798389007344232,
        "dense_rank": 7,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_35",
        "text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In ...",
        "full_text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. Philosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "Is domestic or international trade typically more costly?",
    "answer": "International trade.",
    "source_id": "International trade",
    "url": "https://en.wikipedia.org/wiki/International_trade",
    "category": "comparative",
    "generated_answer": "international trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally",
    "response_time": "9.63s",
    "fused_chunks": [
      {
        "chunk_id": "International_trade_1",
        "text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally re...",
        "full_text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not. However, in practical terms, carrying out trade at an international level is typically a more complex process than domestic trade. The main difference is that international trade is typically more costly than domestic trade. This is due to the fact that cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural differences, product safety, the legal system, and so on. Another difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries. Thus, international trade is mostly restricted to trade in goods and services, and only to a lesser extent to trade in capital, labour, or other factors of production. Trade in goods and services can serve as a substitute for trade in factors of production. Instead of importing a factor of production, a country can import goods that make intensive use of that factor of production and thus embody it. An example of this is the import of labor-intensive goods by the United States from China. Instead of importing Chinese labor, the United States imports goods that were produced with Chinese labor. One report in 2010, suggested that international trade was increased when a country hosted a network of immigrants, but the trade effect was weakened when the immigrants became assimilated into their new country. History\nThe history of international trade chronicles notable events that have affected trading among various economies. Theories and models\nThere are several models that seek to explain the factors behind international trade, the welfare consequences of trade and the pattern of trade. Most traded export products\nLargest countries or regions by total international trade\nThe following table is a list of the 30 largest trading states according to the World Trade Organization in 2024.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "International_trade_0",
        "text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade repr...",
        "full_text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade represents a significant share of gross domestic product (GDP). While international trade has existed throughout history (for example Uttarapatha, Silk Road, Amber Road, salt roads), its economic, social, and political importance has been on the rise in recent centuries. Carrying out trade at an international level is a complex process when compared to domestic trade. When trade takes place between two or more states, factors like currency, government policies, economy, judicial system, laws, and markets influence trade. To ease and justify the process of trade between countries of different economic standing in the modern era, some international economic organizations were formed, such as the World Trade Organization. These organizations work towards the facilitation and growth of international trade. Statistical services of intergovernmental and supranational organizations and governmental statistical agencies publish official statistics on international trade. Characteristics of global trade\nA product that is transferred or sold from a party in one country to a party in another country is an export from the originating country, and an import to the country receiving that product. Imports and exports are accounted for in a country's current account in the balance of payments. Trading globally may give consumers and countries the opportunity to be exposed to new markets and  products. Almost every kind of product can be found in the international market, for example: food, clothes, spare parts, oil, jewellery, wine, stocks, currencies, and water. Services are also traded, such as in tourism, banking, consulting, and transportation. Advanced technology (including transportation), globalization, industrialization, outsourcing and multinational corporations have major impacts on the international trade systems. Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not.",
        "rrf_score": 0.03225806451612903,
        "dense_rank": 2,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "Monetary_policy_20",
        "text": "This view rests on two implicit assumptions: a high responsiveness of import prices to the exchange rate, i.e. producer currency pricing (PCP), and frictionless international financial markets support...",
        "full_text": "This view rests on two implicit assumptions: a high responsiveness of import prices to the exchange rate, i.e. producer currency pricing (PCP), and frictionless international financial markets supporting the efficiency of flexible price allocation. The violation or distortion of these assumptions found in empirical research is the subject of a substantial part of the international optimal monetary policy literature. The policy trade-offs specific to this international perspective are threefold:\nFirst, research suggests only a weak reflection of exchange rate movements in import prices, lending credibility to the opposed theory of local currency pricing (LCP). The consequence is a departure from the classical view in the form of a trade-off between output gaps and misalignments in international relative prices, shifting monetary policy to CPI inflation control and real exchange rate stabilization. Second, another specificity of international optimal monetary policy is the issue of strategic interactions and competitive devaluations, which is due to cross-border spillovers in quantities and prices. Therein, the national authorities of different countries face incentives to manipulate the terms of trade to increase national welfare in the absence of international policy coordination. Even though the gains of international policy coordination might be small, such gains may become very relevant if balanced against incentives for international noncooperation. Third, open economies face policy trade-offs if asset market distortions prevent global efficient allocation. Even though the real exchange rate absorbs shocks in current and expected fundamentals, its adjustment does not necessarily result in a desirable allocation and may even exacerbate the misallocation of consumption and employment at both the domestic and global level. This is because, relative to the case of complete markets, both the Phillips curve and the loss function include a welfare-relevant measure of cross-country imbalances. Consequently, this results in domestic goals, e.g. output gaps or inflation, being traded-off against the stabilization of external variables such as the terms of trade or the demand gap. Hence, the optimal monetary policy in this case consists of redressing demand imbalances and/or correcting international relative prices at the cost of some inflation.",
        "rrf_score": 0.031746031746031744,
        "dense_rank": 3,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Monetary_policy",
        "title": "Monetary policy"
      },
      {
        "chunk_id": "Globalization_12",
        "text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never reco...",
        "full_text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never recovered since the Great Recession. New opposing views from some economists have argued such trends are a result of price drops and in actuality, trade volume is increasing, especially with agricultural products, natural resources and refined petroleum. The 21st century melting of the Arctic will also affect global trade, as it is paving the way for shorter trade routes. Economic globalization\nEconomic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital. Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market. Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes competition, technology, and corporations and industries. Current globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration. International standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations.",
        "rrf_score": 0.029418126757516764,
        "dense_rank": 7,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Globalization",
        "title": "Globalization"
      },
      {
        "chunk_id": "Economics_19",
        "text": "It has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation ...",
        "full_text": "It has been observed that a high volume of trade occurs among regions even with access to a similar technology and mix of factor inputs, including high-income countries. This has led to investigation of economies of scale and agglomeration to explain specialisation in similar but differentiated product lines, to the overall benefit of respective trading parties or regions. The general theory of specialisation applies to trade among individuals, farms, manufacturers, service providers, and economies. Among each of these production systems, there may be a corresponding division of labour with different work groups specializing, or correspondingly different types of capital equipment and differentiated land uses. An example that combines features above is a country that specialises in the production of high-tech knowledge products, as developed countries do, and trades with developing nations for goods produced in factories where labour is relatively cheap and plentiful, resulting in different in opportunity costs of production. More total output and utility thereby results from specializing in production and trading than if each country produced its own high-tech and low-tech products. Theory and observation set out the conditions such that market prices of outputs and productive inputs select an allocation of factor inputs by comparative advantage, so that (relatively) low-cost inputs go to producing low-cost outputs. In the process, aggregate output may increase as a by-product or by design. Such specialisation of production creates opportunities for gains from trade whereby resource owners benefit from trade in the sale of one type of output for other, more highly valued goods. A measure of gains from trade is the increased income levels that trade may facilitate. Supply and demand\nPrices and quantities have been described as the most directly observable attributes of goods produced and exchanged in a market economy. The theory of supply and demand is an organizing principle for explaining how prices coordinate the amounts produced and consumed. In microeconomics, it applies to price and output determination for a market with perfect competition, which includes the condition of no buyers or sellers large enough to have price-setting power.",
        "rrf_score": 0.015625,
        "dense_rank": 4,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Economics",
        "title": "Economics"
      }
    ]
  },
  {
    "question": "Does qualitative or quantitative research use numerical data?",
    "answer": "Quantitative research.",
    "source_id": "Research",
    "url": "https://en.wikipedia.org/wiki/Research",
    "category": "comparative",
    "generated_answer": "Quantitative research Quantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical",
    "response_time": "10.37s",
    "fused_chunks": [
      {
        "chunk_id": "Research_8",
        "text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting num...",
        "full_text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive). Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism. The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that can be summarized, compared, and generalized to larger populations if the data are collected using proper sampling and data collection strategies. Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest. If the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment). If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants. In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible. Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Sociology_20",
        "text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to est...",
        "full_text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims. Sociologists are often divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. Quantitative methodologies hold the dominant position in sociology, especially in the United States. In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two. (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective, and the very term \"methodology\" is often used synonymously with \"statistics\". Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the general public, though this judgment continues to be challenged by antipositivists. The choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to obtain statistical patterns on a target sample, and then combined with a qualitative interview to determine the play of agency. Sampling\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 2,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Research_7",
        "text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen und...",
        "full_text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\n\nExploratory research, which helps to identify and define a problem or question. Constructive research, which tests theories and proposes solutions to a problem or question. Empirical research, which tests the feasibility of a solution using empirical evidence. There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\nQualitative research\nQualitative research refers to much more subjective non-quantitative, use different methods of collecting data, analyzing data, interpreting data for meanings, definitions, characteristics, symbols metaphors of things. Qualitative research further classified into the following types: Ethnography: This research mainly focus on culture of group of people which includes share attributes, language, practices, structure, value, norms and material things, evaluate human lifestyle. Ethno: people, Grapho: to write, this disciple may include ethnic groups, ethno genesis, composition, resettlement and social welfare characteristics. Phenomenology: It is very powerful strategy for demonstrating methodology to health professions education as well as best suited for exploring challenging problems in health professions educations. In addition, PMP researcher Mandy Sha argued that a project management approach is necessary to control the scope, schedule, and cost related to qualitative research design, participant recruitment, data collection, reporting, as well as stakeholder engagement. Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).",
        "rrf_score": 0.03177805800756621,
        "dense_rank": 5,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Sociology_21",
        "text": "Sampling\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'samp...",
        "full_text": "Sampling\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population. In quantitative research, statistics are used to draw inferences from this sample regarding the population as a whole. The process of selecting a sample is referred to as 'sampling'. While it is usually best to sample randomly, concern with differences between specific subpopulations sometimes calls for stratified sampling. Conversely, the impossibility of random sampling sometimes necessitates nonprobability sampling, such as convenience sampling or snowball sampling. Methods\nThe following list of research methods is neither exclusive nor exhaustive:\n\nArchival research (or the Historical method): Draws upon the secondary data located in historical archives and records, such as biographies, memoirs, journals, and so on. Content analysis: The content of interviews and other texts is systematically analysed. Often data is 'coded' as a part of the 'grounded theory' approach using qualitative data analysis (QDA) software, such as Atlas.ti, MAXQDA, NVivo, or QDA Miner. Experimental research: The researcher isolates a single social process and reproduces it in a laboratory (for example, by creating a situation where unconscious sexist judgements are possible), seeking to determine whether or not certain social variables can cause, or depend upon, other variables (for instance, seeing if people's feelings about traditional gender roles can be manipulated by the activation of contrasting gender stereotypes). Participants are randomly assigned to different groups that either serve as controls—acting as reference points because they are tested with regard to the dependent variable, albeit without having been exposed to any independent variables of interest—or receive one or more treatments. Randomization allows the researcher to be sure that any resulting differences between groups are the result of the treatment. Longitudinal study: An extensive examination of a specific person or group over a long period of time. Observation: Using data from the senses, the researcher records information about social phenomenon or behaviour. Observation techniques may or may not feature participation.",
        "rrf_score": 0.030158730158730156,
        "dense_rank": 3,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Education_19",
        "text": "Quantitative research mirrors the methodologies of the natural sciences, employing precise numerical measurements to collect data from numerous observations and utilizing statistical tools for analysi...",
        "full_text": "Quantitative research mirrors the methodologies of the natural sciences, employing precise numerical measurements to collect data from numerous observations and utilizing statistical tools for analysis. Its goal is to attain an objective and impartial understanding. Conversely, qualitative research typically involves a smaller sample size and seeks to gain a nuanced insight into subjective and personal factors, such as individuals' experiences within the educational process. Mixed-methods research aims to integrate data gathered from both approaches to achieve a balanced and comprehensive understanding. Data collection methods vary and may include direct observation, test scores, interviews, and questionnaires. Research projects may investigate fundamental factors influencing all forms of education or focus on specific applications, seek solutions to particular problems, or evaluate the effectiveness of educational initiatives and policies. Subfields\nEducation studies encompasses various subfields such as pedagogy, educational research, comparative education, and the philosophy, psychology, sociology, economics, and history of education. The philosophy of education is the branch of applied philosophy that examines many of the fundamental assumptions underlying the theory and practice of education. It explores education both as a process and a discipline while seeking to provide precise definitions of its nature and distinctions from other phenomena. Additionally, it delves into the purpose of education, its various types, and the conceptualization of teachers, students, and their relationship. Furthermore, it encompasses educational ethics, which examines the moral implications of education, such as the ethical principles guiding it and how teachers should apply them to specific situations. The philosophy of education boasts a long history and was a subject of discourse in ancient Greek philosophy. The term \"pedagogy\" is sometimes used interchangeably with education studies, but in a more specific sense, it refers to the subfield focused on teaching methods. It investigates how educational objectives, such as knowledge transmission or the development of skills and character traits, can be achieved. Pedagogy is concerned with the methods and techniques employed in teaching within conventional educational settings. While some definitions confine it to this context, in a broader sense, it encompasses all forms of education, including teaching methods beyond traditional school environments.",
        "rrf_score": 0.03007688828584351,
        "dense_rank": 7,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Education",
        "title": "Education"
      }
    ]
  },
  {
    "question": "Which focuses on interfaces between people and computers: HCI or computer graphics?",
    "answer": "HCI.",
    "source_id": "Human–computer interaction",
    "url": "https://en.wikipedia.org/wiki/Human–computer_interaction",
    "category": "comparative",
    "generated_answer": "HCI",
    "response_time": "5.76s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_graphics_0",
        "text": "Computer graphics deals with generating images and art with the aid of computers. Computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer...",
        "full_text": "Computer graphics deals with generating images and art with the aid of computers. Computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research. Computer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. It is also used for processing image data received from the physical world, such as photo and video content. Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, and video games in general. Overview\nThe term computer graphics has been used in a broad sense to describe \"almost everything on computers that is not text or sound\". Typically, the term computer graphics refers to several different things:\n\nthe representation and manipulation of image data by a computer\nthe various technologies used to create and manipulate images\nmethods for digitally synthesizing and manipulating visual content, see study of computer graphics\nToday, computer graphics is widespread. Such imagery is found in and on television, newspapers, weather reports, and in a variety of medical investigations and surgical procedures. A well-constructed graph can present complex statistics in a form that is easier to understand and interpret. In the media \"such graphs are used to illustrate papers, reports, theses\", and other presentation material. Many tools have been developed to visualize data. Computer-generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_18",
        "text": "The most effective models have generally been trained on massive amounts of image and text data scraped from the web. By 2022, the best of these models, for example Dall-E 2 and Stable Diffusion, are ...",
        "full_text": "The most effective models have generally been trained on massive amounts of image and text data scraped from the web. By 2022, the best of these models, for example Dall-E 2 and Stable Diffusion, are able to create images in a range of styles, ranging from imitations of living artists to near-photorealistic, in a matter of seconds, given powerful enough hardware. Three-dimensional\n3D graphics, compared to 2D graphics, are graphics that use a three-dimensional representation of geometric data. For the purpose of performance, this is stored in the computer. This includes images that may be for later display or for real-time viewing. Despite these differences, 3D computer graphics rely on similar algorithms as 2D computer graphics do in the frame and raster graphics (like in 2D) in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques. 3D computer graphics are the same as 3D models. The model is contained within the graphical data file, apart from the rendering. However, there are differences that include the 3D model being the representation of any 3D object. Until visually displayed, a model is not graphic. Due to printing, 3D models are not only confined to virtual space. 3D rendering is how a model can be displayed. Also can be used in non-graphical computer simulations and calculations. Computer animation\nComputer animation is the art of creating moving images via the use of computers. It is a subfield of computer graphics and animation. Increasingly it is created by means of 3D computer graphics, though 2D computer graphics are still widely used for stylistic, low bandwidth, and faster real-time rendering needs. Sometimes the target of the animation is the computer itself, but sometimes the target is another medium, such as film. It is also referred to as CGI (Computer-generated imagery or computer-generated imaging), especially when used in films.",
        "rrf_score": 0.029850746268656716,
        "dense_rank": 7,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_6",
        "text": "In 1968, Dave Evans and Ivan Sutherland founded the first computer graphics hardware company, Evans & Sutherland. While Sutherland originally wanted the company to be located in Cambridge, Massachuset...",
        "full_text": "In 1968, Dave Evans and Ivan Sutherland founded the first computer graphics hardware company, Evans & Sutherland. While Sutherland originally wanted the company to be located in Cambridge, Massachusetts, Salt Lake City was instead chosen due to its proximity to the professors' research group at the University of Utah. Also in 1968 Arthur Appel described the first ray casting algorithm, the first of a class of ray tracing-based rendering algorithms that have since become fundamental in achieving photorealism in graphics by modeling the paths that rays of light take from a light source, to surfaces in a scene, and into the camera. In 1969, the ACM initiated A Special Interest Group on Graphics (SIGGRAPH) which organizes conferences, graphics standards, and publications within the field of computer graphics. By 1973, the first annual SIGGRAPH conference was held, which has become one of the focuses of the organization. SIGGRAPH has grown in size and importance as the field of computer graphics has expanded over time. 1970s\nSubsequently, a number of breakthroughs in the field occurred at the University of Utah in the 1970s, which had hired Ivan Sutherland. He was paired with David C. Evans to teach an advanced computer graphics class, which contributed a great deal of founding research to the field and taught several students who would grow to found several of the industry's most important companies – namely Pixar, Silicon Graphics, and Adobe Systems. Tom Stockham led the image processing group at UU which worked closely with the computer graphics lab. One of these students was Edwin Catmull. Catmull had just come from The Boeing Company and had been working on his degree in physics. Growing up on Disney, Catmull loved animation yet quickly discovered that he did not have the talent for drawing. Now Catmull (along with many others) saw computers as the natural progression of animation and they wanted to be part of the revolution. The first computer animation that Catmull saw was his own. He created an animation of his hand opening and closing.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Linux_10",
        "text": "Most low-level Linux components, including various parts of the userland, use the CLI exclusively. The CLI is particularly suited for automation of repetitive or delayed tasks and provides very simple...",
        "full_text": "Most low-level Linux components, including various parts of the userland, use the CLI exclusively. The CLI is particularly suited for automation of repetitive or delayed tasks and provides very simple inter-process communication. On desktop systems, the most popular user interfaces are the GUI shells, packaged together with extensive desktop environments, such as KDE Plasma, GNOME, MATE, Cinnamon, LXDE, Pantheon, and Xfce, though a variety of additional user interfaces exist. Most popular user interfaces are based on the X Window System, often simply called \"X\" or \"X11\". It provides network transparency and permits a graphical application running on one system to be displayed on another where a user may interact with the application; however, certain extensions of the X Window System are not capable of working over the network. Several X display servers exist, with the reference implementation, X.Org Server, being the most popular. Several types of window managers exist for X11, including tiling, dynamic, stacking, and compositing. Window managers provide means to control the placement and appearance of individual application windows, and interact with the X Window System. Simpler X window managers such as dwm, ratpoison, or i3wm provide a minimalist functionality, while more elaborate window managers such as FVWM, Enlightenment, or Window Maker provide more features such as a built-in taskbar and themes, but are still lightweight when compared to desktop environments. Desktop environments include window managers as part of their standard installations, such as Mutter (GNOME), KWin (KDE), or Xfwm (xfce), although users may choose to use a different window manager if preferred. Wayland is a display server protocol intended as a replacement for the X11 protocol; as of 2022, it has received relatively wide adoption. Unlike X11, Wayland does not need an external window manager and compositing manager. Therefore, a Wayland compositor takes the role of the display server, window manager, and compositing manager. Weston is the reference implementation of Wayland, while GNOME's Mutter and KDE's KWin are being ported to Wayland as standalone display servers.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Linux",
        "title": "Linux"
      },
      {
        "chunk_id": "Computer_graphics_11",
        "text": "In 1988, the first shaders – small programs designed specifically to do shading as a separate algorithm – were developed by Pixar, which had already spun off from Industrial Light & Magic as a separat...",
        "full_text": "In 1988, the first shaders – small programs designed specifically to do shading as a separate algorithm – were developed by Pixar, which had already spun off from Industrial Light & Magic as a separate entity – though the public would not see the results of such technological progress until the next decade. In the late 1980s, Silicon Graphics (SGI) computers were used to create some of the first fully computer-generated short films at Pixar, and Silicon Graphics machines were considered a high-water mark for the field during the decade. The 1980s is also called the golden era of videogames; millions-selling systems from Atari, Nintendo and Sega, among other companies, exposed computer graphics for the first time to a new, young, and impressionable audience – as did MS-DOS-based personal computers, Apple IIs, Macs, and Amigas, all of which also allowed users to program their own games if skilled enough. For the arcades, advances were made in commercial, real-time 3D graphics. In 1988, the first dedicated real-time 3D graphics boards were introduced for arcades, with the Namco System 21 and Taito Air System. On the professional side, Evans & Sutherland and SGI developed 3D raster graphics hardware that directly influenced the later single-chip graphics processing unit (GPU), a technology where a separate and very powerful chip is used in parallel processing with a CPU to optimize graphics. The decade also saw computer graphics applied to many additional professional markets, including location-based entertainment and education with the E&S Digistar, vehicle design, vehicle simulation, and chemistry. 1990s\nThe 1990s' highlight was the emergence of 3D modeling on a mass scale and an rise in the quality of CGI generally. Home computers became able to take on rendering tasks that previously had been limited to workstations costing thousands of dollars; as 3D modelers became available for home systems, the popularity of Silicon Graphics workstations declined and powerful Microsoft Windows and Apple Macintosh machines running Autodesk products like 3D Studio or other home rendering software ascended in importance. By the end of the decade, the GPU would begin its rise to the prominence it still enjoys today.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      }
    ]
  },
  {
    "question": "Why did deep learning surge after 2012?",
    "answer": "Because of faster computers (especially GPUs) and the availability of large datasets.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "TDNN was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutional layers and downsampling layers to classify non-linearily separable pattern classes",
    "response_time": "13.98s",
    "fused_chunks": [
      {
        "chunk_id": "Deep_learning_9",
        "text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision ...",
        "full_text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. Deep learning revolution\nThe deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning. A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_24",
        "text": "(e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically. In f...",
        "full_text": "(e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website. With the support of Innovation Diffusion Theory (IDT), a study analyzed the diffusion of Deep Learning in BRICS and OECD countries using data from Google Trends. Errors\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). Cyber threat\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 2,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_general_intelligence_9",
        "text": "Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances...",
        "full_text": "Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs. In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert. In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave. In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27. In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training.",
        "rrf_score": 0.029877369007803793,
        "dense_rank": 5,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Deep_learning_4",
        "text": "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep net...",
        "full_text": "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\". The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.",
        "rrf_score": 0.029631255487269532,
        "dense_rank": 8,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_5",
        "text": "popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used conv...",
        "full_text": "popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images. Recurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training.",
        "rrf_score": 0.02877846790890269,
        "dense_rank": 9,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      }
    ]
  },
  {
    "question": "Why can AI systems create filter bubbles?",
    "answer": "Because they optimize for engagement and repeatedly show similar content.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust",
    "response_time": "11.21s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_21",
        "text": "Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that t...",
        "full_text": "Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem. In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer and Nobel Prize-winning computer scientist Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. The ability to influence electorates has been proved in at least one study. This same study shows more inaccurate statements from the models when they advocate for candidates of the political right. AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models. Algorithmic bias and fairness\nMachine learning applications can be biased if they learn from biased data. The developers may not be aware that the bias exists. Discriminatory behavior by some LLMs can be observed in their output. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases. On 28 June 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black.",
        "rrf_score": 0.031544957774465976,
        "dense_rank": 6,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_0",
        "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...",
        "full_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters.",
        "rrf_score": 0.030798389007344232,
        "dense_rank": 3,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_1",
        "text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of ...",
        "full_text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. Reasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. Knowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_20",
        "text": "Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuc...",
        "full_text": "Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI. On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it. Misinformation\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_39",
        "text": "Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI\nAI researchers are divide...",
        "full_text": "Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. Machine consciousness, sentience, and mind\nThere is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction. Consciousness\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "Why is explainability important in AI?",
    "answer": "Because people affected by decisions have a right to an explanation.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "humans can understand the decisions or predictions made by the AI",
    "response_time": "6.51s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_21",
        "text": "Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine ex...",
        "full_text": "Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes. In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users. Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research itself. Explainability\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. Overfitting\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is. Other limitations and vulnerabilities\nLearners can also be disappointed by \"learning the wrong lesson\".",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_intelligence_38",
        "text": "However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"inte...",
        "full_text": "However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. Neat vs. scruffy\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both. Soft vs. hard computing\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 2,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_28",
        "text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, la...",
        "full_text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Geoffrey Hinton, Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"",
        "rrf_score": 0.02964426877470356,
        "dense_rank": 6,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "Why is international trade complex?",
    "answer": "Because currencies, laws, and government policies differ between countries.",
    "source_id": "International trade",
    "url": "https://en.wikipedia.org/wiki/International_trade",
    "category": "inferential",
    "generated_answer": "cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural",
    "response_time": "10.92s",
    "fused_chunks": [
      {
        "chunk_id": "International_trade_0",
        "text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade repr...",
        "full_text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade represents a significant share of gross domestic product (GDP). While international trade has existed throughout history (for example Uttarapatha, Silk Road, Amber Road, salt roads), its economic, social, and political importance has been on the rise in recent centuries. Carrying out trade at an international level is a complex process when compared to domestic trade. When trade takes place between two or more states, factors like currency, government policies, economy, judicial system, laws, and markets influence trade. To ease and justify the process of trade between countries of different economic standing in the modern era, some international economic organizations were formed, such as the World Trade Organization. These organizations work towards the facilitation and growth of international trade. Statistical services of intergovernmental and supranational organizations and governmental statistical agencies publish official statistics on international trade. Characteristics of global trade\nA product that is transferred or sold from a party in one country to a party in another country is an export from the originating country, and an import to the country receiving that product. Imports and exports are accounted for in a country's current account in the balance of payments. Trading globally may give consumers and countries the opportunity to be exposed to new markets and  products. Almost every kind of product can be found in the international market, for example: food, clothes, spare parts, oil, jewellery, wine, stocks, currencies, and water. Services are also traded, such as in tourism, banking, consulting, and transportation. Advanced technology (including transportation), globalization, industrialization, outsourcing and multinational corporations have major impacts on the international trade systems. Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "International_trade_1",
        "text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally re...",
        "full_text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not. However, in practical terms, carrying out trade at an international level is typically a more complex process than domestic trade. The main difference is that international trade is typically more costly than domestic trade. This is due to the fact that cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural differences, product safety, the legal system, and so on. Another difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries. Thus, international trade is mostly restricted to trade in goods and services, and only to a lesser extent to trade in capital, labour, or other factors of production. Trade in goods and services can serve as a substitute for trade in factors of production. Instead of importing a factor of production, a country can import goods that make intensive use of that factor of production and thus embody it. An example of this is the import of labor-intensive goods by the United States from China. Instead of importing Chinese labor, the United States imports goods that were produced with Chinese labor. One report in 2010, suggested that international trade was increased when a country hosted a network of immigrants, but the trade effect was weakened when the immigrants became assimilated into their new country. History\nThe history of international trade chronicles notable events that have affected trading among various economies. Theories and models\nThere are several models that seek to explain the factors behind international trade, the welfare consequences of trade and the pattern of trade. Most traded export products\nLargest countries or regions by total international trade\nThe following table is a list of the 30 largest trading states according to the World Trade Organization in 2024.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 3,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "Globalization_12",
        "text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never reco...",
        "full_text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never recovered since the Great Recession. New opposing views from some economists have argued such trends are a result of price drops and in actuality, trade volume is increasing, especially with agricultural products, natural resources and refined petroleum. The 21st century melting of the Arctic will also affect global trade, as it is paving the way for shorter trade routes. Economic globalization\nEconomic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital. Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market. Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes competition, technology, and corporations and industries. Current globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration. International standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations.",
        "rrf_score": 0.03128054740957967,
        "dense_rank": 2,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Globalization",
        "title": "Globalization"
      },
      {
        "chunk_id": "Globalization_13",
        "text": "Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the ...",
        "full_text": "Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations. A multinational corporation, or worldwide enterprise, is an organization that owns or controls the production of goods or services in one or more countries other than their home country. It can also be referred to as an international corporation, a transnational corporation, or a stateless corporation. A free-trade area is the region encompassing a trade bloc whose member countries have signed a free-trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers –  import quotas and tariffs –  and to increase trade of goods and services with each other. If people are also free to move between the countries, in addition to a free-trade agreement, it would also be considered an open border. Arguably, the most significant free-trade area in the world is the European Union, a politico-economic union of 27 member states that are primarily located in Europe. The EU has developed European Single Market through a standardized system of laws that apply in all member states. EU policies aim to ensure the free movement of people, goods, services, and capital within the internal market,\n\nTrade facilitation looks at how procedures and controls governing the movement of goods across national borders can be improved to reduce associated cost burdens and maximize efficiency while safeguarding legitimate regulatory objectives. Global trade in services is also significant. For example, in India, business process outsourcing has been described as the \"primary engine of the country's development over the next few decades, contributing broadly to GDP growth, employment growth, and poverty alleviation\". William I. Robinson's theoretical approach to globalization is a critique of Wallerstein's World Systems Theory. He believes that the global capital experienced today is due to a new and distinct form of globalization which began in the 1980s. Robinson argues not only are economic activities expanded across national boundaries but also there is a transnational fragmentation of these activities.",
        "rrf_score": 0.030117753623188408,
        "dense_rank": 4,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Globalization",
        "title": "Globalization"
      },
      {
        "chunk_id": "Language_25",
        "text": "Latin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word o...",
        "full_text": "Latin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word order in questions, which exists in many languages. This rule explains why when in English, the phrase \"John is talking to Lucy\" is turned into a question, it becomes \"Who is John talking to? \", and not \"John is talking to who?\". The latter example may be used as a way of placing special emphasis on \"who\", thereby slightly altering the meaning of the question. Syntax also includes the rules for how complex sentences are structured by grouping words together in units, called phrases, that can occupy different places in a larger syntactic structure. Sentences can be described as consisting of phrases connected in a tree structure, connecting the phrases to each other at different levels. To the right is a graphic representation of the syntactic analysis of the English sentence \"the cat sat on the mat\". The sentence is analyzed as being constituted by a noun phrase, a verb, and a prepositional phrase; the prepositional phrase is further divided into a preposition and a noun phrase, and the noun phrases consist of an article and a noun. The reason sentences can be seen as being composed of phrases is because each phrase would be moved around as a single element if syntactic operations were carried out. For example, \"the cat\" is one phrase, and \"on the mat\" is another, because they would be treated as single units if a decision was made to emphasize the location by moving forward the prepositional phrase: \"[And] on the mat, the cat sat\". There are many different formalist and functionalist frameworks that propose theories for describing syntactic structures, based on different assumptions about what language is and how it should be described. Each of them would analyze a sentence such as this in a different manner. Typology and universals\nLanguages can be classified in relation to their grammatical types.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Language",
        "title": "Language"
      }
    ]
  },
  {
    "question": "Why is computer vision linked to neurobiology?",
    "answer": "Because it draws inspiration from biological visual systems.",
    "source_id": "Computer vision",
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "category": "inferential",
    "generated_answer": "the primary visual cortex",
    "response_time": "5.65s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_vision_3",
        "text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, s...",
        "full_text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields. Signal processing\nYet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. Robotic navigation\nRobot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n\nVisual computing\nOther fields\nBesides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_0",
        "text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical ...",
        "full_text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Definition\nComputer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.",
        "rrf_score": 0.031746031746031744,
        "dense_rank": 3,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_4",
        "text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how...",
        "full_text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry. Distinctions\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted:\n\nImage processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images.",
        "rrf_score": 0.030330882352941176,
        "dense_rank": 8,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Robotics_18",
        "text": "As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and...",
        "full_text": "As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras. In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities. There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology. Environmental interaction and navigation\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.",
        "rrf_score": 0.030303030303030304,
        "dense_rank": 6,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Robotics",
        "title": "Robotics"
      },
      {
        "chunk_id": "Computer_vision_7",
        "text": "An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. I...",
        "full_text": "An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans—ultrasonic images or X-ray images, for example—to reduce the influence of noise. Machine vision\nA second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting. Military\nThe obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability. Autonomous vehicles\nOne of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV).",
        "rrf_score": 0.029877369007803793,
        "dense_rank": 5,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      }
    ]
  },
  {
    "question": "How did GPUs and big datasets together influence deep learning success?",
    "answer": "GPUs enabled fast training and large datasets provided sufficient learning material.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "increased computer power (including the hundred-fold increase in speed by switching",
    "response_time": "7.96s",
    "fused_chunks": [
      {
        "chunk_id": "Deep_learning_9",
        "text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision ...",
        "full_text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. Deep learning revolution\nThe deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning. A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 4,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Deep_learning_24",
        "text": "(e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically. In f...",
        "full_text": "(e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website. With the support of Innovation Diffusion Theory (IDT), a study analyzed the diffusion of Deep Learning in BRICS and OECD countries using data from Google Trends. Errors\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). Cyber threat\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target.",
        "rrf_score": 0.030834914611005692,
        "dense_rank": 2,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Machine_learning_25",
        "text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financi...",
        "full_text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financial incentives\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. Hardware\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. Tensor Processing Units (TPUs)\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.",
        "rrf_score": 0.029418126757516764,
        "dense_rank": 9,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_27",
        "text": "Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quan...",
        "full_text": "Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimisation. Common optimisation techniques include pruning, quantisation, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing. Software\nSoftware suites containing a variety of machine learning algorithms include the following:\n\nFree and open-source software\nProprietary software with free and open-source editions\nKNIME\nRapidMiner\n\nProprietary software\nJournals\nJournal of Machine Learning Research\nMachine Learning\nNature Machine Intelligence\nNeural Computation\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n\nConferences\nAAAI Conference on Artificial Intelligence\nAssociation for Computational Linguistics (ACL)\nEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)\nInternational Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB)\nInternational Conference on Machine Learning (ICML)\nInternational Conference on Learning Representations (ICLR)\nInternational Conference on Intelligent Robots and Systems (IROS)\nConference on Knowledge Discovery and Data Mining (KDD)\nConference on Neural Information Processing Systems (NeurIPS)\n\nSee also\nAutomated machine learning – Process of automating the application of machine learning\nBig data – Extremely large or complex datasets\nDeep learning — branch of ML concerned with artificial neural networks\nDifferentiable programming – Programming paradigm\nList of datasets for machine-learning research\nList of machine learning algorithms and List of algorithms for machine learning and statistical classification\nM-theory (learning framework) – Framework in machine learning\nMachine unlearning – Field of study in artificial intelligence\nOutline of machine learning\nSolomonoff's theory of inductive inference – Mathematical theory\n\nReferences\nSources\nDomingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0-465-06570-7. Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019. Poole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical Approach. New York: Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "How do RLHF and hallucinations connect?",
    "answer": "RLHF is used to reduce hallucinations.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "unanswerable",
    "response_time": "5.87s",
    "fused_chunks": [
      {
        "chunk_id": "Religion_13",
        "text": "Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities. Hallucinations and delusion...",
        "full_text": "Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities. Hallucinations and delusions related to religious content occurs in about 60% of people with schizophrenia. While this number varies across cultures, this had led to theories about a number of influential religious phenomena and possible relation to psychotic disorders. A number of prophetic experiences are consistent with psychotic symptoms, although retrospective diagnoses are practically impossible. Schizophrenic episodes are also experienced by people who do not have belief in gods. Religious content is also common in temporal lobe epilepsy, and obsessive-compulsive disorder. Atheistic content is also found to be common with temporal lobe epilepsy. Comparativism\nComparative religion is the branch of the study of religions concerned with the systematic comparison of the doctrines and practices of the world's religions. In general, the comparative study of religion yields a deeper understanding of the fundamental philosophical concerns of religion such as ethics, metaphysics, and the nature and form of salvation. Studying such material is meant to give one a richer and more sophisticated understanding of human beliefs and practices regarding the sacred, numinous, spiritual and divine. In the field of comparative religion, a common geographical classification of the main world religions includes Middle Eastern religions (including Zoroastrianism and Iranian religions), Indian religions, East Asian religions, African religions, American religions, Oceanic religions, and classical Hellenistic religions. Classification\nIn the 19th and 20th centuries, the academic practice of comparative religion divided religious belief into philosophically defined categories called world religions. Some academics studying the subject have divided religions into three broad categories:\n\nWorld religions, a term which refers to transcultural, international religions;\nIndigenous religions, which refers to smaller, culture-specific or nation-specific religious groups; and\nNew religious movements, which refers to recently developed religions.",
        "rrf_score": 0.030886196246139225,
        "dense_rank": 1,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Religion",
        "title": "Religion"
      },
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Semantics_1",
        "text": "Semantics is relevant to the fields of formal logic, computer science, and psychology. Definition and related fields\nSemantics is the study of meaning in languages. It is a systematic inquiry that exa...",
        "full_text": "Semantics is relevant to the fields of formal logic, computer science, and psychology. Definition and related fields\nSemantics is the study of meaning in languages. It is a systematic inquiry that examines what linguistic meaning is and how it arises. It investigates how expressions are built up from different layers of constituents, like morphemes, words, clauses, sentences, and texts, and how the meanings of the constituents affect one another. Semantics can focus on a specific language, like English, but in its widest sense, it investigates meaning structures relevant to all languages. As a descriptive discipline, it aims to determine how meaning works without prescribing what meaning people should associate with particular expressions. Some of its key questions are \"How do the meanings of words combine to create the meanings of sentences? \", \"How do meanings relate to the minds of language users, and to the things words refer to? \", and \"What is the connection between what a word means, and the contexts in which it is used?\". The main disciplines engaged in semantics are linguistics, semiotics, and philosophy. Besides its meaning as a field of inquiry, semantics can also refer to theories within this field, like truth-conditional semantics, and to the meaning of particular expressions, like the semantics of the word fairy. As a field of inquiry, semantics has both an internal and an external side. The internal side is interested in the connection between words and the mental phenomena they evoke, like ideas and conceptual representations. The external side examines how words refer to objects in the world and under what conditions a sentence is true. Many related disciplines investigate language and meaning. Semantics contrasts with other subfields of linguistics focused on distinct aspects of language. Phonology studies the different types of sounds used in languages and how sounds are connected to form words while syntax examines the rules that dictate how to arrange words to create sentences.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Semantics",
        "title": "Semantics"
      },
      {
        "chunk_id": "Psychoanalysis_9",
        "text": "All attempts to develop a suitable neuronal treatment failed; in fact, the detailed examinations did not reveal any organic defects. In the monograph written on this cases, Freud documented his differ...",
        "full_text": "All attempts to develop a suitable neuronal treatment failed; in fact, the detailed examinations did not reveal any organic defects. In the monograph written on this cases, Freud documented his differential-diagnostically supported suspicion that neurotic symptoms probably would have psychological causes. Finishing the ineffective hypnosis, the idea of psychoanalysis began to receive serious attention; Freud initially called it free association. His first attempt to explain neurotical symptoms on this path was presented in Studies on Hysteria (1895). Co-authored with Josef Breuer, this is generally seen as the birth of psychoanalysis. The work based on their partly joint treatment of Bertha Pappenheim, referred to by the pseudonym \"Anna O.\" Bertha herself had dubbed the treatment talking cure. Breuer, a distinguished physician, was astonished but remained unspecific; while Freud formulated his hypothesis that Anna's hystera seemed to be caused by distressing but unconscious experiences related to sexuality, basing his assumption on corresponding free associations made by the young women. For example, she sometimes liked to jokingly rename her talking cure as chimney sweeping, an association about the fairy tale through which part of a pregnant woman's house 'the stork' gives birth to the baby – or in Lacan's words: \"The more Anna provided signifers, the more she chattered on, the better it went.\" See also Jokes and Their Relation to the Unconscious (1905). Around the same time, Freud had started to develop a neurological hypothesis about mental phenomena such as memory, but soon abandoned this attempt and left it unpublished. Insights into neuronal-biochemical processes that store experiences in the brain – like engraving the proverbial tabula rasa with some code – belongs to the physiological branch of science and lead in a different direction of research than the psychological question of what the differences between consciousness and unconsciousness are. After some thought about a suitable term, Freud called his new instrument and field of research psychoanalysis, introduced in his essay \"Inheritance and Etiology of Neuroses\", written in 1896.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Psychoanalysis",
        "title": "Psychoanalysis"
      },
      {
        "chunk_id": "Internet_of_things_18",
        "text": "On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted ...",
        "full_text": "On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role on the Internet and the Web, geo-spatial standards will play a key role on the Internet of things. A solution to \"basket of remotes\"\nMany IoT devices have the potential to take a piece of this market. Jean-Louis Gassée (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note, where he predicts that the most likely problem will be what he calls the \"basket of remotes\" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another. For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, \"where collected data is used to predict and trigger actions on the specific devices\" while making them work together. Social Internet of things\nSocial Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices. SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services, and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering. Social Network for IoT Devices (Not Human)\nIoT defines a device with an identity like a citizen in a community and connect them to the Internet to provide services to its users. SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human. How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      }
    ]
  },
  {
    "question": "How do AI agents improve over time?",
    "answer": "They incorporate learning algorithms and adapt from experience.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "through experience or training",
    "response_time": "6.47s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_14",
        "text": "Military\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting i...",
        "full_text": "Military\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous. AI has been used in military operations in Iraq, Syria, Israel and Ukraine. Generative AI\nAgents\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. Web search\nMicrosoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search. For safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content. Google officially pushed its AI Search at its Google I/O event on 20 May 2025. It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 3,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_4",
        "text": "An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was e...",
        "full_text": "An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was echoed by Scott Aaronson during his time at OpenAI. AI-complete problems\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that AGI would be needed to solve it, because the solution is beyond the capabilities of a purpose-specific algorithm. Many problems have been conjectured to require general intelligence to solve. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance. However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning. History\nClassical AI\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's fictional character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\". Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_28",
        "text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, la...",
        "full_text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Geoffrey Hinton, Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_0",
        "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...",
        "full_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_3",
        "text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent m...",
        "full_text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. Learning\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "What is digital media?",
    "answer": "Digitized information broadcast through screens or speakers.",
    "source_id": "Digital media",
    "url": "https://en.wikipedia.org/wiki/Digital_media",
    "category": "factual",
    "generated_answer": "any communication media that operates in conjunction with various encoded machine-readable data formats",
    "response_time": "7.30s",
    "fused_chunks": [
      {
        "chunk_id": "Digital_media_0",
        "text": "In mass communication, digital media is any communication media that operates in conjunction with various encoded machine-readable data formats. Digital content can be created, viewed, distributed, mo...",
        "full_text": "In mass communication, digital media is any communication media that operates in conjunction with various encoded machine-readable data formats. Digital content can be created, viewed, distributed, modified, listened to, and preserved on a digital electronic device, including digital data storage media (in contrast to analog electronic media) and digital broadcasting. Digital is defined as any data represented by a series of digits, and media refers to methods of broadcasting or communicating this information. Together, digital media refers to mediums of digitized information broadcast through a screen and/or a speaker. This also includes text, audio, video, and graphics  are transmitted over the internet for consumption on digital devices. Digital media platforms, such as YouTube, Kick, and Twitch, accounted for viewership rates of 27.9 billion hours in 2020. A contributing factor to its part in what is commonly referred to as the digital revolution can be attributed to the use of interconnectivity. Examples\nExamples of digital media include software, digital images,web pages and websites, social media, digital data and databases, digital audio such as MP3, electronic documents and electronic books. Digital media often contrasts with print media, such as printed books, newspapers and magazines, and other traditional or analog media, such as photographic film, audio tapes or video tapes. Digital media has had a significantly broad and complex impact on society and culture. Combined with the Internet and personal computing, digital media has caused disruptive innovation in publishing, journalism, public relations, entertainment, education, commerce and politics. Digital media has also posed new challenges to copyright and intellectual property laws, fostering an open content movement in which content creators voluntarily give up some or all of their legal rights to their work. The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history, called the Information Age, perhaps leading to a paperless society in which all media are produced and consumed on computers.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_5",
        "text": "The transition has created some uncertainty about definitions. Digital media, new media, multimedia, and similar terms all have a relationship to both the engineering innovations and cultural impact o...",
        "full_text": "The transition has created some uncertainty about definitions. Digital media, new media, multimedia, and similar terms all have a relationship to both the engineering innovations and cultural impact of digital media. The blending of digital media with other media, and with cultural and social factors, is sometimes known as new media or \"the new media.\" Similarly, digital media seems to demand a new set of communications skills, called transliteracy, media literacy, or digital literacy. These skills include not only the ability to read and write—traditional literacy—but the ability to navigate the Internet, evaluate sources, and create digital content. The idea that we are moving toward a fully digital, paperless society is accompanied by the fear that we may soon—or currently—be facing a digital dark age, in which older media are no longer accessible on modern devices or using modern methods of scholarship. Digital media has a significant, wide-ranging and complex effect on society and culture. A senior engineer at Motorola named Martin Cooper was the first person to make a phone call on April 3, 1973. He decided the first phone call should be to a rival telecommunications company saying \"I'm speaking via a mobile phone\". Ten years later, Motorola released the Motorola DynaTAC, the first commercially available mobile phone. In the early 1990s Nokia released the Nokia 1011, the first mass-produced mobile phone. The number of smartphone users has increased dramatically, as has the commercial landscape. Android and iOS dominate the smartphone market. A study by Gartner found that in 2016 about 88% of the worldwide smartphones were Android while iOS had a market share of about 12%. About 85% of the mobile market revenue came from mobile games. The impact of the digital revolution can also be assessed by exploring the amount of worldwide mobile smart device users there are. This can be split into 2 categories; smart phone users and smart tablet users. Worldwide there are currently 2.32 billion smartphone users across the world. This figure is to exceed 2.87 billion by 2020.",
        "rrf_score": 0.0315136476426799,
        "dense_rank": 2,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_7",
        "text": "It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping a...",
        "full_text": "It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping and complex. Between 2000 and 2015, the print newspaper advertising revenue has fallen from $60 billion to a nearly $20 billion. Even one of the most popular days for papers, Sunday, has seen a 9% circulation decrease the lowest since 1945. In journalism, digital media and citizen journalism have led to the loss of thousands of jobs in print media and the bankruptcy of many major newspapers. But the rise of digital journalism has also created thousands of new jobs and specializations. E-books and self-publishing are changing the book industry, and digital textbooks and other media-inclusive curricula are changing primary and secondary education. In academia, digital media has led to a new form of scholarship, also called digital scholarship, making open access and open science possible thanks to the low cost of distribution. New fields of study have grown, such as digital humanities and digital history. It has changed the way libraries are used and their role in society. Every major media, communications and academic endeavor is facing a period of transition and uncertainty related to digital media. Often time the magazine or publisher have a Digital edition which can be referred to an electronic formatted version identical to the print version. There is a huge benefit to the publisher and cost, as half of traditional publishers' costs come from production, including raw materials, technical processing, and distribution. Since 2004, there has been a decrease in newspaper industry employment, with only about 40,000 people working in the workforce currently. Alliance of Audited Media & Publishers information during the 2008 recession, over 10% of print sales are diminished for certain magazines, with a hardship coming from only 75% of the sales advertisements as before. However, in 2018, major newspapers advertising revenue was 35% from digital ads. In contrast, mobile versions of newspapers and magazines came in second with a huge growth of 135%.",
        "rrf_score": 0.03007688828584351,
        "dense_rank": 6,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_3",
        "text": "The year 2002 is assumed to be the year when human kind was able to store more information in digital than in analog media (the \"beginning of the digital age\"). Digital computers\nThough they used mach...",
        "full_text": "The year 2002 is assumed to be the year when human kind was able to store more information in digital than in analog media (the \"beginning of the digital age\"). Digital computers\nThough they used machine-readable media, Babbage's engines, player pianos, jacquard looms and many other early calculating machines were themselves analog computers, with physical, mechanical parts. The first truly digital media came into existence with the rise of digital computers. Digital computers use binary code and Boolean logic to store and process information, allowing one machine in one configuration to perform many different tasks. The first modern, programmable, digital computers, the Manchester Mark 1 and the EDSAC, were independently invented between 1948 and 1949. Though different in many ways from modern computers, these machines had digital software controlling their logical operations. They were encoded in binary, a system of ones and zeroes that are combined to make hundreds of characters. The 1s and 0s of binary are the \"digits\" of digital media. \"As We May Think\"\nWhile digital media did not come into common use until the late 20th century, the conceptual foundation of digital media is traced to the work of scientist and engineer Vannevar Bush and his celebrated essay \"As We May Think\", published in The Atlantic Monthly in 1945. Bush envisioned a system of devices that could be used to help scientists, doctors, and historians, among others, to store, analyze and communicate information. Calling this then-imaginary device a \"memex\", Bush wrote:\n\nThe owner of the memex, let us say, is interested in the origin and properties of the bow and arrow. Specifically, he is studying why the short Turkish bow was apparently superior to the English long bow in the skirmishes of the Crusades. He has dozens of possibly pertinent books and articles in his memex. First, he runs through an encyclopedia, finds an interesting but sketchy article, and leaves it projected. Next, in history, he finds another pertinent item and ties the two together. Thus he goes, building a trail of many items.",
        "rrf_score": 0.029631255487269532,
        "dense_rank": 7,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_6",
        "text": "This can be split into 2 categories; smart phone users and smart tablet users. Worldwide there are currently 2.32 billion smartphone users across the world. This figure is to exceed 2.87 billion by 20...",
        "full_text": "This can be split into 2 categories; smart phone users and smart tablet users. Worldwide there are currently 2.32 billion smartphone users across the world. This figure is to exceed 2.87 billion by 2020. Smart tablet users reached a total of 1 billion in 2015, 15% of the world's population. The statistics evidence the impact of digital media communications today. What is also of relevance is the fact that the number of smart device users is rising rapidly yet the amount of functional uses increase daily. A smartphone or tablet can be used for hundreds of daily needs. There are currently over 1 million apps on the Apple App store. These represent significant opportunities for digital marketing strategies. A smartphone user is impacted with digital advertising every second they open their Apple or Android device. This further evidences the digital revolution and the impact of revolution. This has resulted in a total of 13 billion dollars being paid out to the various app developers over the years. This growth has fueled the development of millions of software applications. Most of these apps are able to generate income via in app advertising. Gross revenue for 2020 is projected to be about $189 million. Disruption in industry\nCompared with print media, the mass media, and other analog technologies, digital media are easy to copy, store, share and modify. This quality of digital media has led to significant changes in many industries, especially journalism, publishing, education, entertainment, and the music business. The overall effect of these changes is so far-reaching that it is difficult to quantify. For example, in movie-making, the transition from analog film cameras to digital cameras is nearly complete. The transition has economic benefits to Hollywood, making distribution easier and making it possible to add high-quality digital effects to films. At the same time, it has affected the analog special effects, stunt, and animation industries in Hollywood. It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping and complex.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      }
    ]
  },
  {
    "question": "What is HCI?",
    "answer": "The study of how people interact with computer systems.",
    "source_id": "Human–computer interaction",
    "url": "https://en.wikipedia.org/wiki/Human–computer_interaction",
    "category": "factual",
    "generated_answer": "human-machine",
    "response_time": "10.69s",
    "fused_chunks": [
      {
        "chunk_id": "Automation_13",
        "text": "The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry. Information technology, together with industrial machinery and processes, can assist in the ...",
        "full_text": "The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry. Information technology, together with industrial machinery and processes, can assist in the design, implementation, and monitoring of control systems. One example of an industrial control system is a programmable logic controller (PLC). PLCs are specialized hardened computers which are frequently used to synchronize the flow of inputs from (physical) sensors and events with the flow of outputs to actuators and events. Human-machine interfaces (HMI) or computer human interfaces (CHI), formerly known as man-machine interfaces, are usually employed to communicate with PLCs and other computers. Service personnel who monitor and control through HMIs can be called by different names. In the industrial process and manufacturing environments, they are called operators or something similar. In boiler houses and central utility departments, they are called stationary engineers. Different types of automation tools exist:\n\nANN – Artificial neural network\nDCS – Distributed control system\nHMI – Human machine interface\nRPA – Robotic process automation\nSCADA – Supervisory control and data acquisition\nPLC – Programmable logic controller\nInstrumentation\nMotion control\nRobotics\nHost simulation software (HSS) is a commonly used testing tool that is used to test the equipment software. HSS is used to test equipment performance concerning factory automation standards (timeouts, response time, processing time). Cognitive automation\nCognitive automation, as a subset of AI, is an emerging genus of automation enabled by cognitive computing. Its primary concern is the automation of clerical tasks and workflows that consist of structuring unstructured data. Cognitive automation relies on multiple disciplines: natural language processing, real-time computing, machine learning algorithms, big data analytics, and evidence-based learning. According to Deloitte, cognitive automation enables the replication of human tasks and judgment \"at rapid speeds and considerable scale.\" Such tasks include:\n\nDocument redaction\nData extraction and document synthesis / reporting\nContract management\nNatural language search\nCustomer, employee, and stakeholder onboarding\nManual activities and verifications\nFollow-up and email communications\n\nRecent and emerging applications\nCAD AI\nArtificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate in 3D modeling.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Automation",
        "title": "Automation"
      },
      {
        "chunk_id": "Philosophy_15",
        "text": "The main traditional responses are materialism, which assumes that matter is more fundamental; idealism, which assumes that mind is more fundamental; and dualism, which assumes that mind and matter ar...",
        "full_text": "The main traditional responses are materialism, which assumes that matter is more fundamental; idealism, which assumes that mind is more fundamental; and dualism, which assumes that mind and matter are distinct types of entities. In contemporary philosophy, another common view is functionalism, which understands mental states in terms of the functional or causal roles they play. The mind-body problem is closely related to the hard problem of consciousness, which asks how the physical brain can produce qualitatively subjective experiences. The philosophy of religion investigates the basic concepts, assumptions, and arguments associated with religion. It critically reflects on what religion is, how to define the divine, and whether one or more gods exist. It also includes the discussion of worldviews that reject religious doctrines. Further questions addressed by the philosophy of religion are: \"How are we to interpret religious language, if not literally? \"; \"Is divine omniscience compatible with free will? \"; and, \"Are the great variety of world religions in some way compatible in spite of their apparently contradictory theological claims?\" It includes topics from nearly all branches of philosophy. It differs from theology since theological debates typically take place within one religious tradition, whereas debates in the philosophy of religion transcend any particular set of theological assumptions. The philosophy of science examines the fundamental concepts, assumptions, and problems associated with science. It reflects on what science is and how to distinguish it from pseudoscience. It investigates the methods employed by scientists, how their application can result in knowledge, and on what assumptions they are based. It also studies the purpose and implications of science. Some of its questions are \"What counts as an adequate explanation? \"; \"Is a scientific law anything more than a description of a regularity? \"; and \"Can some special sciences be explained entirely in the terms of a more general science?\" It is a vast field that is commonly divided into the philosophy of the natural sciences and the philosophy of the social sciences, with further subdivisions for each of the individual sciences under these headings.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Linux_10",
        "text": "Most low-level Linux components, including various parts of the userland, use the CLI exclusively. The CLI is particularly suited for automation of repetitive or delayed tasks and provides very simple...",
        "full_text": "Most low-level Linux components, including various parts of the userland, use the CLI exclusively. The CLI is particularly suited for automation of repetitive or delayed tasks and provides very simple inter-process communication. On desktop systems, the most popular user interfaces are the GUI shells, packaged together with extensive desktop environments, such as KDE Plasma, GNOME, MATE, Cinnamon, LXDE, Pantheon, and Xfce, though a variety of additional user interfaces exist. Most popular user interfaces are based on the X Window System, often simply called \"X\" or \"X11\". It provides network transparency and permits a graphical application running on one system to be displayed on another where a user may interact with the application; however, certain extensions of the X Window System are not capable of working over the network. Several X display servers exist, with the reference implementation, X.Org Server, being the most popular. Several types of window managers exist for X11, including tiling, dynamic, stacking, and compositing. Window managers provide means to control the placement and appearance of individual application windows, and interact with the X Window System. Simpler X window managers such as dwm, ratpoison, or i3wm provide a minimalist functionality, while more elaborate window managers such as FVWM, Enlightenment, or Window Maker provide more features such as a built-in taskbar and themes, but are still lightweight when compared to desktop environments. Desktop environments include window managers as part of their standard installations, such as Mutter (GNOME), KWin (KDE), or Xfwm (xfce), although users may choose to use a different window manager if preferred. Wayland is a display server protocol intended as a replacement for the X11 protocol; as of 2022, it has received relatively wide adoption. Unlike X11, Wayland does not need an external window manager and compositing manager. Therefore, a Wayland compositor takes the role of the display server, window manager, and compositing manager. Weston is the reference implementation of Wayland, while GNOME's Mutter and KDE's KWin are being ported to Wayland as standalone display servers.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Linux",
        "title": "Linux"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Software_engineering_1",
        "text": "At the time, there was perceived to be a \"software crisis\". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Session...",
        "full_text": "At the time, there was perceived to be a \"software crisis\". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Sessions' keynotes of Frederick Brooks and Margaret Hamilton. In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process. The Process Maturity Levels introduced became the Capability Maturity Model Integration for Development (CMMI-DEV), which defined how the US Government evaluates the abilities of a software development team. Modern, generally accepted best practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of the major computing disciplines. In modern systems, where concepts such as Edge Computing, Internet of Things and Cyber-physical Systems are prevalent, software is a critical factor. Thus, software engineering is closely related to the Systems Engineering discipline. The Systems Engineering Body of Knowledge claims:\n\nSoftware is prominent in most modern systems architectures and is often the primary means for integrating complex system components. Software engineering and systems engineering are not merely related disciplines; they are intimately intertwined....Good systems engineering is a key factor in enabling good software engineering. Terminology\nDefinition\nNotable definitions of software engineering include:\n\n\"The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software. \"—The Bureau of Labor Statistics—IEEE Systems and software engineering – Vocabulary\n\"The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software. \"—IEEE Standard Glossary of Software Engineering Terminology\n\"An engineering discipline concerned with all aspects of software production.\" — Ian Sommerville\n\"The establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Software_engineering",
        "title": "Software engineering"
      }
    ]
  },
  {
    "question": "What is an export?",
    "answer": "A product sent from one country to another.",
    "source_id": "International trade",
    "url": "https://en.wikipedia.org/wiki/International_trade",
    "category": "factual",
    "generated_answer": "the act of obtaining the files from the repositories",
    "response_time": "9.56s",
    "fused_chunks": [
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Version_control_12",
        "text": "Conflict\nA conflict occurs when different parties make changes to the same document, and the system is unable to reconcile the changes. A user must resolve the conflict by combining the changes, or by...",
        "full_text": "Conflict\nA conflict occurs when different parties make changes to the same document, and the system is unable to reconcile the changes. A user must resolve the conflict by combining the changes, or by selecting one change in favour of the other. Delta compression\nMost revision control software uses delta compression, which retains only the differences between successive versions of files. This allows for more efficient storage of many different versions of files. Dynamic stream\nA stream in which some or all file versions are mirrors of the parent stream's versions. Export\nExporting is the act of obtaining the files from the repository. It is similar to checking out except that it creates a clean directory tree without the version-control metadata used in a working copy. This is often used prior to publishing the contents, for example. Fetch\nSee pull. Forward integration\nThe process of merging changes made in the main trunk into a development (feature or team) branch. Head\nAlso sometimes called tip, this refers to the most recent commit, either to the trunk or to a branch. The trunk and each branch have their own head, though HEAD is sometimes loosely used to refer to the trunk. Import\nImporting is the act of copying a local directory tree (that is not currently a working copy) into the repository for the first time. Initialize\nTo create a new, empty repository. Interleaved deltas\nSome revision control software uses Interleaved deltas, a method that allows storing the history of text based files in a more efficient way than by using Delta compression. Label\nSee tag. Locking\nWhen a developer locks a file, no one else can update that file until it is unlocked. Locking can be supported by the version control system, or via informal communications between developers (aka social locking). Mainline\nSimilar to trunk, but there can be a mainline for each branch. Merge\nA merge or integration is an operation in which two sets of changes are applied to a file or set of files.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Version_control",
        "title": "Version control"
      },
      {
        "chunk_id": "International_trade_1",
        "text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally re...",
        "full_text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not. However, in practical terms, carrying out trade at an international level is typically a more complex process than domestic trade. The main difference is that international trade is typically more costly than domestic trade. This is due to the fact that cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural differences, product safety, the legal system, and so on. Another difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries. Thus, international trade is mostly restricted to trade in goods and services, and only to a lesser extent to trade in capital, labour, or other factors of production. Trade in goods and services can serve as a substitute for trade in factors of production. Instead of importing a factor of production, a country can import goods that make intensive use of that factor of production and thus embody it. An example of this is the import of labor-intensive goods by the United States from China. Instead of importing Chinese labor, the United States imports goods that were produced with Chinese labor. One report in 2010, suggested that international trade was increased when a country hosted a network of immigrants, but the trade effect was weakened when the immigrants became assimilated into their new country. History\nThe history of international trade chronicles notable events that have affected trading among various economies. Theories and models\nThere are several models that seek to explain the factors behind international trade, the welfare consequences of trade and the pattern of trade. Most traded export products\nLargest countries or regions by total international trade\nThe following table is a list of the 30 largest trading states according to the World Trade Organization in 2024.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "Philosophy_3",
        "text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the ...",
        "full_text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: \"What can I know? \"; \"What should I do? \"; \"What may I hope? \"; and \"What is the human being?\" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as \"the midwife of the sciences\". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a \"rigorous science\" investigating essences.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Sociology_19",
        "text": "Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is...",
        "full_text": "Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\"). A perennial question within this debate is that of \"social reproduction\": how are structures (specifically, structures producing inequality) reproduced through the choices of individuals? Synchrony and diachrony\nSynchrony and diachrony (or statics and dynamics) within social theory are terms that refer to a distinction that emerged through the work of Levi-Strauss who inherited it from the linguistics of Ferdinand de Saussure. Synchrony slices moments of time for analysis, thus it is an analysis of static social reality. Diachrony, on the other hand, attempts to analyse dynamic sequences. Following Saussure, synchrony would refer to social phenomena as a static concept like a language, while diachrony would refer to unfolding processes like actual speech. In Anthony Giddens' introduction to Central Problems in Social Theory, he states that, \"in order to show the interdependence of action and structure…we must grasp the time space relations inherent in the constitution of all social interaction.\" And like structure and agency, time is integral to discussion of social reproduction. In terms of sociology, historical sociology is often better positioned to analyse social life as diachronic, while survey research takes a snapshot of social life and is thus better equipped to understand social life as synchronized. Some argue that the synchrony of social structure is a methodological perspective rather than an ontological claim. Nonetheless, the problem for theory is how to integrate the two manners of recording and thinking about social data. Research methodology\nSociological research methods may be divided into two broad, though often supplementary, categories:\n\nQualitative designs emphasize understanding of social phenomena through direct observation, communication with participants, or analysis of texts, and may stress contextual and subjective accuracy over generality. Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      }
    ]
  },
  {
    "question": "What is IoT?",
    "answer": "A network of devices connected and communicating via the Internet.",
    "source_id": "Internet of things",
    "url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "category": "factual",
    "generated_answer": "IoT describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communication",
    "response_time": "14.66s",
    "fused_chunks": [
      {
        "chunk_id": "Internet_of_things_19",
        "text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devi...",
        "full_text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users. Function\nIoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, shares information, monitors, navigates and groups with other IoT devices in the same or nearby network realizing SIoT and facilitating useful service compositions in order to help its users proactively in every day's life especially during emergency. Examples\nIoT-based smart home technology monitors health data of patients or aging adults by analyzing their physiological parameters and prompt the nearby health facilities when emergency medical services needed. In case emergency, automatically, ambulance of a nearest available hospital will be called with pickup location provided, ward assigned, patient's health data will be transmitted to the emergency department, and display on the doctor's computer immediately for further action. IoT sensors on the vehicles, road and traffic lights monitor the conditions of the vehicles and drivers and alert when attention needed and also coordinate themselves automatically to ensure autonomous driving is working normally. Unfortunately if an accident happens, IoT camera will inform the nearest hospital and police station for help. Challenges\nInternet of things is multifaceted and complicated. One of the main factors that hindering people from adopting and use Internet of things (IoT) based products and services is its complexity. Installation and setup is a challenge to people, therefore, there is a need for IoT devices to mix match and configure themselves automatically to provide different services at different situation. System security always a concern for any technology, and it is more crucial for SIoT as not only security of oneself need to be considered but also the mutual trust mechanism between collaborative IoT devices from time to time, from place to place. Another critical challenge for SIoT is the accuracy and reliability of the sensors.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_16",
        "text": "Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power ha...",
        "full_text": "Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the Internet to a server with sufficient processing power. Decentralized IoT\nDecentralized Internet of things, or decentralized IoT, is a modified IoT which utilizes fog computing to handle and balance requests of connected IoT devices in order to reduce loading on the cloud servers and improve responsiveness for latency-sensitive IoT applications like vital signs monitoring of patients, vehicle-to-vehicle communication of autonomous driving, and critical failure detection of industrial devices. Performance is improved, especially for huge IoT systems with millions of nodes. Conventional IoT is connected via a mesh network and led by a major head node (centralized controller). The head node decides how a data is created, stored, and transmitted. In contrast, decentralized IoT attempts to divide IoT systems into smaller divisions. The head node authorizes partial decision-making power to lower level sub-nodes under mutual agreed policy. Some approaches to decentralized IoT attempt to address the limited bandwidth and hashing capacity of battery-powered devices via blockchain, although traditional linear chains often struggle with the throughput required for billions of devices. To resolve this, recent research focuses on sharded distributed ledger protocols (such as Cerberus) that utilize braided synchronization. These protocols theoretically allow network throughput to scale linearly with the number of nodes, enabling the massive concurrency required for autonomous machine-to-machine transactions. Complexity\nIn semi-open or closed loops (i.e., value chains, whenever a global finality can be settled) the IoT will often be considered and studied as a complex system due to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a chaotic environment (since systems always have finality). As a practical approach, not all elements on the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability.",
        "rrf_score": 0.03007688828584351,
        "dense_rank": 7,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_18",
        "text": "On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted ...",
        "full_text": "On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role on the Internet and the Web, geo-spatial standards will play a key role on the Internet of things. A solution to \"basket of remotes\"\nMany IoT devices have the potential to take a piece of this market. Jean-Louis Gassée (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note, where he predicts that the most likely problem will be what he calls the \"basket of remotes\" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another. For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, \"where collected data is used to predict and trigger actions on the specific devices\" while making them work together. Social Internet of things\nSocial Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices. SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services, and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering. Social Network for IoT Devices (Not Human)\nIoT defines a device with an identity like a citizen in a community and connect them to the Internet to provide services to its users. SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human. How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_0",
        "text": "The Internet of things (IoT) describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and syste...",
        "full_text": "The Internet of things (IoT) describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communication networks. The IoT encompasses electronics, communication, and computer science engineering. \"Internet of things\" has been considered a misnomer because devices do not need to be connected to the public Internet; they only need to be connected to a network and be individually addressable. The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning. Traditional fields of embedded systems, wireless sensor networks, and control systems independently and collectively enable the Internet of Things. While in the consumer market, IoT technology is most synonymous with \"smart home\" products—including devices and appliances like thermostats and smart speakers—the technology's largest applications are in the business and industrial sectors. Commercial asset tracking and fleet management represent the largest single application of IoT, accounting for 22% of the total market, driven by the need to monitor mobile assets like vehicles and shipping containers. Other major applications include industrial monitoring, smart metering in utilities, and connected healthcare. However, several concerns exist regarding the risks associated with the growth and diffusion of IoT technologies and products, particularly in the areas of privacy and security. Consequently, several industries, technology companies, and governments (or their branches, ministries, bureaus, departments, etc.) of many countries have taken multiple steps and implemented a variety of precautionary measures to address these concerns adequately and minimize safety risks, including the development and implementation of international and local standards, guidelines, and regulatory frameworks. Due to their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly creates regulatory ambiguities, complicating jurisdictional boundaries of the data transfer.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_11",
        "text": "Between 2006 and January 2024, there were over 440 living labs (though not all are currently active) that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative an...",
        "full_text": "Between 2006 and January 2024, there were over 440 living labs (though not all are currently active) that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. When companies intend to implement and develop IoT services for smart cities, they need to have economic incentives. The US government plays a key role in smart city projects; changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the US government provides tax incentives and affordable rent, improves public transport, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share a common infrastructure and labor markets, and take advantage of locally embedded technologies, production processes, and transaction costs. Military\nThe Internet of Military Things (IoMT) is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an urban environment and involves the use of sensors, munitions, vehicles, robots, human-wearable biometrics, and other smart technology that is relevant on the battlefield. One of the examples of IOT devices used in the military is the Xaver 1000 system. The Xaver 1000 was developed by Israel's Camero Tech, which is the latest in the company's line of \"through-wall imaging systems.\" The Xaver line uses millimeter wave (MMW) radar, or radar in the range of 30-300 gigahertz. It is equipped with an AI-based life target tracking system as well as its own 3D 'sense-through-the-wall' technology. Internet of Battlefield Things\nThe Internet of Battlefield Things (IoBT) is a project initiated and executed by the U.S. Army Research Laboratory (ARL) that focuses on the basic science related to the IoT that enhances the capabilities of Army soldiers. In 2017, ARL launched the Internet of Battlefield Things Collaborative Research Alliance (IoBT-CRA), establishing a working collaboration between industry, university, and Army researchers to advance the theoretical foundations of IoT technologies and their applications to Army operations.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      }
    ]
  },
  {
    "question": "What is rendering in computer graphics?",
    "answer": "Producing a 2D image from a 3D scene.",
    "source_id": "Computer graphics",
    "url": "https://en.wikipedia.org/wiki/Computer_graphics",
    "category": "factual",
    "generated_answer": "generation of a 2D image from a 3D model by means of computer programs",
    "response_time": "9.72s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_graphics_18",
        "text": "The most effective models have generally been trained on massive amounts of image and text data scraped from the web. By 2022, the best of these models, for example Dall-E 2 and Stable Diffusion, are ...",
        "full_text": "The most effective models have generally been trained on massive amounts of image and text data scraped from the web. By 2022, the best of these models, for example Dall-E 2 and Stable Diffusion, are able to create images in a range of styles, ranging from imitations of living artists to near-photorealistic, in a matter of seconds, given powerful enough hardware. Three-dimensional\n3D graphics, compared to 2D graphics, are graphics that use a three-dimensional representation of geometric data. For the purpose of performance, this is stored in the computer. This includes images that may be for later display or for real-time viewing. Despite these differences, 3D computer graphics rely on similar algorithms as 2D computer graphics do in the frame and raster graphics (like in 2D) in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques. 3D computer graphics are the same as 3D models. The model is contained within the graphical data file, apart from the rendering. However, there are differences that include the 3D model being the representation of any 3D object. Until visually displayed, a model is not graphic. Due to printing, 3D models are not only confined to virtual space. 3D rendering is how a model can be displayed. Also can be used in non-graphical computer simulations and calculations. Computer animation\nComputer animation is the art of creating moving images via the use of computers. It is a subfield of computer graphics and animation. Increasingly it is created by means of 3D computer graphics, though 2D computer graphics are still widely used for stylistic, low bandwidth, and faster real-time rendering needs. Sometimes the target of the animation is the computer itself, but sometimes the target is another medium, such as film. It is also referred to as CGI (Computer-generated imagery or computer-generated imaging), especially when used in films.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_20",
        "text": "Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel ...",
        "full_text": "Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel typically has three subpixels such as red, green, and blue. Graphics are visual representations on a surface, such as a computer screen. Examples are photographs, drawing, graphics designs, maps, engineering drawings, or other images. Graphics often combine text and illustration. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style. Primitives\nPrimitives are basic units which a graphics system may combine to create more complex images or models. Examples would be sprites and character maps in 2D video games, geometric primitives in CAD, or polygons or triangles in 3D rendering. Primitives may be supported in hardware for efficient rendering, or the building blocks provided by a graphics application. Rendering\nRendering is the generation of a 2D image from a 3D model by means of computer programs. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texturing, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The rendering program is usually built into the computer graphics software, though others are available as plug-ins or entirely separate programs. The term \"rendering\" may be by analogy with an \"artist's rendering\" of a scene. Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 3,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_10",
        "text": "Modern computers, dating from the 1980s, often use graphical user interfaces (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key ...",
        "full_text": "Modern computers, dating from the 1980s, often use graphical user interfaces (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key elements of multimedia technology. In the field of realistic rendering, Japan's Osaka University developed the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, in 1982, for the purpose of rendering realistic 3D computer graphics. According to the Information Processing Society of Japan: \"The core of 3D image rendering is calculating the luminance of each pixel making up a rendered surface from the given viewpoint, light source, and object position. The LINKS-1 system was developed to realize an image rendering methodology in which each pixel could be parallel processed independently using ray tracing. By developing a new software methodology specifically for high-speed image rendering, LINKS-1 was able to rapidly render highly realistic images.\" The LINKS-1 was the world's most powerful computer, as of 1984. Also in the field of realistic rendering, the general rendering equation of David Immel and James Kajiya was developed in 1986 – an important step towards implementing global illumination, which is necessary to pursue photorealism in computer graphics. The continuing popularity of Star Wars and other science fiction franchises were relevant in cinematic CGI at this time, as Lucasfilm and Industrial Light & Magic became known as the \"go-to\" house by many other studios for topnotch computer graphics in film. Important advances in chroma keying (\"bluescreening\", etc.) were made for the later films of the original trilogy. Two other pieces of video would also outlast the era as historically relevant: Dire Straits' iconic, near-fully-CGI video for their song \"Money for Nothing\" in 1985, which popularized CGI among music fans of that era, and a scene from Young Sherlock Holmes the same year featuring the first fully CGI character in a feature movie (an animated stained-glass knight).",
        "rrf_score": 0.03125763125763126,
        "dense_rank": 5,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_16",
        "text": "In cinema, most animated movies are CGI now; many animated CGI films are made per year, but few, if any, attempt photorealism due to continuing fears of the uncanny valley. Most are 3D cartoons. In vi...",
        "full_text": "In cinema, most animated movies are CGI now; many animated CGI films are made per year, but few, if any, attempt photorealism due to continuing fears of the uncanny valley. Most are 3D cartoons. In videogames, the Microsoft Xbox One, Sony PlayStation 4, and Nintendo Switch dominated the home space and were all capable of advanced 3D graphics; Windows was still one of the most active gaming platforms as well. 2020s\nIn the 2020s', advances in ray-tracing technology allowed it to be used for real-time rendering, as well as AI-powered graphics for generating or upscaling frames. While ray-tracing existed before, Nvidia was the first to push for ray-tracing with ray-tracing cores, as well as for AI with DLSS and Tensor cores. AMD followed suit with the same; FSR, Tensor cores and ray-tracing cores. Image types\nTwo-dimensional\n2D computer graphics are the computer-based generation of digital images—mostly from models, such as digital image, and by techniques specific to them. 2D computer graphics are mainly used in applications that were originally developed upon traditional printing and drawing technologies such as typography. In those applications, the two-dimensional image is not just a representation of a real-world object, but an independent artifact with added semantic value; two-dimensional models are therefore preferred because they give more direct control of the image than 3D computer graphics, whose approach is more akin to photography than to typography. Pixel art\nA large form of digital art, pixel art is created through the use of raster graphics software, where images are edited on the pixel level. Graphics in most old (or relatively limited) computer and video games, graphing calculator games, and many mobile phone games are mostly pixel art. Sprite graphics\nA sprite is a two-dimensional image or animation that is integrated into a larger scene. Initially including just graphical objects handled separately from the memory bitmap of a video display, this now includes various manners of graphical overlays.",
        "rrf_score": 0.030536130536130537,
        "dense_rank": 6,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_21",
        "text": "Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeli...",
        "full_text": "Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU. A GPU is a device able to assist the CPU in calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the rendering equation. The rendering equation does not account for all lighting phenomena, but is a general lighting model for computer-generated imagery. 'Rendering' is also used to describe the process of calculating effects in a video editing file to produce final video output. 3D projection\n3D projection is a method of mapping three dimensional points to a two dimensional plane. As most current methods for displaying graphical data are based on planar two dimensional media, the use of this type of projection is widespread. This method is used in most real-time 3D applications and typically uses rasterization to produce the final image. Ray tracing\n\nRay tracing is a technique from the family of image order algorithms for generating an image by tracing the path of light through pixels in an image plane. The technique is capable of producing a high degree of photorealism; usually higher than that of typical scanline rendering methods, but at a greater computational cost. Shading\n\nShading refers to depicting depth in 3D models or illustrations by varying levels of darkness. It is a process used in drawing for depicting levels of darkness on paper by applying media more densely or with a darker shade for darker areas, and less densely or with a lighter shade for lighter areas. There are various techniques of shading including cross hatching where perpendicular lines of varying closeness are drawn in a grid pattern to shade an area. The closer the lines are together, the darker the area appears. Likewise, the farther apart the lines are, the lighter the area appears. The term has been recently generalized to mean that shaders are applied. Texture mapping\nTexture mapping is a method for adding detail, surface texture, or colour to a computer-generated graphic or 3D model.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      }
    ]
  },
  {
    "question": "What is a transformer model in AI known for?",
    "answer": "Accelerating progress in generative AI and language modeling.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "unanswerable",
    "response_time": "9.76s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_1",
        "text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of ...",
        "full_text": "Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. Goals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. Reasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics. Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem. Knowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. A knowledge base is a body of knowledge represented in a form that can be used by a program.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_11",
        "text": "In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\". In September 2025, a review of surveys of scientists and industry e...",
        "full_text": "In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\". In September 2025, a review of surveys of scientists and industry experts from the last 15 years reported that most agreed that artificial general intelligence (AGI) will occur before the year 2100. A more recent analysis by AIMultiple reported that, “Current surveys of AI researchers are predicting AGI around 2040”. Whole brain emulation\nWhile the development of transformer models like in ChatGPT is considered the most promising path to AGI, whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain. Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it. Early estimates\nFor low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 4,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_25",
        "text": "SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number...",
        "full_text": "SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts. Bad actors and weaponized AI\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states. A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots. AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.",
        "rrf_score": 0.030834914611005692,
        "dense_rank": 2,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "What kind of tasks can large language models perform?",
    "answer": "Reading, writing, translating, and summarizing text.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "unanswerable",
    "response_time": "10.05s",
    "fused_chunks": [
      {
        "chunk_id": "Natural_language_processing_2",
        "text": "This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical unde...",
        "full_text": "This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, many systems relied on corpora that were specifically developed for the tasks they were designed to perform. This reliance has been a major limitation to their broader effectiveness and continues to affect similar systems. Consequently, significant research has focused on methods for learning effectively from limited amounts of data. 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, large quantities of non-annotated data are available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical. 2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 1,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Natural_language_processing_11",
        "text": "Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2...",
        "full_text": "Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications. Logic translation\nTranslate a text from a natural language into formal logic. Machine translation (MT)\nAutomatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly. Natural language understanding (NLU)\nConvert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization. Natural language generation (NLG):\nConvert information from computer databases or semantic intents into readable human language. Book generation\nNot an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models.",
        "rrf_score": 0.030330882352941176,
        "dense_rank": 4,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Natural_language_processing_12",
        "text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantic...",
        "full_text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Document AI\nA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants. Dialogue management\nComputer systems intended to converse with a human. Question answering\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada? \"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation\nGiven a description of an image, generate an image that matches the description. Text-to-scene generation\nGiven a description of a scene, generate a 3D model of the scene. Text-to-video\nGiven a description of a video, generate a video that matches the description. General tendencies and (possible) future directions\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:\n\nInterest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "What is a recommender system used for?",
    "answer": "Suggesting content or products to users.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "advice and help to do their business",
    "response_time": "9.33s",
    "fused_chunks": [
      {
        "chunk_id": "Algorithm_0",
        "text": "In mathematics and computer science, an algorithm ( ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Al...",
        "full_text": "In mathematics and computer science, an algorithm ( ) is a finite sequence of mathematically rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning). In contrast, a heuristic is an approach to solving problems without well-defined correct or optimal results. For example, although social media recommender systems are commonly called \"algorithms\", they actually rely on heuristics as there is no truly \"correct\" recommendation. As an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input. Etymology\nAround 825 AD, Persian scientist and polymath Muḥammad ibn Mūsā al-Khwārizmī wrote kitāb al-ḥisāb al-hindī (\"Book of Indian computation\") and kitab al-jam' wa'l-tafriq al-ḥisāb al-hindī (\"Addition and subtraction in Indian arithmetic\"). In the early 12th century, Latin translations of these texts involving the Hindu–Arabic numeral system and arithmetic appeared, for example Liber Alghoarismi de practica arismetrice, attributed to John of Seville, and Liber Algorismi de numero Indorum, attributed to Adelard of Bath. Here, alghoarismi or algorismi is the Latinization of Al-Khwarizmi's name; the text starts with the phrase Dixit Algorismi, or \"Thus spoke Al-Khwarizmi\". The word algorism in English came to mean the use of place-value notation in calculations; it occurs in the Ancrene Wisse from circa 1225.",
        "rrf_score": 0.0315136476426799,
        "dense_rank": 5,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Algorithm",
        "title": "Algorithm"
      },
      {
        "chunk_id": "Database_25",
        "text": "Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more ...",
        "full_text": "Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier? \", or \"if a product is sold with two different forms of packaging, are those the same product or different products? \", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes. Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data. Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Database",
        "title": "Database"
      },
      {
        "chunk_id": "Consumer_behavior_40",
        "text": "The three categories are: \"trackers\", \"hunters\", and \"explorers\". \"Trackers\" are the online consumers who are exactly looking for a product that they definitely wish to buy. They use the Internet for ...",
        "full_text": "The three categories are: \"trackers\", \"hunters\", and \"explorers\". \"Trackers\" are the online consumers who are exactly looking for a product that they definitely wish to buy. They use the Internet for the information about its price, delivery methods, post-purchase service, and so on. Once they have found the proper information, little effort is needed to let them do the business. \"Hunters\" just know the categories of the product that they need, for instance, a novel for leisure time. However, they have not made specific decision on whose novel to buy. They use the Internet to find a list of product of their needed categories to make comparison. This kind of online consumer needs advice and help to do their business. \"Explorers\" do not even have the categories of product on their minds. In fact, they just want to buy something online. There is more uncertainty of this type of online consumers. Influence of the Internet on buying process\nAs the preceding table shows, the first row indicates the process of a consumer buying a new product, while the second and third row illustrates the positive influences the Internet could have on buying process by creating effective communications with online consumers. For example, suppose a consumer carelessly sees an advertisement about laptops on Wechat, a popular Chinese social media developed by Tencent. He begins to feel that his laptop is a bit out of date and wants to buy a new one, which is the outcome of good advertisement placed on a daily Internet tool. He does not know anything about how to buy a new one as business changes so fast today, so he searches on Google to find an answer. On the results page he finds promotional ads which mainly come from JD.com and Taobao, two competing Chinese online retailers in this field. He prefers to use JD.com, which provides detailed comparisons of brands, prices, locations, and methods of payment and delivery. After careful selection, he makes his order through JD.com via Wechat payment.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Consumer_behavior",
        "title": "Consumer behavior"
      },
      {
        "chunk_id": "Artificial_intelligence_8",
        "text": "Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and pe...",
        "full_text": "Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). Classifiers and statistical learning methods\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience. There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. The naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability. Neural networks are also used as classifiers. Artificial neural networks\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Social_media_22",
        "text": "European Union\nThe European Union initially took a similar approach. However, in 2020, the European Commission presented two legislative proposals: The Digital Services Act (DSA) and the Digital Marke...",
        "full_text": "European Union\nThe European Union initially took a similar approach. However, in 2020, the European Commission presented two legislative proposals: The Digital Services Act (DSA) and the Digital Markets Act (DMA). Both proposals were enacted in July 2022. The DSA entered into force on 17 February 2024, the DMA in March 2024. This legislation can be summarized in the following four objectives, articulated by MEPs:\n\n\"What is illegal offline must also be illegal online\". \"Very large online platforms\" must therefore, among other things\ndelete illegal content (propaganda, election interference, hate crimes and online harms such as harassment and child abuse) and better protect fundamental rights\nredesign their systems to ensure a \"high level of privacy, security and protection of minors\", by prohibiting advertising based on personal data, designing recommender systems to minimize risks for children and demonstrating this to the European Commission via a risk assessment, and\nnot use sensitive personal data such as race, gender and religion to target advertising. Violators could face a complete ban in Europe or fines of up to 6% of global sales. Such content moderation requires extensive investment by platform providers. Enforcement resources may not be sufficient to ensure compliance. The DSA allows a country to require information to be deleted that is illegal only in that jurisdiction. According to Patrick Breyer from the German Pirate Party, a problem could arise from the Hungarian government requesting a video to be deleted that is critical of Victor Orban, as he foresaw the potential for such determinations to be applied EU-wide. Discussions and proposals\nGeneral\n2018 Nobel Laureate Paul Romer advocated taxing negative externalities of social media platforms. Similar to a carbon tax – negative social effects could be compensated for by a financial levy on the platforms. Assuming that the tax did not deter the actions that produced the externalities, the revenue raised could be used to address them. However, consensus has yet to emerge on how to measure or mitigate the harms, nor to craft a tax, . Another proposal is to invoke competition law.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Social_media",
        "title": "Social media"
      }
    ]
  },
  {
    "question": "What is the goal of reinforcement learning agents?",
    "answer": "To maximize cumulative reward.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "factual",
    "generated_answer": "to maximise some notion of cumulative reward",
    "response_time": "9.72s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_3",
        "text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent m...",
        "full_text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. Learning\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Machine_learning_8",
        "text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the...",
        "full_text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Other types\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. Self-learning\nSelf-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 4,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.030117753623188408,
        "dense_rank": 9,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_intelligence_4",
        "text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other gui...",
        "full_text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Creativity_14",
        "text": "Computational creativity\nJürgen Schmidhuber's formal theory of creativity postulates that creativity, curiosity, and interestingness are by-products of a simple computational principle for measuring a...",
        "full_text": "Computational creativity\nJürgen Schmidhuber's formal theory of creativity postulates that creativity, curiosity, and interestingness are by-products of a simple computational principle for measuring and optimizing learning progress. Consider an agent able to manipulate its environment and thus its own sensory inputs. The agent can use a black box optimization method such as reinforcement learning to learn, through informed trial and error, sequences of actions that maximize the expected sum of its future reward signals. There are extrinsic reward signals for achieving externally given goals, such as finding food when hungry. But for Schmidhuber's objective function to be maximized also includes an additional, intrinsic term to model \"wow-effects\". This non-standard term motivates purely creative behavior of the agent, even when there are no external goals. A wow-effect is formally defined as follows: as the agent is creating and predicting and encoding the continually growing history of actions and sensory inputs, it keeps improving the predictor or encoder, which can be implemented as an artificial neural network, or some other machine learning device, that can exploit regularities in the data to improve its performance over time. The improvements can be measured precisely, by computing the difference in computational costs (storage size, number of required synapses, errors, time) needed to encode new observations before and after learning. This difference depends on the encoder's present subjective knowledge, which changes over time, but the theory formally takes this into account. The cost difference measures the strength of the present wow-effect due to sudden improvements in data compression or computational speed. It becomes an intrinsic reward signal for the action selector. The objective function thus motivates the action optimizer to create action sequences that cause more wow-effects. Irregular, random data (or noise) do not permit any wow-effects or learning progress, and thus are \"boring\" by nature (providing no reward). Already-known and predictable regularities also are boring. Temporarily interesting are only the initially unknown, novel, regular patterns in both actions and observations. This motivates the agent to perform continual, open-ended, active, creative exploration.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Creativity",
        "title": "Creativity"
      }
    ]
  },
  {
    "question": "What does computer graphics study?",
    "answer": "Methods for digitally synthesizing and manipulating visual content.",
    "source_id": "Computer graphics",
    "url": "https://en.wikipedia.org/wiki/Computer_graphics",
    "category": "factual",
    "generated_answer": "methods for digitally synthesizing and manipulating",
    "response_time": "8.99s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_graphics_0",
        "text": "Computer graphics deals with generating images and art with the aid of computers. Computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer...",
        "full_text": "Computer graphics deals with generating images and art with the aid of computers. Computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research. Computer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. It is also used for processing image data received from the physical world, such as photo and video content. Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, and video games in general. Overview\nThe term computer graphics has been used in a broad sense to describe \"almost everything on computers that is not text or sound\". Typically, the term computer graphics refers to several different things:\n\nthe representation and manipulation of image data by a computer\nthe various technologies used to create and manipulate images\nmethods for digitally synthesizing and manipulating visual content, see study of computer graphics\nToday, computer graphics is widespread. Such imagery is found in and on television, newspapers, weather reports, and in a variety of medical investigations and surgical procedures. A well-constructed graph can present complex statistics in a form that is easier to understand and interpret. In the media \"such graphs are used to illustrate papers, reports, theses\", and other presentation material. Many tools have been developed to visualize data. Computer-generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_18",
        "text": "The most effective models have generally been trained on massive amounts of image and text data scraped from the web. By 2022, the best of these models, for example Dall-E 2 and Stable Diffusion, are ...",
        "full_text": "The most effective models have generally been trained on massive amounts of image and text data scraped from the web. By 2022, the best of these models, for example Dall-E 2 and Stable Diffusion, are able to create images in a range of styles, ranging from imitations of living artists to near-photorealistic, in a matter of seconds, given powerful enough hardware. Three-dimensional\n3D graphics, compared to 2D graphics, are graphics that use a three-dimensional representation of geometric data. For the purpose of performance, this is stored in the computer. This includes images that may be for later display or for real-time viewing. Despite these differences, 3D computer graphics rely on similar algorithms as 2D computer graphics do in the frame and raster graphics (like in 2D) in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques. 3D computer graphics are the same as 3D models. The model is contained within the graphical data file, apart from the rendering. However, there are differences that include the 3D model being the representation of any 3D object. Until visually displayed, a model is not graphic. Due to printing, 3D models are not only confined to virtual space. 3D rendering is how a model can be displayed. Also can be used in non-graphical computer simulations and calculations. Computer animation\nComputer animation is the art of creating moving images via the use of computers. It is a subfield of computer graphics and animation. Increasingly it is created by means of 3D computer graphics, though 2D computer graphics are still widely used for stylistic, low bandwidth, and faster real-time rendering needs. Sometimes the target of the animation is the computer itself, but sometimes the target is another medium, such as film. It is also referred to as CGI (Computer-generated imagery or computer-generated imaging), especially when used in films.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 2,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_26",
        "text": "Computer Graphics for Java Programmers, Second Edition, John-Wiley & Sons, ISBN 978-0-470-03160-5. David Rogers (1998). Procedural Elements for Computer Graphics. McGraw-Hill. James D. Foley, Andries ...",
        "full_text": "Computer Graphics for Java Programmers, Second Edition, John-Wiley & Sons, ISBN 978-0-470-03160-5. David Rogers (1998). Procedural Elements for Computer Graphics. McGraw-Hill. James D. Foley, Andries Van Dam, Steven K. Feiner and John F. Hughes (1995). Computer Graphics: Principles and Practice. Addison-Wesley. Donald Hearn and M. Pauline Baker (1994). Computer Graphics. Prentice-Hall. Francis S. Hill (2001). Computer Graphics. Prentice Hall. John Lewell (1985). Computer Graphics: A Survey of Current Techniques and Applications. Van Nostrand Reinhold. Jeffrey J. McConnell (2006). Computer Graphics: Theory Into Practice. Jones & Bartlett Publishers. R. D. Parslow, R. W. Prowse, Richard Elliot Green (1969). Computer Graphics: Techniques and Applications. Peter Shirley and others. (2005). Fundamentals of computer graphics. A.K. Peters, Ltd.\nM. Slater, A. Steed, Y. Chrysantho (2002). Computer graphics and virtual environments: from realism to real-time. Addison-Wesley. Wolfgang Höhl (2008): Interactive environments with open-source software, Springer Wien New York, ISBN 3-211-79169-8\n\nExternal links\n\nA Critical History of Computer Graphics and Animation (archived 5 April 2007)\nHistory of Computer Graphics series of articles (archived 2 March 2007)\nComputer Graphics research at UC Berkeley\nThomas Dreher: History of Computer Art, chap. IV.2 Computer Animation\nHistory of Computer Graphics on RUS",
        "rrf_score": 0.03125763125763126,
        "dense_rank": 5,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_1",
        "text": "Many tools have been developed to visualize data. Computer-generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As te...",
        "full_text": "Many tools have been developed to visualize data. Computer-generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As technology has improved, 3D computer graphics have become more common, but 2D computer graphics are still widely used. Computer graphics has emerged as a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Over the past decade, other specialized fields have been developed like information visualization, and scientific visualization more concerned with \"the visualization of three dimensional phenomena (architectural, meteorological, medical, biological, etc. ), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component\". History\nThe precursor sciences to the development of modern computer graphics were the advances in electrical engineering, electronics, and television that took place during the first half of the twentieth century. Screens could display art since the Lumiere brothers' use of mattes to create special effects for the earliest films dating from 1895, but such displays were limited and not interactive. The first cathode ray tube, the Braun tube, was invented in 1897 – it in turn would permit the oscilloscope and the military control panel – the more direct precursors of the field, as they provided the first two-dimensional electronic displays that responded to programmatic or user input. Nevertheless, computer graphics remained relatively unknown as a discipline until the 1950s and the post-World War II period – during which time the discipline emerged from a combination of both pure university and laboratory academic research into more advanced computers and the United States military's further development of technologies like radar, aviation, and rocketry developed during the war. New kinds of displays were needed to process the wealth of information resulting from such projects, leading to the development of computer graphics as a discipline. 1950s\nEarly projects like the Whirlwind and SAGE Projects introduced the CRT as a viable display and interaction interface and introduced the light pen as an input device.",
        "rrf_score": 0.029418126757516764,
        "dense_rank": 9,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_25",
        "text": "He developed the Bresenham's line algorithm, his most well-known invention in 1962, and the midpoint circle algorithm. He retired from 27 years of service at IBM as a Senior Technical Staff Member, ta...",
        "full_text": "He developed the Bresenham's line algorithm, his most well-known invention in 1962, and the midpoint circle algorithm. He retired from 27 years of service at IBM as a Senior Technical Staff Member, taught for 16 years at Winthrop University and has nine patents. Other pioneers\nPierre Bézier\nJim Blinn\nJohn Carmack\nPaul de Casteljau\nEd Catmull\nFrank Crow\nJames D. Foley\nWilliam Fetter\nHenry Fuchs\nHenri Gouraud\nCharles Loop\nNadia Magnenat Thalmann\nBenoit Mandelbrot\nMartin Newell\nFred Parke\nBui Tuong Phong\nSteve Russell\nDaniel J. Sandin\nAlvy Ray Smith\nBob Sproull\nIvan Sutherland\nDaniel Thalmann\nJohnson K. Yan\nAndries van Dam\nJohn Warnock\nJ. Turner Whitted\nLance Williams\nJim Kajiya\nJames H. Clark\n\nOrganizations\nSIGGRAPH\nGDC\nBell Telephone Laboratories\nLink Flight Simulation, division of Singer Company\nUnited States Armed Forces, particularly the Whirlwind computer and SAGE Project\nBoeing\nEurographics\nEvans and Sutherland\nIBM\nRenault\nNYIT\nThe computer science department of the University of Utah\nLucasfilm and Industrial Light & Magic\nAutodesk\nAdobe Systems\nPixar\nSilicon Graphics, Khronos Group & OpenGL\nThe DirectX division at Microsoft\nNvidia\nAMD (ATI Technologies)\n\nStudy of computer graphics\nThe study of computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing. As an academic discipline, computer graphics studies the manipulation of visual and geometric information using computational techniques. It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues. Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities. Applications\nComputer graphics may be used in the following areas:\n\nAugmented reality\nComputational biology\nComputational photography\nComputational physics\nComputer-aided design\nComputing\nDesign\nDigital art\nEducation\nExtended reality\nGraphic design\nInfographics\nInformation visualization\nMixed reality\nMotion graphics\nRational drug design\nSimulation\nScientific visualization\nSpecial effects for cinema\nVideo games\nVirtual reality\nWeb3D\n\nSee also\nComputer representation of surfaces\nGlossary of computer graphics\n\nNotes\nReferences\nFurther reading\nL. Ammeraal and K. Zhang (2007). Computer Graphics for Java Programmers, Second Edition, John-Wiley & Sons, ISBN 978-0-470-03160-5. David Rogers (1998). Procedural Elements for Computer Graphics. McGraw-Hill.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      }
    ]
  },
  {
    "question": "Which focuses on language: NLP or computer vision?",
    "answer": "Natural language processing (NLP).",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "comparative",
    "generated_answer": "Natural language processing (NLP)",
    "response_time": "7.96s",
    "fused_chunks": [
      {
        "chunk_id": "Natural_language_processing_0",
        "text": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is ...",
        "full_text": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly. Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation. History\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. Symbolic NLP (1950s – early 1990s)\nThe premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. 1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed. 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 1,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Computer_vision_4",
        "text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how...",
        "full_text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry. Distinctions\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted:\n\nImage processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images.",
        "rrf_score": 0.03076923076923077,
        "dense_rank": 5,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Natural_language_processing_12",
        "text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantic...",
        "full_text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Document AI\nA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants. Dialogue management\nComputer systems intended to converse with a human. Question answering\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada? \"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation\nGiven a description of an image, generate an image that matches the description. Text-to-scene generation\nGiven a description of a scene, generate a 3D model of the scene. Text-to-video\nGiven a description of a video, generate a video that matches the description. General tendencies and (possible) future directions\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:\n\nInterest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Natural_language_processing_13",
        "text": "Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, ...",
        "full_text": "Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\nElimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\n\nCognition\nMost higher-level NLP applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:\n\nApply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance. The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Computer_vision_5",
        "text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. Thi...",
        "full_text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology. Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      }
    ]
  },
  {
    "question": "Which is about visual data: computer vision or HCI?",
    "answer": "Computer vision.",
    "source_id": "Computer vision",
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "category": "comparative",
    "generated_answer": "HCI",
    "response_time": "5.91s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_vision_0",
        "text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical ...",
        "full_text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Definition\nComputer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 3,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_5",
        "text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. Thi...",
        "full_text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology. Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 1,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_4",
        "text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how...",
        "full_text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry. Distinctions\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted:\n\nImage processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images.",
        "rrf_score": 0.031054405392392875,
        "dense_rank": 2,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_3",
        "text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, s...",
        "full_text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields. Signal processing\nYet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. Robotic navigation\nRobot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n\nVisual computing\nOther fields\nBesides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view.",
        "rrf_score": 0.03076923076923077,
        "dense_rank": 5,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_7",
        "text": "An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. I...",
        "full_text": "An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans—ultrasonic images or X-ray images, for example—to reduce the influence of noise. Machine vision\nA second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting. Military\nThe obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability. Autonomous vehicles\nOne of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV).",
        "rrf_score": 0.029857397504456328,
        "dense_rank": 6,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      }
    ]
  },
  {
    "question": "Which relies on rewards: reinforcement learning or clustering?",
    "answer": "Reinforcement learning.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "comparative",
    "generated_answer": "Reinforcement",
    "response_time": "7.56s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_7",
        "text": "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in...",
        "full_text": "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself. Dimensionality reduction\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation. Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data).",
        "rrf_score": 0.030117753623188408,
        "dense_rank": 4,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_8",
        "text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the...",
        "full_text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Other types\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. Self-learning\nSelf-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_1",
        "text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early...",
        "full_text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think? \", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\". Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms. Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Creativity_14",
        "text": "Computational creativity\nJürgen Schmidhuber's formal theory of creativity postulates that creativity, curiosity, and interestingness are by-products of a simple computational principle for measuring a...",
        "full_text": "Computational creativity\nJürgen Schmidhuber's formal theory of creativity postulates that creativity, curiosity, and interestingness are by-products of a simple computational principle for measuring and optimizing learning progress. Consider an agent able to manipulate its environment and thus its own sensory inputs. The agent can use a black box optimization method such as reinforcement learning to learn, through informed trial and error, sequences of actions that maximize the expected sum of its future reward signals. There are extrinsic reward signals for achieving externally given goals, such as finding food when hungry. But for Schmidhuber's objective function to be maximized also includes an additional, intrinsic term to model \"wow-effects\". This non-standard term motivates purely creative behavior of the agent, even when there are no external goals. A wow-effect is formally defined as follows: as the agent is creating and predicting and encoding the continually growing history of actions and sensory inputs, it keeps improving the predictor or encoder, which can be implemented as an artificial neural network, or some other machine learning device, that can exploit regularities in the data to improve its performance over time. The improvements can be measured precisely, by computing the difference in computational costs (storage size, number of required synapses, errors, time) needed to encode new observations before and after learning. This difference depends on the encoder's present subjective knowledge, which changes over time, but the theory formally takes this into account. The cost difference measures the strength of the present wow-effect due to sudden improvements in data compression or computational speed. It becomes an intrinsic reward signal for the action selector. The objective function thus motivates the action optimizer to create action sequences that cause more wow-effects. Irregular, random data (or noise) do not permit any wow-effects or learning progress, and thus are \"boring\" by nature (providing no reward). Already-known and predictable regularities also are boring. Temporarily interesting are only the initially unknown, novel, regular patterns in both actions and observations. This motivates the agent to perform continual, open-ended, active, creative exploration.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Creativity",
        "title": "Creativity"
      },
      {
        "chunk_id": "Behaviorism_7",
        "text": "The treat is the positive reinforcement to the behavior of sitting. The key to making positive reinforcement effect is to reward the behavior immediately. Negative reinforcement: Increases the frequen...",
        "full_text": "The treat is the positive reinforcement to the behavior of sitting. The key to making positive reinforcement effect is to reward the behavior immediately. Negative reinforcement: Increases the frequency of a behavior, but the behavior results from removing unpleasant or unwanted stimulus. For example, a child hates being nagged (negative) to clean his room (behavior) which increases the frequency of the child cleaning his room to prevent his mother from nagging. Another example would be putting on sunscreen (behavior) before going outside to prevent sunburn (negative). Positive punishment: Providing a stimulus that an individual does not desire to decrease undesired behaviors. For example, if a child engages in an undesired behavior, then parents may spank (stimulus) the child to correct their behavior. Negative punishment: Removing a stimulus that an individual desires in order to decrease undesired behaviors. An example of this would be grounding a child for failing a test. Grounding in this example is taking away the child's ability to play video games. As long as it is clear that the ability to play video games was taken away because they failed a test, this is negative punishment. The key here is the connection to the behavior and the result of the behavior. A classical experiment in operant conditioning, for example, is the Skinner Box, \"puzzle box\" or operant conditioning chamber to test the effects of operant conditioning principles on rats, cats and other species. From this experiment, he discovered that the rats learned very effectively if they were rewarded frequently with food. Skinner also found that he could shape (create new behavior) the rats' behavior through the use of rewards, which could, in turn, be applied to human learning as well. Skinner's model was based on the premise that reinforcement is used for the desired actions or responses while punishment was used to stop the responses of the undesired actions that are not. This theory proved that humans or animals will repeat any action that leads to a positive outcome, and avoid any action that leads to a negative outcome.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Behaviorism",
        "title": "Behaviorism"
      }
    ]
  },
  {
    "question": "Which handles human–computer interfaces: HCI or IoT?",
    "answer": "Human–computer interaction (HCI).",
    "source_id": "Human–computer interaction",
    "url": "https://en.wikipedia.org/wiki/Human–computer_interaction",
    "category": "comparative",
    "generated_answer": "IoT",
    "response_time": "6.89s",
    "fused_chunks": [
      {
        "chunk_id": "Internet_of_things_19",
        "text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devi...",
        "full_text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users. Function\nIoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, shares information, monitors, navigates and groups with other IoT devices in the same or nearby network realizing SIoT and facilitating useful service compositions in order to help its users proactively in every day's life especially during emergency. Examples\nIoT-based smart home technology monitors health data of patients or aging adults by analyzing their physiological parameters and prompt the nearby health facilities when emergency medical services needed. In case emergency, automatically, ambulance of a nearest available hospital will be called with pickup location provided, ward assigned, patient's health data will be transmitted to the emergency department, and display on the doctor's computer immediately for further action. IoT sensors on the vehicles, road and traffic lights monitor the conditions of the vehicles and drivers and alert when attention needed and also coordinate themselves automatically to ensure autonomous driving is working normally. Unfortunately if an accident happens, IoT camera will inform the nearest hospital and police station for help. Challenges\nInternet of things is multifaceted and complicated. One of the main factors that hindering people from adopting and use Internet of things (IoT) based products and services is its complexity. Installation and setup is a challenge to people, therefore, there is a need for IoT devices to mix match and configure themselves automatically to provide different services at different situation. System security always a concern for any technology, and it is more crucial for SIoT as not only security of oneself need to be considered but also the mutual trust mechanism between collaborative IoT devices from time to time, from place to place. Another critical challenge for SIoT is the accuracy and reliability of the sensors.",
        "rrf_score": 0.03225806451612903,
        "dense_rank": 2,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Automation_13",
        "text": "The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry. Information technology, together with industrial machinery and processes, can assist in the ...",
        "full_text": "The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry. Information technology, together with industrial machinery and processes, can assist in the design, implementation, and monitoring of control systems. One example of an industrial control system is a programmable logic controller (PLC). PLCs are specialized hardened computers which are frequently used to synchronize the flow of inputs from (physical) sensors and events with the flow of outputs to actuators and events. Human-machine interfaces (HMI) or computer human interfaces (CHI), formerly known as man-machine interfaces, are usually employed to communicate with PLCs and other computers. Service personnel who monitor and control through HMIs can be called by different names. In the industrial process and manufacturing environments, they are called operators or something similar. In boiler houses and central utility departments, they are called stationary engineers. Different types of automation tools exist:\n\nANN – Artificial neural network\nDCS – Distributed control system\nHMI – Human machine interface\nRPA – Robotic process automation\nSCADA – Supervisory control and data acquisition\nPLC – Programmable logic controller\nInstrumentation\nMotion control\nRobotics\nHost simulation software (HSS) is a commonly used testing tool that is used to test the equipment software. HSS is used to test equipment performance concerning factory automation standards (timeouts, response time, processing time). Cognitive automation\nCognitive automation, as a subset of AI, is an emerging genus of automation enabled by cognitive computing. Its primary concern is the automation of clerical tasks and workflows that consist of structuring unstructured data. Cognitive automation relies on multiple disciplines: natural language processing, real-time computing, machine learning algorithms, big data analytics, and evidence-based learning. According to Deloitte, cognitive automation enables the replication of human tasks and judgment \"at rapid speeds and considerable scale.\" Such tasks include:\n\nDocument redaction\nData extraction and document synthesis / reporting\nContract management\nNatural language search\nCustomer, employee, and stakeholder onboarding\nManual activities and verifications\nFollow-up and email communications\n\nRecent and emerging applications\nCAD AI\nArtificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate in 3D modeling.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Automation",
        "title": "Automation"
      },
      {
        "chunk_id": "Internet_of_things_15",
        "text": "The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth. It includes various data...",
        "full_text": "The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth. It includes various database systems that store sensor data, such as time series databases or asset stores using backend data storage systems (e.g. Cassandra, PostgreSQL). The cloud tier in most cloud-based IoT system features event queuing and messaging system that handles communication that transpires in all tiers. Some experts classified the three-tiers in the IoT system as edge, platform, and enterprise and these are connected by proximity network, access network, and service network, respectively. Building on the Internet of things, the web of things is an architecture for the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called BPM Everywhere which is a blending of traditional process management with process mining and special capabilities to automate the control of large numbers of coordinated devices. Network architecture\nThe Internet of things requires huge scalability in the network space to handle the surge of devices. IETF 6LoWPAN can be used to connect devices to IP networks. With billions of devices being added to the Internet space, IPv6 will play a major role in handling the network layer scalability. IETF's Constrained Application Protocol, ZeroMQ, and MQTT can provide lightweight data transport. In practice many groups of IoT devices are hidden behind gateway nodes and may not have unique addresses. Also the vision of everything-interconnected is not needed for most applications as it is mainly the data which need interconnecting at a higher layer. Fog computing is a viable alternative to prevent such a large burst of data flow through the Internet. The edge devices' computation power to analyze and process data is extremely limited. Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_3",
        "text": "Applications\nThe extensive set of applications for IoT devices is often divided into consumer, commercial, industrial, and infrastructure spaces. Consumers\nA growing portion of IoT devices is created ...",
        "full_text": "Applications\nThe extensive set of applications for IoT devices is often divided into consumer, commercial, industrial, and infrastructure spaces. Consumers\nA growing portion of IoT devices is created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities. Home automation\nIoT devices are part of the broader concept of home automation, which generally includes lighting, heating and air conditioning, media and security systems, and camera systems. Moreover, long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage. A smart home, also known as an automated home, could be based on a platform or hubs that control smart devices and appliances. For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch. This could be a dedicated app or iOS native applications such as Siri. This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge. There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products. These include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub. In addition to the commercial systems, there are many non-proprietary, open source ecosystems, including Home Assistant, OpenHAB, and Domoticz. Elder care\nOne key application of a smart home is to assist the elderly and individuals with disabilities. These home systems use assistive technology to accommodate an owner's specific disabilities. Voice control can assist users with sight and mobility limitations while alert systems can be connected directly to cochlear implants worn by individuals with hearing impairments. They can also be equipped with additional safety features, including sensors that monitor for medical emergencies such as falls or seizures. Smart home technology applied in this way can provide users with more freedom and a higher quality of life.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_14",
        "text": "Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning te...",
        "full_text": "Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as regression, support vector machine, and random forest to advanced ones such as convolutional neural networks, LSTM, and variational autoencoder. In the future, the Internet of things may be a non-deterministic and open network in which auto-organized or intelligent entities (web services, SOA components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend, clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments. Architecture\nIoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud. Devices include networked things, such as the sensors and actuators found in IoT equipment, particularly those that use protocols such as Modbus, Bluetooth, Zigbee, or proprietary protocols, to connect to an Edge Gateway. The Edge Gateway layer consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or fog computing. Edge Gateway layer is also required to give a common view of the devices to the upper layers to facilitate in easier management. The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      }
    ]
  },
  {
    "question": "Which involves cross-border exchange: domestic trade or international trade?",
    "answer": "International trade.",
    "source_id": "International trade",
    "url": "https://en.wikipedia.org/wiki/International_trade",
    "category": "comparative",
    "generated_answer": "international trade",
    "response_time": "5.47s",
    "fused_chunks": [
      {
        "chunk_id": "International_trade_1",
        "text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally re...",
        "full_text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not. However, in practical terms, carrying out trade at an international level is typically a more complex process than domestic trade. The main difference is that international trade is typically more costly than domestic trade. This is due to the fact that cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural differences, product safety, the legal system, and so on. Another difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries. Thus, international trade is mostly restricted to trade in goods and services, and only to a lesser extent to trade in capital, labour, or other factors of production. Trade in goods and services can serve as a substitute for trade in factors of production. Instead of importing a factor of production, a country can import goods that make intensive use of that factor of production and thus embody it. An example of this is the import of labor-intensive goods by the United States from China. Instead of importing Chinese labor, the United States imports goods that were produced with Chinese labor. One report in 2010, suggested that international trade was increased when a country hosted a network of immigrants, but the trade effect was weakened when the immigrants became assimilated into their new country. History\nThe history of international trade chronicles notable events that have affected trading among various economies. Theories and models\nThere are several models that seek to explain the factors behind international trade, the welfare consequences of trade and the pattern of trade. Most traded export products\nLargest countries or regions by total international trade\nThe following table is a list of the 30 largest trading states according to the World Trade Organization in 2024.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "International_trade_0",
        "text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade repr...",
        "full_text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade represents a significant share of gross domestic product (GDP). While international trade has existed throughout history (for example Uttarapatha, Silk Road, Amber Road, salt roads), its economic, social, and political importance has been on the rise in recent centuries. Carrying out trade at an international level is a complex process when compared to domestic trade. When trade takes place between two or more states, factors like currency, government policies, economy, judicial system, laws, and markets influence trade. To ease and justify the process of trade between countries of different economic standing in the modern era, some international economic organizations were formed, such as the World Trade Organization. These organizations work towards the facilitation and growth of international trade. Statistical services of intergovernmental and supranational organizations and governmental statistical agencies publish official statistics on international trade. Characteristics of global trade\nA product that is transferred or sold from a party in one country to a party in another country is an export from the originating country, and an import to the country receiving that product. Imports and exports are accounted for in a country's current account in the balance of payments. Trading globally may give consumers and countries the opportunity to be exposed to new markets and  products. Almost every kind of product can be found in the international market, for example: food, clothes, spare parts, oil, jewellery, wine, stocks, currencies, and water. Services are also traded, such as in tourism, banking, consulting, and transportation. Advanced technology (including transportation), globalization, industrialization, outsourcing and multinational corporations have major impacts on the international trade systems. Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 2,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "Monetary_policy_20",
        "text": "This view rests on two implicit assumptions: a high responsiveness of import prices to the exchange rate, i.e. producer currency pricing (PCP), and frictionless international financial markets support...",
        "full_text": "This view rests on two implicit assumptions: a high responsiveness of import prices to the exchange rate, i.e. producer currency pricing (PCP), and frictionless international financial markets supporting the efficiency of flexible price allocation. The violation or distortion of these assumptions found in empirical research is the subject of a substantial part of the international optimal monetary policy literature. The policy trade-offs specific to this international perspective are threefold:\nFirst, research suggests only a weak reflection of exchange rate movements in import prices, lending credibility to the opposed theory of local currency pricing (LCP). The consequence is a departure from the classical view in the form of a trade-off between output gaps and misalignments in international relative prices, shifting monetary policy to CPI inflation control and real exchange rate stabilization. Second, another specificity of international optimal monetary policy is the issue of strategic interactions and competitive devaluations, which is due to cross-border spillovers in quantities and prices. Therein, the national authorities of different countries face incentives to manipulate the terms of trade to increase national welfare in the absence of international policy coordination. Even though the gains of international policy coordination might be small, such gains may become very relevant if balanced against incentives for international noncooperation. Third, open economies face policy trade-offs if asset market distortions prevent global efficient allocation. Even though the real exchange rate absorbs shocks in current and expected fundamentals, its adjustment does not necessarily result in a desirable allocation and may even exacerbate the misallocation of consumption and employment at both the domestic and global level. This is because, relative to the case of complete markets, both the Phillips curve and the loss function include a welfare-relevant measure of cross-country imbalances. Consequently, this results in domestic goals, e.g. output gaps or inflation, being traded-off against the stabilization of external variables such as the terms of trade or the demand gap. Hence, the optimal monetary policy in this case consists of redressing demand imbalances and/or correcting international relative prices at the cost of some inflation.",
        "rrf_score": 0.030886196246139225,
        "dense_rank": 9,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Monetary_policy",
        "title": "Monetary policy"
      },
      {
        "chunk_id": "Globalization_12",
        "text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never reco...",
        "full_text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never recovered since the Great Recession. New opposing views from some economists have argued such trends are a result of price drops and in actuality, trade volume is increasing, especially with agricultural products, natural resources and refined petroleum. The 21st century melting of the Arctic will also affect global trade, as it is paving the way for shorter trade routes. Economic globalization\nEconomic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital. Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market. Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes competition, technology, and corporations and industries. Current globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration. International standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations.",
        "rrf_score": 0.030090497737556562,
        "dense_rank": 8,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Globalization",
        "title": "Globalization"
      },
      {
        "chunk_id": "International_trade_5",
        "text": "In 2014, Mali's gold production was 45.8 tonnes, while the UAE's gold imports were 59.9 tonnes. Statistics\nList of countries by imports\nList of countries by exports\nList of countries by merchandise ex...",
        "full_text": "In 2014, Mali's gold production was 45.8 tonnes, while the UAE's gold imports were 59.9 tonnes. Statistics\nList of countries by imports\nList of countries by exports\nList of countries by merchandise exports\nList of countries by service exports and imports\nList of top exporting countries by product category\nList of countries by exports per capita\nList of countries by leading trade partners\nList of countries by net goods exports (Balance of trade)\nList of sovereign states by current account balance\nList of countries by tariff rate\n\nSee also\nReferences\nFurther reading\nHelpman, Elhanan (2011). Understanding Global Trade. Cambridge, Massachusetts: Harvard University Press. ISBN 978-0-674-06078-4. Nelson, Scott Reynolds (2022). Oceans of Grain: How American Wheat Remade the World. New York: Basic Books. ISBN 978-1-5416-4646-9. Linsi, Lukas; Burgoon, Brian; Mügge, Daniel K (2023). \"The Problem with Trade Measurement in International Relations\". International Studies Quarterly. 67 (2) sqad020. doi:10.1093/isq/sqad020. Sources\nExternal links\nData\nStatistics from intergovernmental sources\nData on the value of exports and imports and their quantities often broken down by detailed lists of products are available in statistical collections on international trade published by the statistical services of intergovernmental and supranational organisations and national statistical institutes. The definitions and methodological concepts applied for the various statistical collections on international trade often differ in terms of definition (e.g. special trade vs. general trade) and coverage (reporting thresholds, inclusion of trade in services, estimates for smuggled goods and cross-border provision of illegal services). Metadata providing information on definitions and methods are often published along with the data. United Nations Commodity Trade Database\nTrade Map, trade statistics for international business development\nWTO Statistics Portal\nStatistical Portal: OECD\nEuropean Union International Trade in Goods Data\nFood and Agricultural Trade Data Archived 2010-07-10 at the Wayback Machine by FAO\n\nOther external links\nThe MIT Observatory of Economic Complexity\nThe McGill Faculty of Law runs a Regional Trade Agreements Database that contains the text of almost all preferential and regional trade agreements in the world. ptas.mcgill.ca\nHistorical documents on international trade available on FRASER (St Louis Fed)",
        "rrf_score": 0.029877369007803793,
        "dense_rank": 5,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      }
    ]
  },
  {
    "question": "Why do large language models sometimes hallucinate?",
    "answer": "Because they generate text probabilistically and may produce false information.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "do language very well [but t]hey're not so good at thinking",
    "response_time": "9.05s",
    "fused_chunks": [
      {
        "chunk_id": "Language_42",
        "text": "These cases run counter to decades of linguistic theory, which had regarded language as fundamentally arbitrary... [Many words onomatopoeically imitate a sound. Also] there's the 'bouba-kiki' effect, ...",
        "full_text": "These cases run counter to decades of linguistic theory, which had regarded language as fundamentally arbitrary... [Many words onomatopoeically imitate a sound. Also] there's the 'bouba-kiki' effect, whereby people from varying cultures are more likely to associate the nonsense word 'bouba' with a rounded shape and 'kiki' with a spiked one.... [S]omehow we all have a feeling about this,' says Aleksandra Ćwiek... [She and her colleagues have] show[n] that people associate the trilled 'R' sound with roughness and the 'L' sound with smoothness. Mark Dingemanse... in 2013 found [that] the conversational 'Huh?' and similar words in other languages may be universal.\" (p. 18.) Stix, Gary, \"Thinking without Words: Cognition doesn't require language, it turns out\" (interview with Evelina Fedorenko, a cognitive neuroscientist at the Massachusetts Institute of Technology), Scientific American, vol. 332, no. 3 (March 2025), pp. 86–88. \"[I]n the tradition of linguist Noam Chomsky... we use language for thinking: to think is why language evolved in our species. [However, evidence that thought and language are separate systems is found, for example, by] looking at deficits in different abilities – for instance, in people with brain damage... who have impairments in language – some form of aphasia [ – yet are clearly able to think].\" (p. 87.) Conversely, \"large language models such as GPT-2... do language very well [but t]hey're not so good at thinking, which... nicely align[s] with the idea that the language system by itself is not what makes you think.\" (p. 88.) Swadesh, Morris (1934). \"The phonemic principle\". Language. 10 (2): 117–129. doi:10.2307/409603. JSTOR 409603.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Language",
        "title": "Language"
      },
      {
        "chunk_id": "Language_33",
        "text": "Change\nAll languages change as speakers adopt or invent new ways of speaking and pass them on to other members of their speech community. Language change happens at all levels from the phonological le...",
        "full_text": "Change\nAll languages change as speakers adopt or invent new ways of speaking and pass them on to other members of their speech community. Language change happens at all levels from the phonological level to the levels of vocabulary, morphology, syntax, and discourse. Even though language change is often initially evaluated negatively by speakers of the language who often consider changes to be \"decay\" or a sign of slipping norms of language usage, it is natural and inevitable. Changes may affect specific sounds or the entire phonological system. Sound change can consist of the replacement of one speech sound or phonetic feature by another, the complete loss of the affected sound, or even the introduction of a new sound in a place where there had been none. Sound changes can be conditioned in which case a sound is changed only if it occurs in the vicinity of certain other sounds. Sound change is usually assumed to be regular, which means that it is expected to apply mechanically whenever its structural conditions are met, irrespective of any non-phonological factors. On the other hand, sound changes can sometimes be sporadic, affecting only one particular word or a few words, without any seeming regularity. Sometimes a simple change triggers a chain shift in which the entire phonological system is affected. This happened in the Germanic languages when the sound change known as Grimm's law affected all the stop consonants in the system. The original consonant *bʰ became /b/ in the Germanic languages, the previous *b in turn became /p/, and the previous *p became /f/. The same process applied to all stop consonants and explains why Italic languages such as Latin have p in words like pater and pisces, whereas Germanic languages, like English, have father and fish. Another example is the Great Vowel Shift in English, which is the reason that the spelling of English vowels do not correspond well to their current pronunciation. This is because the vowel shift brought the already established orthography out of synchronization with pronunciation.",
        "rrf_score": 0.029411764705882353,
        "dense_rank": 8,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Language",
        "title": "Language"
      },
      {
        "chunk_id": "Language_12",
        "text": "In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior se...",
        "full_text": "In this way, neuroscientists in the 19th century discovered that two areas in the brain are crucially implicated in language processing. The first area is Wernicke's area, which is in the posterior section of the superior temporal gyrus in the dominant cerebral hemisphere. People with a lesion in this area of the brain develop receptive aphasia, a condition in which there is a major impairment of language comprehension, while speech retains a natural-sounding rhythm and a relatively normal sentence structure. The second area is Broca's area, in the posterior inferior frontal gyrus of the dominant hemisphere. People with a lesion to this area develop expressive aphasia, meaning that they know what they want to say, they just cannot get it out. They are typically able to understand what is being said to them, but unable to speak fluently. Other symptoms that may be present in expressive aphasia include problems with word repetition. The condition affects both spoken and written language. Those with this aphasia also exhibit ungrammatical speech and show inability to use syntactic information to determine the meaning of sentences. Both expressive and receptive aphasia also affect the use of sign language, in analogous ways to how they affect speech, with expressive aphasia causing signers to sign slowly and with incorrect grammar, whereas a signer with receptive aphasia will sign fluently, but make little sense to others and have difficulties comprehending others' signs. This shows that the impairment is specific to the ability to use language, not to the physiology used for speech production. With technological advances in the late 20th century, neurolinguists have also incorporated non-invasive techniques such as functional magnetic resonance imaging (fMRI) and electrophysiology to study language processing in individuals without impairments. Anatomy of speech\nSpoken language relies on human physical ability to produce sound, which is a longitudinal wave propagated through the air at a frequency capable of vibrating the ear drum. This ability depends on the physiology of the human speech organs. These organs consist of the lungs, the voice box (larynx), and the upper vocal tract – the throat, the mouth, and the nose.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Language",
        "title": "Language"
      },
      {
        "chunk_id": "Philosophy_3",
        "text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the ...",
        "full_text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: \"What can I know? \"; \"What should I do? \"; \"What may I hope? \"; and \"What is the human being?\" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as \"the midwife of the sciences\". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a \"rigorous science\" investigating essences.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Ethics_1",
        "text": "An influential development in the 20th century was the emergence of metaethics. Definition\nEthics, also called moral philosophy, is the study of moral phenomena. It is one of the main branches of phil...",
        "full_text": "An influential development in the 20th century was the emergence of metaethics. Definition\nEthics, also called moral philosophy, is the study of moral phenomena. It is one of the main branches of philosophy and investigates the nature of morality and the principles that govern the moral evaluation of conduct, character traits, and institutions. It examines what obligations people have, what behavior is right and wrong, and how to lead a good life. Some of its key questions are \"How should one live?\" and \"What gives meaning to life?\". In contemporary philosophy, ethics is usually divided into normative ethics, applied ethics, and metaethics. Morality concerns what people ought to do rather than what they actually do, what they want to do, or what social conventions require. As a rational and systematic field of inquiry, ethics studies practical reasons why people should act one way rather than another. Most ethical theories seek universal principles that express a general standpoint of what is objectively right and wrong. In a slightly different sense, the term ethics can also refer to individual ethical theories in the form of a rational system of moral principles, such as Aristotelian ethics, and to a moral code that certain societies, social groups, or professions follow, as in Protestant work ethic and medical ethics. The English word ethics has its roots in the Ancient Greek word êthos (ἦθος), meaning 'character' and 'personal disposition'. This word gave rise to the Ancient Greek word ēthikós (ἠθικός), which was translated into Latin as ethica and entered the English language in the 15th century through the Old French term éthique. The term morality originates in the Latin word moralis, meaning 'manners' and 'character'. It was introduced into the English language during the Middle English period through the Old French term moralité. The terms ethics and morality are usually used interchangeably but some philosophers distinguish between the two. According to one view, morality focuses on what moral obligations people have while ethics is broader and includes ideas about what is good and how to lead a meaningful life.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Ethics",
        "title": "Ethics"
      }
    ]
  },
  {
    "question": "Why are GPUs preferred for AI training?",
    "answer": "They perform parallel computations efficiently.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "increase in computer power (including the hundred-fold increase in speed by switching",
    "response_time": "8.86s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_25",
        "text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financi...",
        "full_text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financial incentives\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. Hardware\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. Tensor Processing Units (TPUs)\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Deep_learning_9",
        "text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision ...",
        "full_text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. Deep learning revolution\nThe deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning. A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "rrf_score": 0.030679156908665108,
        "dense_rank": 1,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_11",
        "text": "Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant. The transistor density in integrated cir...",
        "full_text": "Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant. The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang. Applications\nAI and machine learning technology is used in most of the essential applications of the 2020s, including:\nsearch engines (such as Google Search)\ntargeting online advertisements\nrecommendation systems (offered by Netflix, YouTube or Amazon) driving internet traffic\ntargeted advertising (AdSense, Facebook)\nvirtual assistants (such as Siri or Alexa)\nautonomous vehicles (including drones, ADAS and self-driving cars)\nautomatic language translation (Microsoft Translator, Google Translate)\nfacial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet)\nimage labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO). Health and medicine\nIt has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold. Games\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy!",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Deep_learning_14",
        "text": "This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that...",
        "full_text": "This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting. DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations. Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. Hardware\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI . OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months. Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      }
    ]
  },
  {
    "question": "Why can AI raise ethical concerns?",
    "answer": "Because it can impact privacy, fairness, and safety.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "AI sentience would give rise to concerns of welfare and legal protection",
    "response_time": "8.21s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_28",
        "text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, la...",
        "full_text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Geoffrey Hinton, Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 4,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_30",
        "text": "Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can...",
        "full_text": "Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses. Frameworks\nArtificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks. Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers. The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.",
        "rrf_score": 0.029850746268656716,
        "dense_rank": 7,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_15",
        "text": "In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such a...",
        "full_text": "In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals. Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights. Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue. Benefits\nAGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, inexpensive and personalized education. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society. AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life. Advancements in medicine and healthcare\nAGI would improve healthcare by making medical diagnostics faster, less expensive, and more accurate. AI-driven systems can analyse patient data and detect diseases at an early stage. This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse. AGI systems could also recommend personalised treatment plans based on genetics and medical history. Additionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's disease. In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_16",
        "text": "For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be u...",
        "full_text": "For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation. During the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages. Ethics\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning. Risks and harm\nPrivacy and copyright\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency. Sensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "Why is digital media influential in society?",
    "answer": "It enables widespread and fast information sharing.",
    "source_id": "Digital media",
    "url": "https://en.wikipedia.org/wiki/Digital_media",
    "category": "inferential",
    "generated_answer": "The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history",
    "response_time": "10.90s",
    "fused_chunks": [
      {
        "chunk_id": "Digital_media_0",
        "text": "In mass communication, digital media is any communication media that operates in conjunction with various encoded machine-readable data formats. Digital content can be created, viewed, distributed, mo...",
        "full_text": "In mass communication, digital media is any communication media that operates in conjunction with various encoded machine-readable data formats. Digital content can be created, viewed, distributed, modified, listened to, and preserved on a digital electronic device, including digital data storage media (in contrast to analog electronic media) and digital broadcasting. Digital is defined as any data represented by a series of digits, and media refers to methods of broadcasting or communicating this information. Together, digital media refers to mediums of digitized information broadcast through a screen and/or a speaker. This also includes text, audio, video, and graphics  are transmitted over the internet for consumption on digital devices. Digital media platforms, such as YouTube, Kick, and Twitch, accounted for viewership rates of 27.9 billion hours in 2020. A contributing factor to its part in what is commonly referred to as the digital revolution can be attributed to the use of interconnectivity. Examples\nExamples of digital media include software, digital images,web pages and websites, social media, digital data and databases, digital audio such as MP3, electronic documents and electronic books. Digital media often contrasts with print media, such as printed books, newspapers and magazines, and other traditional or analog media, such as photographic film, audio tapes or video tapes. Digital media has had a significantly broad and complex impact on society and culture. Combined with the Internet and personal computing, digital media has caused disruptive innovation in publishing, journalism, public relations, entertainment, education, commerce and politics. Digital media has also posed new challenges to copyright and intellectual property laws, fostering an open content movement in which content creators voluntarily give up some or all of their legal rights to their work. The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history, called the Information Age, perhaps leading to a paperless society in which all media are produced and consumed on computers.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 3,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_7",
        "text": "It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping a...",
        "full_text": "It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping and complex. Between 2000 and 2015, the print newspaper advertising revenue has fallen from $60 billion to a nearly $20 billion. Even one of the most popular days for papers, Sunday, has seen a 9% circulation decrease the lowest since 1945. In journalism, digital media and citizen journalism have led to the loss of thousands of jobs in print media and the bankruptcy of many major newspapers. But the rise of digital journalism has also created thousands of new jobs and specializations. E-books and self-publishing are changing the book industry, and digital textbooks and other media-inclusive curricula are changing primary and secondary education. In academia, digital media has led to a new form of scholarship, also called digital scholarship, making open access and open science possible thanks to the low cost of distribution. New fields of study have grown, such as digital humanities and digital history. It has changed the way libraries are used and their role in society. Every major media, communications and academic endeavor is facing a period of transition and uncertainty related to digital media. Often time the magazine or publisher have a Digital edition which can be referred to an electronic formatted version identical to the print version. There is a huge benefit to the publisher and cost, as half of traditional publishers' costs come from production, including raw materials, technical processing, and distribution. Since 2004, there has been a decrease in newspaper industry employment, with only about 40,000 people working in the workforce currently. Alliance of Audited Media & Publishers information during the 2008 recession, over 10% of print sales are diminished for certain magazines, with a hardship coming from only 75% of the sales advertisements as before. However, in 2018, major newspapers advertising revenue was 35% from digital ads. In contrast, mobile versions of newspapers and magazines came in second with a huge growth of 135%.",
        "rrf_score": 0.03128054740957967,
        "dense_rank": 2,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_1",
        "text": "The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history, called the Information Age, perhaps leading to a paperless society in whic...",
        "full_text": "The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history, called the Information Age, perhaps leading to a paperless society in which all media are produced and consumed on computers. However, challenges to a digital transition remain, including outdated copyright laws, censorship, the digital divide, and the spectre of a digital dark age, in which older media becomes inaccessible to new or upgraded information systems. Digital media has a significant, wide-ranging and complex impact on society and culture. Business model\nDigital media platforms like YouTube operate through a triple-product business model in which they provide information and entertainment (infotainment) to the public, often at no cost, while also capturing their attention and collecting user data to sell to advertisers. This business model aims to maximize consumer engagement on the platform. Paid Media\nPaid media refers to promotional channels that marketers pay to use, including traditional media (e.g., television, radio, print, and outdoor advertising) and online and digital media (e.g., paid search ads, web and social media display ads, mobile ads, and email marketing). This model compels businesses to develop sponsored media and then pay social media platforms like Instagram to show it to customers in their newsfeeds. These customers become exposed to paid media, sometimes referred to as promoted or sponsored posts. Owned Media\nOwned media refers to digital assets and channels that a company or individual controls and manages. This includes websites, social media profiles (e.g., Facebook), blogs, and any other content platforms owned and operated by the entity. An entity is the owner or controller of the channel, such as a business or an individual managing their online presence. Earned Media\nEarned media refers to public relations channels such as television, newspapers, blogs, or video sites that do not require direct payment or control by marketers but are included because viewers, readers, or users are interested in them.",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 5,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Sociology_26",
        "text": "Online communities may be studied statistically through network analysis or interpreted qualitatively through virtual ethnography. Moreover, organizational change is catalysed through new media, there...",
        "full_text": "Online communities may be studied statistically through network analysis or interpreted qualitatively through virtual ethnography. Moreover, organizational change is catalysed through new media, thereby influencing social change at-large, perhaps forming the framework for a transformation from an industrial to an informational society. One notable text is Manuel Castells' The Internet Galaxy—the title of which forms an inter-textual reference to Marshall McLuhan's The Gutenberg Galaxy. Closely related to the sociology of the Internet is digital sociology, which expands the scope of study to address not only the internet but also the impact of the other digital media and devices that have emerged since the first decade of the twenty-first century. Media\nAs with cultural studies, media study is a distinct discipline that owes to the convergence of sociology and other social sciences and humanities, in particular, literary criticism and critical theory. Though neither the production process nor the critique of aesthetic forms is in the remit of sociologists, analyses of socializing factors, such as ideological effects and audience reception, stem from sociological theory and method. Thus the 'sociology of the media' is not a subdiscipline per se, but the media is a common and often indispensable topic. Economic sociology\nThe term \"economic sociology\" was first used by William Stanley Jevons in 1879, later to be coined in the works of Durkheim, Weber, and Simmel between 1890 and 1920. Economic sociology arose as a new approach to the analysis of economic phenomena, emphasizing class relations and modernity as a philosophical concept. The relationship between capitalism and modernity is a salient issue, perhaps best demonstrated in Weber's The Protestant Ethic and the Spirit of Capitalism (1905) and Simmel's The Philosophy of Money (1900). The contemporary period of economic sociology, also known as new economic sociology, was consolidated by the 1985 work of Mark Granovetter titled \"Economic Action and Social Structure: The Problem of Embeddedness\".",
        "rrf_score": 0.030679156908665108,
        "dense_rank": 1,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Digital_media_8",
        "text": "However, in 2018, major newspapers advertising revenue was 35% from digital ads. In contrast, mobile versions of newspapers and magazines came in second with a huge growth of 135%. The New York Times ...",
        "full_text": "However, in 2018, major newspapers advertising revenue was 35% from digital ads. In contrast, mobile versions of newspapers and magazines came in second with a huge growth of 135%. The New York Times has noted a 47% year of year rise in their digital subscriptions. 43% of adults get news often from news websites or social media, compared with 49% for television. Pew Research also asked respondents if they got news from a streaming device on their TV – 9% of U.S. adults said that they do so often. Individual as content creator\nDigital media has also allowed individuals to be much more active in content creation. Anyone with access to computers and the Internet can participate in social media and contribute their own writing, art, videos, photography and commentary to the Internet, as well as conduct business online. The dramatic reduction in the costs required to create and share content have led to a democratization of content creation as well as the creation of new types of content, like blogs, memes, and video essays. Some of these activities have also been labelled citizen journalism. This spike in user-created content is due to the development of the internet as well as the way in which users interact with media today. As more users join and use social media sites, the relevance of content creation increases. The release of technologies such mobile devices allow for easier and quicker access to all things media. Many media creation tools that were once available to only a few are now free and easy to use. The cost of devices that can access the Internet is steadily falling, and personal ownership of multiple digital devices is now becoming the standard. These elements have significantly affected political participation. Digital media is seen by many scholars as having a role in Arab Spring, and crackdowns on the use of digital and social media by embattled governments are increasingly common. Many governments restrict access to digital media in some way, either to prevent obscenity or in a broader form of political censorship. Over the years, YouTube has grown to become a website with user generated media.",
        "rrf_score": 0.02964426877470356,
        "dense_rank": 6,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      }
    ]
  },
  {
    "question": "Why does wind power challenge grid stability?",
    "answer": "Because wind energy output is variable.",
    "source_id": "Wind power",
    "url": "https://en.wikipedia.org/wiki/Wind_power",
    "category": "inferential",
    "generated_answer": "instantaneous",
    "response_time": "12.41s",
    "fused_chunks": [
      {
        "chunk_id": "Wind_power_5",
        "text": "Wind power's share of worldwide electricity usage in 2021 was almost 7%, up from 3.5% in 2015. There is no generally accepted maximum level of wind penetration. The limit for a particular grid will de...",
        "full_text": "Wind power's share of worldwide electricity usage in 2021 was almost 7%, up from 3.5% in 2015. There is no generally accepted maximum level of wind penetration. The limit for a particular grid will depend on the existing generating plants, pricing mechanisms, capacity for energy storage, demand management, and other factors. An interconnected electric power grid will already include reserve generating and transmission capacity to allow for equipment failures. This reserve capacity can also serve to compensate for the varying power generation produced by wind stations. Studies have indicated that 20% of the total annual electrical energy consumption may be incorporated with minimal difficulty. These studies have been for locations with geographically dispersed wind farms, some degree of dispatchable energy or hydropower with storage capacity, demand management, and interconnected to a large grid area enabling the export of electric power when needed. Electrical utilities continue to study the effects of large-scale penetration of wind generation on system stability. A wind energy penetration figure can be specified for different duration of time but is often quoted annually. To generate almost all electricity from wind annually requires substantial interconnection to other systems, for example some wind power in Scotland is sent to the rest of the British grid. On a monthly, weekly, daily, or hourly basis—or less—wind might supply as much as or more than 100% of current use, with the rest stored, exported or curtailed. The seasonal industry might then take advantage of high wind and low usage times such as at night when wind output can exceed normal demand. Such industry might include the production of silicon, aluminum, steel, or natural gas, and hydrogen, and using future long-term storage to facilitate 100% energy from variable renewable energy. Homes and businesses can also be programmed to vary electricity demand, for example by remotely turning up water heater thermostats. Variability\nWind power is variable, and during low wind periods, it may need to be replaced by other power sources.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_6",
        "text": "Homes and businesses can also be programmed to vary electricity demand, for example by remotely turning up water heater thermostats. Variability\nWind power is variable, and during low wind periods, it...",
        "full_text": "Homes and businesses can also be programmed to vary electricity demand, for example by remotely turning up water heater thermostats. Variability\nWind power is variable, and during low wind periods, it may need to be replaced by other power sources. Transmission networks presently cope with outages of other generation plants and daily changes in electrical demand, but the variability of intermittent power sources such as wind power is more frequent than those of conventional power generation plants which, when scheduled to be operating, may be able to deliver their nameplate capacity around 95% of the time. Electric power generated from wind power can be highly variable at several different timescales: hourly, daily, or seasonally. Annual variation also exists but is not as significant. Because instantaneous electrical generation and consumption must remain in balance to maintain grid stability, this variability can present substantial challenges to incorporating large amounts of wind power into a grid system. Intermittency and the non-dispatchable nature of wind energy production can raise costs for regulation, incremental operating reserve, and (at high penetration levels) could require an increase in the already existing energy demand management, load shedding, storage solutions, or system interconnection with HVDC cables. Fluctuations in load and allowance for the failure of large fossil-fuel generating units require operating reserve capacity, which can be increased to compensate for the variability of wind generation. Utility-scale batteries are often used to balance hourly and shorter timescale variation, but car batteries may gain ground from the mid-2020s. Wind power advocates argue that periods of low wind can be dealt with by simply restarting existing power stations that have been held in readiness, or interlinking with HVDC. The combination of diversifying variable renewables by type and location, forecasting their variation, and integrating them with dispatchable renewables, flexible fueled generators, and demand response can create a power system that has the potential to meet power supply needs reliably. Integrating ever-higher levels of renewables is being successfully demonstrated in the real world. Solar power tends to be complementary to wind.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 2,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_3",
        "text": "At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system. Generator characteristics and st...",
        "full_text": "At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system. Generator characteristics and stability\nMost modern turbines use variable speed generators combined with either a partial or full-scale power converter between the turbine generator and the collector system, which generally have more desirable properties for grid interconnection and have low-voltage ride-through capabilities. Modern turbines use either doubly fed electric machines with partial-scale converters or squirrel-cage induction generators or synchronous generators (both permanently and electrically excited) with full-scale converters. Black start is possible and is being further developed for places (such as Iowa) which generate most of their electricity from wind. Transmission system operators will supply a wind farm developer with a grid code to specify the requirements for interconnection to the transmission grid. This will include the power factor, the constancy of frequency, and the dynamic behaviour of the wind farm turbines during a system fault. Offshore wind power\nOffshore wind power is wind farms in large bodies of water, usually the sea. These installations can use the more frequent and powerful winds that are available in these locations and have less visual impact on the landscape than land-based projects. However, the construction and maintenance costs are considerably higher. As of November 2021, the Hornsea Wind Farm in the United Kingdom is the largest offshore wind farm in the world at 1,218 MW. Collection and transmission network\nNear offshore wind farms may be connected by AC and far offshore by HVDC. Wind power resources are not always located near areas with a high population density. As transmission lines become longer, the losses associated with power transmission increase, as modes of losses at lower lengths are exacerbated and new modes of losses are no longer negligible as the length is increased; making it harder to transport large loads over large distances. When the transmission capacity does not meet the generation capacity, wind farms are forced to produce below their full potential or stop running altogether, in a process known as curtailment. While this leads to potential renewable generation left untapped, it prevents possible grid overload or risk to reliable service.",
        "rrf_score": 0.031099324975891997,
        "dense_rank": 8,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_4",
        "text": "While this leads to potential renewable generation left untapped, it prevents possible grid overload or risk to reliable service. One of the biggest current challenges to wind power grid integration i...",
        "full_text": "While this leads to potential renewable generation left untapped, it prevents possible grid overload or risk to reliable service. One of the biggest current challenges to wind power grid integration in some countries is the necessity of developing new transmission lines to carry power from wind farms, usually in remote lowly populated areas due to availability of wind, to high load locations, usually on the coasts where population density is higher. Any existing transmission lines in remote locations may not have been designed for the transport of large amounts of energy. In particular geographic regions, peak wind speeds may not coincide with peak demand for electrical power, whether offshore or onshore. A possible future option may be to interconnect widely dispersed geographic areas with an HVDC super grid. Wind power capacity and production\nIn 2024, wind supplied over 2,494 TWh of electricity, which was 8.1% of world electricity. Growth trends\nTo help meet the Paris Agreement's goals to limit climate change, analysts say it should expand much faster than it currently is – by over 1% of electricity generation per year. Expansion of wind power is being hindered by fossil fuel subsidies. The actual amount of electric power that wind can generate is calculated by multiplying the nameplate capacity by the capacity factor, which varies according to equipment and location. Estimates of the capacity factors for wind installations are in the range of 35% to 44%. Capacity factor\nSince wind speed is not constant, a wind farm's annual energy production is never as much as the sum of the generator nameplate ratings multiplied by the total hours in a year. The ratio of actual productivity in a year to this theoretical maximum is called the capacity factor. Online data is available for some locations, and the capacity factor can be calculated from the yearly output. Penetration\nWind energy penetration is the fraction of energy produced by wind compared with the total generation. Wind power's share of worldwide electricity usage in 2021 was almost 7%, up from 3.5% in 2015. There is no generally accepted maximum level of wind penetration.",
        "rrf_score": 0.03076923076923077,
        "dense_rank": 5,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_11",
        "text": "An estimated 1.25 million people were employed in wind power in 2020. Small-scale wind power\nSmall-scale wind power is the name given to wind generation systems with the capacity to produce up to 50 k...",
        "full_text": "An estimated 1.25 million people were employed in wind power in 2020. Small-scale wind power\nSmall-scale wind power is the name given to wind generation systems with the capacity to produce up to 50 kW of electrical power. Isolated communities, that may otherwise rely on diesel generators, may use wind turbines as an alternative. Individuals may purchase these systems to reduce or eliminate their dependence on grid electric power for economic reasons, or to reduce their carbon footprint. Wind turbines have been used for household electric power generation in conjunction with battery storage over many decades in remote areas. Examples of small-scale wind power projects in an urban setting can be found in New York City, where, since 2009, several building projects have capped their roofs with Gorlov-type helical wind turbines. Although the energy they generate is small compared to the buildings' overall consumption, they help to reinforce the building's 'green' credentials in ways that \"showing people your high-tech boiler\" cannot, with some of the projects also receiving the direct support of the New York State Energy Research and Development Authority. Grid-connected domestic wind turbines may use grid energy storage, thus replacing purchased electric power with locally produced power when available. The surplus power produced by domestic microgenerators can, in some jurisdictions, be fed into the network and sold to the utility company, producing a retail credit for the microgenerators' owners to offset their energy costs. Off-grid system users can either adapt to intermittent power or use batteries, photovoltaic, or diesel systems to supplement the wind turbine. Equipment such as parking meters, traffic warning signs, street lighting, or wireless Internet gateways may be powered by a small wind turbine, possibly combined with a photovoltaic system, that charges a small battery replacing the need for a connection to the power grid. Airborne wind turbines, such as kites, can be used in places at risk of hurricanes, as they can be taken down in advance. Impact on environment and landscape\nThe environmental impact of electricity generation from wind power is minor when compared to that of fossil fuel power.",
        "rrf_score": 0.03055037313432836,
        "dense_rank": 7,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      }
    ]
  },
  {
    "question": "How do transformers and big data together improve language models?",
    "answer": "Transformers learn patterns effectively from large datasets.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "generate text based on the semantic relationships between words in sentences",
    "response_time": "9.30s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.030798389007344232,
        "dense_rank": 3,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Deep_learning_16",
        "text": "Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language model...",
        "full_text": "Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:\n\nScale-up/out and accelerated DNN training and decoding\nSequence discriminative training\nFeature processing by deep models with solid understanding of the underlying mechanisms\nAdaptation of DNNs and related deep models\nMulti-task and transfer learning by DNNs and related deep models\nCNNs and how to design them to best exploit domain knowledge of speech\nRNN and its rich LSTM variants\nOther types of deep models including tensor-based models and integrated deep generative/discriminative models. More recent speech recognition models use Transformers or Temporal Convolution Networks with significant success and widespread applications. All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning. Image recognition\nA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available. Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces. Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.",
        "rrf_score": 0.03055037313432836,
        "dense_rank": 7,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_0",
        "text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from ...",
        "full_text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. Overview\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Big_data_27",
        "text": "Big data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load pa...",
        "full_text": "Big data analysis is often shallow compared to analysis of smaller data sets. In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing. Big data is a buzzword and a \"vague term\", but at the same time an \"obsession\" with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy Awards and election predictions solely based on Twitter were more often off than on target. Big data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed. On the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant. Ioannidis argued that \"most published research findings are false\" due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a \"significant\" result being false grows fast – even more so, when only positive results are published. Furthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U.S. presidential election with varying degrees of success. Critiques of big data policing and surveillance\nBig data has been used in policing and surveillance by institutions like law enforcement and corporations (see: corporate surveillance and surveillance capitalism).",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Big_data",
        "title": "Big data"
      },
      {
        "chunk_id": "Natural_language_processing_2",
        "text": "This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical unde...",
        "full_text": "This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, many systems relied on corpora that were specifically developed for the tasks they were designed to perform. This reliance has been a major limitation to their broader effectiveness and continues to affect similar systems. Consequently, significant research has focused on methods for learning effectively from limited amounts of data. 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, large quantities of non-annotated data are available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical. 2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      }
    ]
  },
  {
    "question": "How do recommender systems and filter bubbles relate?",
    "answer": "Recommenders can repeatedly show similar content, creating filter bubbles.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "They were given the goal of maximizing user engagement (that is, the only goal was to keep people watching",
    "response_time": "9.13s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_20",
        "text": "Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuc...",
        "full_text": "Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI. On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it. Misinformation\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Consumer_behavior_40",
        "text": "The three categories are: \"trackers\", \"hunters\", and \"explorers\". \"Trackers\" are the online consumers who are exactly looking for a product that they definitely wish to buy. They use the Internet for ...",
        "full_text": "The three categories are: \"trackers\", \"hunters\", and \"explorers\". \"Trackers\" are the online consumers who are exactly looking for a product that they definitely wish to buy. They use the Internet for the information about its price, delivery methods, post-purchase service, and so on. Once they have found the proper information, little effort is needed to let them do the business. \"Hunters\" just know the categories of the product that they need, for instance, a novel for leisure time. However, they have not made specific decision on whose novel to buy. They use the Internet to find a list of product of their needed categories to make comparison. This kind of online consumer needs advice and help to do their business. \"Explorers\" do not even have the categories of product on their minds. In fact, they just want to buy something online. There is more uncertainty of this type of online consumers. Influence of the Internet on buying process\nAs the preceding table shows, the first row indicates the process of a consumer buying a new product, while the second and third row illustrates the positive influences the Internet could have on buying process by creating effective communications with online consumers. For example, suppose a consumer carelessly sees an advertisement about laptops on Wechat, a popular Chinese social media developed by Tencent. He begins to feel that his laptop is a bit out of date and wants to buy a new one, which is the outcome of good advertisement placed on a daily Internet tool. He does not know anything about how to buy a new one as business changes so fast today, so he searches on Google to find an answer. On the results page he finds promotional ads which mainly come from JD.com and Taobao, two competing Chinese online retailers in this field. He prefers to use JD.com, which provides detailed comparisons of brands, prices, locations, and methods of payment and delivery. After careful selection, he makes his order through JD.com via Wechat payment.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Consumer_behavior",
        "title": "Consumer behavior"
      },
      {
        "chunk_id": "Semantics_1",
        "text": "Semantics is relevant to the fields of formal logic, computer science, and psychology. Definition and related fields\nSemantics is the study of meaning in languages. It is a systematic inquiry that exa...",
        "full_text": "Semantics is relevant to the fields of formal logic, computer science, and psychology. Definition and related fields\nSemantics is the study of meaning in languages. It is a systematic inquiry that examines what linguistic meaning is and how it arises. It investigates how expressions are built up from different layers of constituents, like morphemes, words, clauses, sentences, and texts, and how the meanings of the constituents affect one another. Semantics can focus on a specific language, like English, but in its widest sense, it investigates meaning structures relevant to all languages. As a descriptive discipline, it aims to determine how meaning works without prescribing what meaning people should associate with particular expressions. Some of its key questions are \"How do the meanings of words combine to create the meanings of sentences? \", \"How do meanings relate to the minds of language users, and to the things words refer to? \", and \"What is the connection between what a word means, and the contexts in which it is used?\". The main disciplines engaged in semantics are linguistics, semiotics, and philosophy. Besides its meaning as a field of inquiry, semantics can also refer to theories within this field, like truth-conditional semantics, and to the meaning of particular expressions, like the semantics of the word fairy. As a field of inquiry, semantics has both an internal and an external side. The internal side is interested in the connection between words and the mental phenomena they evoke, like ideas and conceptual representations. The external side examines how words refer to objects in the world and under what conditions a sentence is true. Many related disciplines investigate language and meaning. Semantics contrasts with other subfields of linguistics focused on distinct aspects of language. Phonology studies the different types of sounds used in languages and how sounds are connected to form words while syntax examines the rules that dictate how to arrange words to create sentences.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Semantics",
        "title": "Semantics"
      },
      {
        "chunk_id": "Consumer_behavior_9",
        "text": "It also needs to monitor other brands in the customer's consideration set to optimise planning for its own brand. During the evaluation of alternatives, the consumer ranks or assesses the relative mer...",
        "full_text": "It also needs to monitor other brands in the customer's consideration set to optimise planning for its own brand. During the evaluation of alternatives, the consumer ranks or assesses the relative merits of different options available. No universal evaluation process is used by consumers across all-buying situations. Instead, consumers generate different evaluation criteria depending on each unique buying situation. Social media further enables consumers to share views with their peers about the product they are looking to purchase. This way, consumers can gauge the positive and negative sides of each alternative, and decide even more conveniently as for the best product to buy. Thus the relevant evaluation attributes vary according to across different types of consumers and purchase contexts. For example, attributes important for evaluating a restaurant would include food quality, price, location, atmosphere, quality of service, and menu selection. Consumers, depending on their geographic, demographic, psychographic and behavioural characteristics, will decide which attributes are important to them. Potential patrons seeking a pleasant dining experience may be willing to travel further distances to patronise a fine-dining venue compared to those wanting a quick meal at a more utilitarian eatery. After evaluating the different product attributes, the consumer ranks each attribute or benefit from highly important to least important. These priorities are directly related to the consumer's needs and wants. Thus, the consumer arrives at a weighted score for each product or brand which represents the consumer's subjective assessment of individual attribute scores weighted in terms of their importance. Using these scores, they arrive at a total mental score or rank for each product/brand under consideration. Purchase decision\nOnce the alternatives have been evaluated, the consumer firms up their resolve to proceed through to the actual purchase. For example, the consumer might say to themself, \"Yes, I will buy Brand X one day.\" This self instruction to make a purchase is known as purchase intent. Purchase intentions are a strong yet imperfect predictor of sales. Sometimes purchase intentions simply do not translate into an actual purchase and this can signal a marketing problem.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Consumer_behavior",
        "title": "Consumer behavior"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      }
    ]
  },
  {
    "question": "How does computer vision support robotics?",
    "answer": "It provides perception of the environment.",
    "source_id": "Computer vision",
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "category": "multi-hop",
    "generated_answer": "autonomous path planning or deliberation for robotic systems to navigate through an environment",
    "response_time": "11.05s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_vision_5",
        "text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. Thi...",
        "full_text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology. Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap.",
        "rrf_score": 0.031544957774465976,
        "dense_rank": 1,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_16",
        "text": "In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system ...",
        "full_text": "In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors. Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower). A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images. While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized. Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective. As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role. See also\nLists\nOutline of computer vision\nList of emerging technologies\nOutline of artificial intelligence\n\nReferences\nFurther reading\nJames E. Dobson (2023). The Birth of Computer Vision. University of Minnesota Press. ISBN 978-1-5179-1421-9. David Marr (1982). Vision. W. H. Freeman and Company. ISBN 978-0-7167-1284-8. Azriel Rosenfeld; Avinash Kak (1982). Digital Picture Processing. Academic Press. ISBN 978-0-12-597301-4. Barghout, Lauren; Lawrence W. Lee (2003). Perceptual information processing system.",
        "rrf_score": 0.031544957774465976,
        "dense_rank": 6,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_0",
        "text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical ...",
        "full_text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Definition\nComputer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.",
        "rrf_score": 0.03149801587301587,
        "dense_rank": 3,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_3",
        "text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, s...",
        "full_text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields. Signal processing\nYet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. Robotic navigation\nRobot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n\nVisual computing\nOther fields\nBesides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view.",
        "rrf_score": 0.02919863597612958,
        "dense_rank": 8,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Robotics_18",
        "text": "As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and...",
        "full_text": "As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras. In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities. There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology. Environmental interaction and navigation\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Robotics",
        "title": "Robotics"
      }
    ]
  },
  {
    "question": "How does IoT combined with AI enable smart environments?",
    "answer": "Connected devices collect data and AI makes decisions from it.",
    "source_id": "Internet of things",
    "url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "category": "multi-hop",
    "generated_answer": "mix and match of potential IoT devices from different systems that benefit its users",
    "response_time": "10.87s",
    "fused_chunks": [
      {
        "chunk_id": "Internet_of_things_19",
        "text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devi...",
        "full_text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users. Function\nIoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, shares information, monitors, navigates and groups with other IoT devices in the same or nearby network realizing SIoT and facilitating useful service compositions in order to help its users proactively in every day's life especially during emergency. Examples\nIoT-based smart home technology monitors health data of patients or aging adults by analyzing their physiological parameters and prompt the nearby health facilities when emergency medical services needed. In case emergency, automatically, ambulance of a nearest available hospital will be called with pickup location provided, ward assigned, patient's health data will be transmitted to the emergency department, and display on the doctor's computer immediately for further action. IoT sensors on the vehicles, road and traffic lights monitor the conditions of the vehicles and drivers and alert when attention needed and also coordinate themselves automatically to ensure autonomous driving is working normally. Unfortunately if an accident happens, IoT camera will inform the nearest hospital and police station for help. Challenges\nInternet of things is multifaceted and complicated. One of the main factors that hindering people from adopting and use Internet of things (IoT) based products and services is its complexity. Installation and setup is a challenge to people, therefore, there is a need for IoT devices to mix match and configure themselves automatically to provide different services at different situation. System security always a concern for any technology, and it is more crucial for SIoT as not only security of oneself need to be considered but also the mutual trust mechanism between collaborative IoT devices from time to time, from place to place. Another critical challenge for SIoT is the accuracy and reliability of the sensors.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 3,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_6",
        "text": "Eventually, V2X is the first step to autonomous driving and connected road infrastructure. Home automation\nIoT devices can be used to monitor and control the mechanical, electrical, and electronic sys...",
        "full_text": "Eventually, V2X is the first step to autonomous driving and connected road infrastructure. Home automation\nIoT devices can be used to monitor and control the mechanical, electrical, and electronic systems used in various types of buildings (e.g., public and private, industrial, institutions, or residential) in home automation and building automation systems. In this context, three main areas are being covered in the literature:\n\nThe integration of the Internet with building energy management systems to create energy-efficient and IOT-driven \"smart buildings\". The possible means of real-time monitoring for reducing energy consumption and monitoring occupant behaviors. The integration of smart devices in the built environment and how they might be used in future applications. Industrial\nAlso known as IIoT, industrial IoT devices acquire and analyze data from connected equipment, operational technology (OT), locations, and people. Combined with operational technology (OT) monitoring devices, IIoT helps regulate and monitor industrial systems. Additionally, the same implementation can be carried out for automated record updates of asset placement in industrial storage units as the size of the assets can vary from a small screw to the whole motor spare part, and misplacement of such assets can cause a loss of manpower time and money. Manufacturing\nThe IoT can connect various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities. Network control and management of manufacturing equipment, asset and situation management, or manufacturing process control enable IoT to be utilized for industrial applications and smart manufacturing. IoT intelligent systems enable rapid manufacturing and optimization of new products and rapid response to product demands. Digital control systems, which aim to automate process controls, operator tools, and service information systems, alongside optimizing plant safety and security, fall within the purview of the IIoT. Furthermore, IoT can also be applied to asset management via predictive maintenance, statistical evaluation, and measurements to maximize reliability. Industrial management systems can be integrated with smart grids, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by networked sensors.",
        "rrf_score": 0.03177805800756621,
        "dense_rank": 5,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_0",
        "text": "The Internet of things (IoT) describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and syste...",
        "full_text": "The Internet of things (IoT) describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communication networks. The IoT encompasses electronics, communication, and computer science engineering. \"Internet of things\" has been considered a misnomer because devices do not need to be connected to the public Internet; they only need to be connected to a network and be individually addressable. The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning. Traditional fields of embedded systems, wireless sensor networks, and control systems independently and collectively enable the Internet of Things. While in the consumer market, IoT technology is most synonymous with \"smart home\" products—including devices and appliances like thermostats and smart speakers—the technology's largest applications are in the business and industrial sectors. Commercial asset tracking and fleet management represent the largest single application of IoT, accounting for 22% of the total market, driven by the need to monitor mobile assets like vehicles and shipping containers. Other major applications include industrial monitoring, smart metering in utilities, and connected healthcare. However, several concerns exist regarding the risks associated with the growth and diffusion of IoT technologies and products, particularly in the areas of privacy and security. Consequently, several industries, technology companies, and governments (or their branches, ministries, bureaus, departments, etc.) of many countries have taken multiple steps and implemented a variety of precautionary measures to address these concerns adequately and minimize safety risks, including the development and implementation of international and local standards, guidelines, and regulatory frameworks. Due to their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly creates regulatory ambiguities, complicating jurisdictional boundaries of the data transfer.",
        "rrf_score": 0.030776515151515152,
        "dense_rank": 6,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_3",
        "text": "Applications\nThe extensive set of applications for IoT devices is often divided into consumer, commercial, industrial, and infrastructure spaces. Consumers\nA growing portion of IoT devices is created ...",
        "full_text": "Applications\nThe extensive set of applications for IoT devices is often divided into consumer, commercial, industrial, and infrastructure spaces. Consumers\nA growing portion of IoT devices is created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities. Home automation\nIoT devices are part of the broader concept of home automation, which generally includes lighting, heating and air conditioning, media and security systems, and camera systems. Moreover, long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage. A smart home, also known as an automated home, could be based on a platform or hubs that control smart devices and appliances. For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch. This could be a dedicated app or iOS native applications such as Siri. This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge. There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products. These include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub. In addition to the commercial systems, there are many non-proprietary, open source ecosystems, including Home Assistant, OpenHAB, and Domoticz. Elder care\nOne key application of a smart home is to assist the elderly and individuals with disabilities. These home systems use assistive technology to accommodate an owner's specific disabilities. Voice control can assist users with sight and mobility limitations while alert systems can be connected directly to cochlear implants worn by individuals with hearing impairments. They can also be equipped with additional safety features, including sensors that monitor for medical emergencies such as falls or seizures. Smart home technology applied in this way can provide users with more freedom and a higher quality of life.",
        "rrf_score": 0.030117753623188408,
        "dense_rank": 4,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_7",
        "text": "Industrial management systems can be integrated with smart grids, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions ...",
        "full_text": "Industrial management systems can be integrated with smart grids, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by networked sensors. In addition to general manufacturing, IoT is also used for processes in the industrialization of construction. Agriculture\nThere are numerous IoT applications in farming such as collecting data on temperature, rainfall, humidity, wind speed, pest infestation, and soil content. This data can be used to automate farming techniques, make informed decisions to improve quality and quantity, minimize risk and waste, and reduce the effort required to manage crops. For example, farmers can now monitor soil temperature and moisture from afar and even apply IoT-acquired data to precision fertilization programs. The overall goal is that data from sensors, coupled with the farmer's knowledge and intuition about his or her farm, can help increase farm productivity, and also help reduce costs. In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide. The FarmBeats project from Microsoft Research that uses TV white space to connect farms is also a part of the Azure Marketplace now. Maritime\nIoT devices are in use to monitor the environments and systems of boats and yachts. Many pleasure boats are left unattended for days in summer, and months in winter so such devices provide valuable early alerts of boat flooding, fire, and deep discharge of batteries. The use of global Internet data networks such as Sigfox, combined with long-life batteries, and microelectronics allows the engine rooms, bilge, and batteries to be constantly monitored and reported to connected Android & Apple applications for example. Infrastructure\nMonitoring and controlling operations of sustainable urban and rural infrastructures like bridges, railway tracks and on- and offshore wind farms is a key application of the IoT.",
        "rrf_score": 0.02967032967032967,
        "dense_rank": 10,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      }
    ]
  },
  {
    "question": "How do supervised learning and labeled data connect?",
    "answer": "Supervised learning trains on labeled examples.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "multi-hop",
    "generated_answer": "Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy",
    "response_time": "16.46s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_8",
        "text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the...",
        "full_text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Other types\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. Self-learning\nSelf-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 3,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_6",
        "text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm...",
        "full_text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm works for all problems. Supervised learning\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised.",
        "rrf_score": 0.03177805800756621,
        "dense_rank": 1,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_10",
        "text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervi...",
        "full_text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary.",
        "rrf_score": 0.031024531024531024,
        "dense_rank": 6,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_3",
        "text": "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of...",
        "full_text": "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. Data compression\nData mining\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). Generalization\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.",
        "rrf_score": 0.030776515151515152,
        "dense_rank": 4,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_intelligence_8",
        "text": "Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and pe...",
        "full_text": "Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). Classifiers and statistical learning methods\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience. There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. The naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability. Neural networks are also used as classifiers. Artificial neural networks\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer.",
        "rrf_score": 0.030309988518943745,
        "dense_rank": 5,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "What does NLP stand for?",
    "answer": "Natural language processing.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "Natural Language Processing",
    "response_time": "9.14s",
    "fused_chunks": [
      {
        "chunk_id": "Natural_language_processing_0",
        "text": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is ...",
        "full_text": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly. Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation. History\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. Symbolic NLP (1950s – early 1990s)\nThe premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. 1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed. 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Natural_language_processing_12",
        "text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantic...",
        "full_text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Document AI\nA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants. Dialogue management\nComputer systems intended to converse with a human. Question answering\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada? \"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation\nGiven a description of an image, generate an image that matches the description. Text-to-scene generation\nGiven a description of a scene, generate a 3D model of the scene. Text-to-video\nGiven a description of a video, generate a video that matches the description. General tendencies and (possible) future directions\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:\n\nInterest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 4,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Natural_language_processing_1",
        "text": "Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the \"patient\" exceeded the very small knowledge base, ELIZA migh...",
        "full_text": "Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer  memory at the time. 1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY). 1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period. Statistical NLP (1990s–present)\nUp until the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing. This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g.",
        "rrf_score": 0.031024531024531024,
        "dense_rank": 6,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Natural_language_processing_13",
        "text": "Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, ...",
        "full_text": "Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\nElimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\n\nCognition\nMost higher-level NLP applications involve aspects that emulate intelligent behavior and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behavior represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above). Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:\n\nApply the theory of conceptual metaphor, explained by Lakoff as \"the understanding of one idea, in terms of another\" which provides an idea of the intent of the author. For example, consider the English word big. When used in a comparison (\"That is a big tree\"), the author's intent is to imply that the tree is physically large relative to other trees or the authors experience. When used metaphorically (\"Tomorrow is a big day\"), the author's intent to imply importance. The intent behind other usages, like in \"She is a big person\", will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.",
        "rrf_score": 0.030090497737556562,
        "dense_rank": 5,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Natural_language_processing_2",
        "text": "This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical unde...",
        "full_text": "This shift was influenced by increasing computational power (see Moore's law) and a decline in the dominance of Chomskyan linguistic theories... (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. 1990s: Many of the notable early successes in statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research, such as IBM alignment models. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, many systems relied on corpora that were specifically developed for the tasks they were designed to perform. This reliance has been a major limitation to their broader effectiveness and continues to affect similar systems. Consequently, significant research has focused on methods for learning effectively from limited amounts of data. 2000s: With the growth of the web, increasing amounts of raw (unannotated) language data have become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, large quantities of non-annotated data are available (including, among other things, the entire content of the World Wide Web), which can often make up for the worse efficiency if the algorithm used has a low enough time complexity to be practical. 2003: word n-gram model, at the time the best statistical algorithm, is outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words, trained on up to 14 million words, by Bengio et al.)",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      }
    ]
  },
  {
    "question": "What is a neural network inspired by?",
    "answer": "Biological neurons in the brain.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "information processing and distributed communication nodes in biological",
    "response_time": "10.08s",
    "fused_chunks": [
      {
        "chunk_id": "Deep_learning_11",
        "text": "but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural ne...",
        "full_text": "but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\". Neural networks\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 2,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_3",
        "text": "The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Nar...",
        "full_text": "The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop. History\nBefore 1980\nThere are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was republished by John Hopfield in 1982. Other early recurrent neural networks were published by Kaoru Nakano in 1971. Already in 1948, Alan Turing produced work on \"Intelligent Machinery\"  that was not published in his lifetime, containing \"ideas related to artificial evolution and learning RNNs\". Frank Rosenblatt (1958) proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight). The book cites an earlier network by R. D. Joseph (1960) \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships.",
        "rrf_score": 0.0315136476426799,
        "dense_rank": 5,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_2",
        "text": "Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance. Deep learning...",
        "full_text": "Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance. Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data is more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks. The term deep learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. Although the history of its appearance is apparently more complicated. Interpretations\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference. The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit. The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator. The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.",
        "rrf_score": 0.02964426877470356,
        "dense_rank": 9,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_0",
        "text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from ...",
        "full_text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. Overview\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.",
        "rrf_score": 0.02964426877470356,
        "dense_rank": 6,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Machine_learning_14",
        "text": "During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specifici...",
        "full_text": "During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions. By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned. Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection. Artificial neural networks\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules. An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "What is backpropagation used for?",
    "answer": "Training neural networks by adjusting weights.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "learn to model complex relationships between inputs and outputs and find patterns in data",
    "response_time": "10.31s",
    "fused_chunks": [
      {
        "chunk_id": "Deep_learning_4",
        "text": "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep net...",
        "full_text": "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\". The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_5",
        "text": "popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used conv...",
        "full_text": "popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images. Recurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 3,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_intelligence_9",
        "text": "There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A netw...",
        "full_text": "There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers. Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function. In feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects. Deep learning\nDeep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces. Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Database_25",
        "text": "Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more ...",
        "full_text": "Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier? \", or \"if a product is sold with two different forms of packaging, are those the same product or different products? \", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes. Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data. Having produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Database",
        "title": "Database"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.015625,
        "dense_rank": 0,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      }
    ]
  },
  {
    "question": "What is a classifier in AI?",
    "answer": "A function that assigns categories using pattern matching.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "functions that use pattern matching to determine the closest match",
    "response_time": "9.09s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_8",
        "text": "Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and pe...",
        "full_text": "Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). Classifiers and statistical learning methods\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience. There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s. The naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability. Neural networks are also used as classifiers. Artificial neural networks\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer.",
        "rrf_score": 0.03131881575727918,
        "dense_rank": 1,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Machine_learning_6",
        "text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm...",
        "full_text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm works for all problems. Supervised learning\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Natural_language_processing_12",
        "text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantic...",
        "full_text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Document AI\nA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants. Dialogue management\nComputer systems intended to converse with a human. Question answering\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada? \"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation\nGiven a description of an image, generate an image that matches the description. Text-to-scene generation\nGiven a description of a scene, generate a 3D model of the scene. Text-to-video\nGiven a description of a video, generate a video that matches the description. General tendencies and (possible) future directions\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:\n\nInterest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      }
    ]
  },
  {
    "question": "What is a CNN commonly used for?",
    "answer": "Image processing and recognizing visual patterns.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "backpropagation, but did not cite the original work. 1980s-2000s The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply",
    "response_time": "15.82s",
    "fused_chunks": [
      {
        "chunk_id": "Deep_learning_5",
        "text": "popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used conv...",
        "full_text": "popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation. In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition. In 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days. In 1990, Wei Zhang implemented a CNN on optical computing hardware. In 1991, a CNN was applied to medical image object segmentation and breast cancer detection in mammograms. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images. Recurrent neural networks (RNN) were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study problems in cognitive psychology. In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, Jürgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below. This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network. In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Philosophy_15",
        "text": "The main traditional responses are materialism, which assumes that matter is more fundamental; idealism, which assumes that mind is more fundamental; and dualism, which assumes that mind and matter ar...",
        "full_text": "The main traditional responses are materialism, which assumes that matter is more fundamental; idealism, which assumes that mind is more fundamental; and dualism, which assumes that mind and matter are distinct types of entities. In contemporary philosophy, another common view is functionalism, which understands mental states in terms of the functional or causal roles they play. The mind-body problem is closely related to the hard problem of consciousness, which asks how the physical brain can produce qualitatively subjective experiences. The philosophy of religion investigates the basic concepts, assumptions, and arguments associated with religion. It critically reflects on what religion is, how to define the divine, and whether one or more gods exist. It also includes the discussion of worldviews that reject religious doctrines. Further questions addressed by the philosophy of religion are: \"How are we to interpret religious language, if not literally? \"; \"Is divine omniscience compatible with free will? \"; and, \"Are the great variety of world religions in some way compatible in spite of their apparently contradictory theological claims?\" It includes topics from nearly all branches of philosophy. It differs from theology since theological debates typically take place within one religious tradition, whereas debates in the philosophy of religion transcend any particular set of theological assumptions. The philosophy of science examines the fundamental concepts, assumptions, and problems associated with science. It reflects on what science is and how to distinguish it from pseudoscience. It investigates the methods employed by scientists, how their application can result in knowledge, and on what assumptions they are based. It also studies the purpose and implications of science. Some of its questions are \"What counts as an adequate explanation? \"; \"Is a scientific law anything more than a description of a regularity? \"; and \"Can some special sciences be explained entirely in the terms of a more general science?\" It is a vast field that is commonly divided into the philosophy of the natural sciences and the philosophy of the social sciences, with further subdivisions for each of the individual sciences under these headings.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Deep_learning_0",
        "text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from ...",
        "full_text": "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and revolves around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. Overview\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_10",
        "text": "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. ...",
        "full_text": "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on  Jürgen Schmidhuber's principle of artificial curiosity)\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone. Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Marketing_13",
        "text": "The process is conducted for two main purposes: better allocation of a firm's finite resources and to better serve the more diversified tastes of contemporary consumers. A firm only possesses a certai...",
        "full_text": "The process is conducted for two main purposes: better allocation of a firm's finite resources and to better serve the more diversified tastes of contemporary consumers. A firm only possesses a certain amount of resources. Thus, it must make choices (and appreciate the related costs) in servicing specific groups of consumers. Moreover, with more diversity in the tastes of modern consumers, firms are noting the benefit of servicing a multiplicity of new markets. Market segmentation can be defined in terms of the STP acronym, meaning Segmentation, Targeting, and Positioning. Segmentation involves the initial splitting up of consumers into persons of like needs/wants/tastes. Commonly used criteria include:\n\nGeographic (such as a country, region, city, town)\nPsychographic (e.g. personality traits or lifestyle traits which influence consumer behaviour)\nDemographic (e.g. age, gender, socio-economic class, education)\nGender\nIncome\nLife-Cycle (e.g. Baby Boomer, Generation X, Millennial, Generation Z)\nLifestyle (e.g. tech savvy, active)\nBehavioral (e.g. brand loyalty, usage rate)\nOnce a segment has been identified to target, a firm must ascertain whether the segment is beneficial for them to service. The DAMP acronym is used as criteria to gauge the viability of a target market. The elements of DAMP are:\n\nDiscernable – how a segment can be differentiated from other segments. Accessible – how a segment can be accessed via Marketing Communications produced by a firm\nMeasurable – can the segment be quantified and its size determined? Profitable – can a sufficient return on investment be attained from a segment's servicing? The next step in the targeting process is the level of differentiation involved in a segment serving. Three modes of differentiation exist, which are commonly applied by firms.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Marketing",
        "title": "Marketing"
      }
    ]
  },
  {
    "question": "What is reinforcement learning driven by?",
    "answer": "Rewards and punishments.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "factual",
    "generated_answer": "interaction between cognition and emotion",
    "response_time": "8.51s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_8",
        "text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the...",
        "full_text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Other types\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. Self-learning\nSelf-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_intelligence_3",
        "text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent m...",
        "full_text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. Learning\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.",
        "rrf_score": 0.031544957774465976,
        "dense_rank": 1,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Machine_learning_9",
        "text": "Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence ...",
        "full_text": "Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion. The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n\nin situation s act a\nreceive a consequence situation s'\ncompute emotion of being in the consequence situation v(s')\nupdate crossbar memory  w'(a,s) = w(a,s) + v(s')\nIt is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour in an environment that contains both desirable and undesirable situations. Feature learning\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data.",
        "rrf_score": 0.03125,
        "dense_rank": 4,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_intelligence_4",
        "text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other gui...",
        "full_text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.",
        "rrf_score": 0.030090497737556562,
        "dense_rank": 5,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Machine_learning_1",
        "text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early...",
        "full_text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think? \", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\". Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms. Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "What is GAN short for?",
    "answer": "Generative adversarial network.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "factual",
    "generated_answer": "unanswerable",
    "response_time": "7.98s",
    "fused_chunks": [
      {
        "chunk_id": "Deep_learning_10",
        "text": "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. ...",
        "full_text": "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on  Jürgen Schmidhuber's principle of artificial curiosity)\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone. Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Machine_learning_2",
        "text": "Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and associati...",
        "full_text": "Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule. Current Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods. In 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques. Relationships to other fields\nArtificial intelligence\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines, including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Computer_network_17",
        "text": "Metropolitan area network\nA metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. Wide...",
        "full_text": "Metropolitan area network\nA metropolitan area network (MAN) is a large computer network that interconnects users with computer resources in a geographic region of the size of a metropolitan area. Wide area network\nA wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances. A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and airwaves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI model: the physical layer, the data link layer, and the network layer. Global area network\nA global area network (GAN) is a network used for supporting mobile users across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs. Scope\nAn intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees). Intranets do not have to be connected to the Internet, but generally have a limited connection. An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers). Networks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity. Intranet\nAn intranet is a set of networks that are under the control of a single administrative entity. An intranet typically uses the Internet Protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits the use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Computer_network",
        "title": "Computer network"
      },
      {
        "chunk_id": "Philosophy_3",
        "text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the ...",
        "full_text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: \"What can I know? \"; \"What should I do? \"; \"What may I hope? \"; and \"What is the human being?\" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as \"the midwife of the sciences\". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a \"rigorous science\" investigating essences.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      }
    ]
  },
  {
    "question": "Who proposed the Turing Test?",
    "answer": "Alan Turing.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "Alan Turing",
    "response_time": "7.83s",
    "fused_chunks": [
      {
        "chunk_id": "Consciousness_39",
        "text": "... The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any anal...",
        "full_text": "... The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform. It can follow analysis; but it has no power of anticipating any analytical relations or truths. Its province is to assist us in making available what we are already acquainted with. One of the most influential contributions to this question was an essay written in 1950 by pioneering computer scientist Alan Turing, titled Computing Machinery and Intelligence. Turing disavowed any interest in terminology, saying that even \"Can machines think?\" is too loaded with spurious connotations to be meaningful; but he proposed to replace all such questions with a specific operational test, which has become known as the Turing test. To pass the test, a computer must be able to imitate a human well enough to fool interrogators. In his essay Turing discussed a variety of possible objections, and presented a counterargument to each of them. The Turing test is commonly cited in discussions of artificial intelligence as a proposed criterion for machine consciousness; it has provoked a great deal of philosophical debate. For example, Daniel Dennett and Douglas Hofstadter argue that anything capable of passing the Turing test is necessarily conscious, while David Chalmers argues that a philosophical zombie could pass the test, yet fail to be conscious. A third group of scholars have argued that with technological growth once machines begin to display any substantial signs of human-like behavior then the dichotomy (of human consciousness compared to human-like consciousness) becomes passé and issues of machine autonomy begin to prevail even as observed in its nascent form within contemporary industry and technology. Jürgen Schmidhuber argues that consciousness is the result of compression. As an agent sees representation of itself recurring in the environment, the compression of this representation can be called consciousness.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Consciousness",
        "title": "Consciousness"
      },
      {
        "chunk_id": "Artificial_intelligence_36",
        "text": "Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines ...",
        "full_text": "Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\" Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine – and no other philosophical discussion is required, or may not even be possible. Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 2,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_2",
        "text": "These include:\n\nthe ability to sense (e.g. see, hear, etc. ), and\nthe ability to act (e.g. move and manipulate objects, change location to explore, etc.) This includes the ability to detect and respon...",
        "full_text": "These include:\n\nthe ability to sense (e.g. see, hear, etc. ), and\nthe ability to act (e.g. move and manipulate objects, change location to explore, etc.) This includes the ability to detect and respond to hazard. Tests for human-level AGI\nSeveral tests meant to confirm human-level AGI have been considered, including:\n\nThe Turing Test (Turing)\nProposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge that it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine. Turing described the test as follows:\nThe idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be experts about machines, must be taken in by the pretence. In 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI. In 2023, it was claimed that \"AI is closer to ever\" to passing the Turing test, though the article's authors reinforced that imitation (as \"large language models\" ever closer to passing the test are built upon) is not synonymous with \"intelligence\". Further, as AI intelligence and human intelligence may differ, \"passing the Turing test is good evidence a system is intelligent, failing it is not good evidence a system is not intelligent.\"",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 4,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Consciousness_41",
        "text": "Another way of putting the argument is to say that computer programs can pass the Turing test for processing the syntax of a language, but that the syntax cannot lead to semantic meaning in the way st...",
        "full_text": "Another way of putting the argument is to say that computer programs can pass the Turing test for processing the syntax of a language, but that the syntax cannot lead to semantic meaning in the way strong AI advocates hoped. In the literature concerning artificial intelligence, Searle's essay has been second only to Turing's in the volume of debate it has generated. Searle himself was vague about what extra ingredients it would take to make a machine conscious: all he proposed was that what was needed was \"causal powers\" of the sort that the brain has and that computers lack. But other thinkers sympathetic to his basic argument have suggested that the necessary (though perhaps still not sufficient) extra conditions may include the ability to pass not just the verbal version of the Turing test, but the robotic version, which requires grounding the robot's words in the robot's sensorimotor capacity to categorize and interact with the things in the world that its words are about, Turing-indistinguishably from a real person. Turing-scale robotics is an empirical branch of research on embodied cognition and situated cognition. In 2014, Victor Argonov has suggested a non-Turing test for machine consciousness based on a machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that a machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.",
        "rrf_score": 0.031024531024531024,
        "dense_rank": 6,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Consciousness",
        "title": "Consciousness"
      },
      {
        "chunk_id": "Natural_language_processing_0",
        "text": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is ...",
        "full_text": "Natural language processing (NLP) is the processing of natural language information by a computer. NLP is a subfield of computer science and is closely associated with artificial intelligence. NLP is also related to information retrieval, knowledge representation, computational linguistics, and linguistics more broadly. Major processing tasks in an NLP system include: speech recognition, text classification, natural language understanding, and natural language generation. History\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language. Symbolic NLP (1950s – early 1990s)\nThe premise of symbolic NLP is often illustrated using John Searle's Chinese room thought experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. 1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten years of research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe) until the late 1980s when the first statistical machine translation systems were developed. 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapy, written by Joseph Weizenbaum between 1964 and 1966. Despite using minimal information about human thought or emotion, ELIZA was able to produce interactions that appeared human-like.",
        "rrf_score": 0.031024531024531024,
        "dense_rank": 3,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      }
    ]
  },
  {
    "question": "What is AGI?",
    "answer": "Artificial general intelligence that matches human cognitive ability.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "a hypothetical type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks",
    "response_time": "11.49s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_1",
        "text": "A framework for classifying AGI was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a c...",
        "full_text": "A framework for classifying AGI was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans). Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous). Characteristics\nThere is no single agreed-upon definition of intelligence as applied to computers. Computer scientist John McCarthy wrote in 2007: \"We cannot yet characterize in general what kinds of computational procedures we want to call intelligent.\" Intelligence traits\nResearchers generally hold that a system is required to do all of the following to be regarded as an AGI:\n\nreason, use strategy, solve puzzles, and make judgments under uncertainty,\nrepresent knowledge, including common sense knowledge,\nplan,\nlearn,\ncommunicate in natural language,\nif necessary, integrate these skills in completion of any given goal. Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy. Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree. Physical traits\nOther capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:\n\nthe ability to sense (e.g. see, hear, etc. ), and\nthe ability to act (e.g. move and manipulate objects, change location to explore, etc.)",
        "rrf_score": 0.03125763125763126,
        "dense_rank": 3,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_0",
        "text": "Artificial general intelligence (AGI) is a hypothetical type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks. Beyond AGI, artificial supe...",
        "full_text": "Artificial general intelligence (AGI) is a hypothetical type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks. Beyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin. Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved. Creating AGI is a stated goal of AI technology companies such as OpenAI, Google, xAI, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries. AGI is a common topic in science fiction and futures studies. Contention exists over whether AGI represents an existential risk. Some AI experts and industry figures have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk. Terminology\nAGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness. In contrast, weak AI (or narrow AI) can solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans. Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution. A framework for classifying AGI was proposed in 2023 by Google DeepMind researchers.",
        "rrf_score": 0.031054405392392875,
        "dense_rank": 2,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_18",
        "text": "Risks\nExistential risks\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic...",
        "full_text": "Risks\nExistential risks\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\". The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench them, preventing moral progress. Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create an entrenched repressive worldwide totalitarian regime. There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass-created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe. Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\". Risk of loss of control and human extinction\nThe thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman. In 2014, Stephen Hawking criticized widespread indifference:\n\nSo, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities.",
        "rrf_score": 0.031054405392392875,
        "dense_rank": 7,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_8",
        "text": "John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasib...",
        "full_text": "John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question, but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI. A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about. In 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking. Blaise Agüera y Arcas and Peter Norvig wrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing that frontier models had already achieved a significant level of general intelligence. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\". Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop.",
        "rrf_score": 0.030090497737556562,
        "dense_rank": 5,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "What is digital media?",
    "answer": "Media encoded in machine-readable digital formats.",
    "source_id": "Digital media",
    "url": "https://en.wikipedia.org/wiki/Digital_media",
    "category": "factual",
    "generated_answer": "any communication media that operates in conjunction with various encoded machine-readable data formats",
    "response_time": "9.93s",
    "fused_chunks": [
      {
        "chunk_id": "Digital_media_0",
        "text": "In mass communication, digital media is any communication media that operates in conjunction with various encoded machine-readable data formats. Digital content can be created, viewed, distributed, mo...",
        "full_text": "In mass communication, digital media is any communication media that operates in conjunction with various encoded machine-readable data formats. Digital content can be created, viewed, distributed, modified, listened to, and preserved on a digital electronic device, including digital data storage media (in contrast to analog electronic media) and digital broadcasting. Digital is defined as any data represented by a series of digits, and media refers to methods of broadcasting or communicating this information. Together, digital media refers to mediums of digitized information broadcast through a screen and/or a speaker. This also includes text, audio, video, and graphics  are transmitted over the internet for consumption on digital devices. Digital media platforms, such as YouTube, Kick, and Twitch, accounted for viewership rates of 27.9 billion hours in 2020. A contributing factor to its part in what is commonly referred to as the digital revolution can be attributed to the use of interconnectivity. Examples\nExamples of digital media include software, digital images,web pages and websites, social media, digital data and databases, digital audio such as MP3, electronic documents and electronic books. Digital media often contrasts with print media, such as printed books, newspapers and magazines, and other traditional or analog media, such as photographic film, audio tapes or video tapes. Digital media has had a significantly broad and complex impact on society and culture. Combined with the Internet and personal computing, digital media has caused disruptive innovation in publishing, journalism, public relations, entertainment, education, commerce and politics. Digital media has also posed new challenges to copyright and intellectual property laws, fostering an open content movement in which content creators voluntarily give up some or all of their legal rights to their work. The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history, called the Information Age, perhaps leading to a paperless society in which all media are produced and consumed on computers.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_5",
        "text": "The transition has created some uncertainty about definitions. Digital media, new media, multimedia, and similar terms all have a relationship to both the engineering innovations and cultural impact o...",
        "full_text": "The transition has created some uncertainty about definitions. Digital media, new media, multimedia, and similar terms all have a relationship to both the engineering innovations and cultural impact of digital media. The blending of digital media with other media, and with cultural and social factors, is sometimes known as new media or \"the new media.\" Similarly, digital media seems to demand a new set of communications skills, called transliteracy, media literacy, or digital literacy. These skills include not only the ability to read and write—traditional literacy—but the ability to navigate the Internet, evaluate sources, and create digital content. The idea that we are moving toward a fully digital, paperless society is accompanied by the fear that we may soon—or currently—be facing a digital dark age, in which older media are no longer accessible on modern devices or using modern methods of scholarship. Digital media has a significant, wide-ranging and complex effect on society and culture. A senior engineer at Motorola named Martin Cooper was the first person to make a phone call on April 3, 1973. He decided the first phone call should be to a rival telecommunications company saying \"I'm speaking via a mobile phone\". Ten years later, Motorola released the Motorola DynaTAC, the first commercially available mobile phone. In the early 1990s Nokia released the Nokia 1011, the first mass-produced mobile phone. The number of smartphone users has increased dramatically, as has the commercial landscape. Android and iOS dominate the smartphone market. A study by Gartner found that in 2016 about 88% of the worldwide smartphones were Android while iOS had a market share of about 12%. About 85% of the mobile market revenue came from mobile games. The impact of the digital revolution can also be assessed by exploring the amount of worldwide mobile smart device users there are. This can be split into 2 categories; smart phone users and smart tablet users. Worldwide there are currently 2.32 billion smartphone users across the world. This figure is to exceed 2.87 billion by 2020.",
        "rrf_score": 0.0315136476426799,
        "dense_rank": 2,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_7",
        "text": "It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping a...",
        "full_text": "It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping and complex. Between 2000 and 2015, the print newspaper advertising revenue has fallen from $60 billion to a nearly $20 billion. Even one of the most popular days for papers, Sunday, has seen a 9% circulation decrease the lowest since 1945. In journalism, digital media and citizen journalism have led to the loss of thousands of jobs in print media and the bankruptcy of many major newspapers. But the rise of digital journalism has also created thousands of new jobs and specializations. E-books and self-publishing are changing the book industry, and digital textbooks and other media-inclusive curricula are changing primary and secondary education. In academia, digital media has led to a new form of scholarship, also called digital scholarship, making open access and open science possible thanks to the low cost of distribution. New fields of study have grown, such as digital humanities and digital history. It has changed the way libraries are used and their role in society. Every major media, communications and academic endeavor is facing a period of transition and uncertainty related to digital media. Often time the magazine or publisher have a Digital edition which can be referred to an electronic formatted version identical to the print version. There is a huge benefit to the publisher and cost, as half of traditional publishers' costs come from production, including raw materials, technical processing, and distribution. Since 2004, there has been a decrease in newspaper industry employment, with only about 40,000 people working in the workforce currently. Alliance of Audited Media & Publishers information during the 2008 recession, over 10% of print sales are diminished for certain magazines, with a hardship coming from only 75% of the sales advertisements as before. However, in 2018, major newspapers advertising revenue was 35% from digital ads. In contrast, mobile versions of newspapers and magazines came in second with a huge growth of 135%.",
        "rrf_score": 0.03007688828584351,
        "dense_rank": 6,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_3",
        "text": "The year 2002 is assumed to be the year when human kind was able to store more information in digital than in analog media (the \"beginning of the digital age\"). Digital computers\nThough they used mach...",
        "full_text": "The year 2002 is assumed to be the year when human kind was able to store more information in digital than in analog media (the \"beginning of the digital age\"). Digital computers\nThough they used machine-readable media, Babbage's engines, player pianos, jacquard looms and many other early calculating machines were themselves analog computers, with physical, mechanical parts. The first truly digital media came into existence with the rise of digital computers. Digital computers use binary code and Boolean logic to store and process information, allowing one machine in one configuration to perform many different tasks. The first modern, programmable, digital computers, the Manchester Mark 1 and the EDSAC, were independently invented between 1948 and 1949. Though different in many ways from modern computers, these machines had digital software controlling their logical operations. They were encoded in binary, a system of ones and zeroes that are combined to make hundreds of characters. The 1s and 0s of binary are the \"digits\" of digital media. \"As We May Think\"\nWhile digital media did not come into common use until the late 20th century, the conceptual foundation of digital media is traced to the work of scientist and engineer Vannevar Bush and his celebrated essay \"As We May Think\", published in The Atlantic Monthly in 1945. Bush envisioned a system of devices that could be used to help scientists, doctors, and historians, among others, to store, analyze and communicate information. Calling this then-imaginary device a \"memex\", Bush wrote:\n\nThe owner of the memex, let us say, is interested in the origin and properties of the bow and arrow. Specifically, he is studying why the short Turkish bow was apparently superior to the English long bow in the skirmishes of the Crusades. He has dozens of possibly pertinent books and articles in his memex. First, he runs through an encyclopedia, finds an interesting but sketchy article, and leaves it projected. Next, in history, he finds another pertinent item and ties the two together. Thus he goes, building a trail of many items.",
        "rrf_score": 0.029631255487269532,
        "dense_rank": 7,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_6",
        "text": "This can be split into 2 categories; smart phone users and smart tablet users. Worldwide there are currently 2.32 billion smartphone users across the world. This figure is to exceed 2.87 billion by 20...",
        "full_text": "This can be split into 2 categories; smart phone users and smart tablet users. Worldwide there are currently 2.32 billion smartphone users across the world. This figure is to exceed 2.87 billion by 2020. Smart tablet users reached a total of 1 billion in 2015, 15% of the world's population. The statistics evidence the impact of digital media communications today. What is also of relevance is the fact that the number of smart device users is rising rapidly yet the amount of functional uses increase daily. A smartphone or tablet can be used for hundreds of daily needs. There are currently over 1 million apps on the Apple App store. These represent significant opportunities for digital marketing strategies. A smartphone user is impacted with digital advertising every second they open their Apple or Android device. This further evidences the digital revolution and the impact of revolution. This has resulted in a total of 13 billion dollars being paid out to the various app developers over the years. This growth has fueled the development of millions of software applications. Most of these apps are able to generate income via in app advertising. Gross revenue for 2020 is projected to be about $189 million. Disruption in industry\nCompared with print media, the mass media, and other analog technologies, digital media are easy to copy, store, share and modify. This quality of digital media has led to significant changes in many industries, especially journalism, publishing, education, entertainment, and the music business. The overall effect of these changes is so far-reaching that it is difficult to quantify. For example, in movie-making, the transition from analog film cameras to digital cameras is nearly complete. The transition has economic benefits to Hollywood, making distribution easier and making it possible to add high-quality digital effects to films. At the same time, it has affected the analog special effects, stunt, and animation industries in Hollywood. It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping and complex.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      }
    ]
  },
  {
    "question": "What is HCI concerned with?",
    "answer": "Interaction between humans and computers.",
    "source_id": "Human–computer interaction",
    "url": "https://en.wikipedia.org/wiki/Human–computer_interaction",
    "category": "factual",
    "generated_answer": "unanswerable",
    "response_time": "8.88s",
    "fused_chunks": [
      {
        "chunk_id": "Vaccine_13",
        "text": "The WHO works with national agencies to monitor inspections of manufacturing facilities and distributors for compliance with GMP and regulatory oversight. Some countries choose to buy vaccines license...",
        "full_text": "The WHO works with national agencies to monitor inspections of manufacturing facilities and distributors for compliance with GMP and regulatory oversight. Some countries choose to buy vaccines licensed by reputable national organizations, such as EMA, FDA, or national agencies in other affluent countries, but such purchases typically are more expensive and may not have distribution resources suitable to local conditions in developing countries. European Union\nIn the European Union (EU), vaccines for pandemic pathogens, such as seasonal influenza, are licensed EU-wide where all the member states comply (\"centralized\"), are licensed for only some member states (\"decentralized\"), or are licensed on an individual national level. Generally, all EU states follow regulatory guidance and clinical programs defined by the European Committee for Medicinal Products for Human Use (CHMP), a scientific panel of the European Medicines Agency (EMA) responsible for vaccine licensure. The CHMP is supported by several expert groups who assess and monitor the progress of a vaccine before and after licensure and distribution. United States\nUnder the FDA, the process of establishing evidence for vaccine clinical safety and efficacy is the same as for the approval process for prescription drugs. If successful through the stages of clinical development, the vaccine licensing process is followed by a Biologics License Application which must provide a scientific review team (from diverse disciplines, such as physicians, statisticians, microbiologists, chemists) and comprehensive documentation for the vaccine candidate having efficacy and safety throughout its development. Also during this stage, the proposed manufacturing facility is examined by expert reviewers for GMP compliance, and the label must have a compliant description to enable health care providers' definition of vaccine-specific use, including its possible risks, to communicate and deliver the vaccine to the public. After licensure, monitoring of the vaccine and its production, including periodic inspections for GMP compliance, continue as long as the manufacturer retains its license, which may include additional submissions to the FDA of tests for potency, safety, and purity for each vaccine manufacturing step.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Vaccine",
        "title": "Vaccine"
      },
      {
        "chunk_id": "Philosophy_3",
        "text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the ...",
        "full_text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: \"What can I know? \"; \"What should I do? \"; \"What may I hope? \"; and \"What is the human being?\" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as \"the midwife of the sciences\". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a \"rigorous science\" investigating essences.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Automation_13",
        "text": "The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry. Information technology, together with industrial machinery and processes, can assist in the ...",
        "full_text": "The improved design, analysis, and manufacture of products enabled by CAx has been beneficial for industry. Information technology, together with industrial machinery and processes, can assist in the design, implementation, and monitoring of control systems. One example of an industrial control system is a programmable logic controller (PLC). PLCs are specialized hardened computers which are frequently used to synchronize the flow of inputs from (physical) sensors and events with the flow of outputs to actuators and events. Human-machine interfaces (HMI) or computer human interfaces (CHI), formerly known as man-machine interfaces, are usually employed to communicate with PLCs and other computers. Service personnel who monitor and control through HMIs can be called by different names. In the industrial process and manufacturing environments, they are called operators or something similar. In boiler houses and central utility departments, they are called stationary engineers. Different types of automation tools exist:\n\nANN – Artificial neural network\nDCS – Distributed control system\nHMI – Human machine interface\nRPA – Robotic process automation\nSCADA – Supervisory control and data acquisition\nPLC – Programmable logic controller\nInstrumentation\nMotion control\nRobotics\nHost simulation software (HSS) is a commonly used testing tool that is used to test the equipment software. HSS is used to test equipment performance concerning factory automation standards (timeouts, response time, processing time). Cognitive automation\nCognitive automation, as a subset of AI, is an emerging genus of automation enabled by cognitive computing. Its primary concern is the automation of clerical tasks and workflows that consist of structuring unstructured data. Cognitive automation relies on multiple disciplines: natural language processing, real-time computing, machine learning algorithms, big data analytics, and evidence-based learning. According to Deloitte, cognitive automation enables the replication of human tasks and judgment \"at rapid speeds and considerable scale.\" Such tasks include:\n\nDocument redaction\nData extraction and document synthesis / reporting\nContract management\nNatural language search\nCustomer, employee, and stakeholder onboarding\nManual activities and verifications\nFollow-up and email communications\n\nRecent and emerging applications\nCAD AI\nArtificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate in 3D modeling.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Automation",
        "title": "Automation"
      },
      {
        "chunk_id": "Philosophy_10",
        "text": "It assumes that declarative knowledge is a combination of different parts and attempts to identify what those parts are. An influential theory in this area claims that knowledge has three components: ...",
        "full_text": "It assumes that declarative knowledge is a combination of different parts and attempts to identify what those parts are. An influential theory in this area claims that knowledge has three components: it is a belief that is justified and true. This theory is controversial and the difficulties associated with it are known as the Gettier problem. Alternative views state that knowledge requires additional components, like the absence of luck; different components, like the manifestation of cognitive virtues instead of justification; or they deny that knowledge can be analyzed in terms of other phenomena. Another area in epistemology asks how people acquire knowledge. Often-discussed sources of knowledge are perception, introspection, memory, inference, and testimony. According to empiricists, all knowledge is based on some form of experience. Rationalists reject this view and hold that some forms of knowledge, like innate knowledge, are not acquired through experience. The regress problem is a common issue in relation to the sources of knowledge and the justification they offer. It is based on the idea that beliefs require some kind of reason or evidence to be justified. The problem is that the source of justification may itself be in need of another source of justification. This leads to an infinite regress or circular reasoning. Foundationalists avoid this conclusion by arguing that some sources can provide justification without requiring justification themselves. Another solution is presented by coherentists, who state that a belief is justified if it coheres with other beliefs of the person. Many discussions in epistemology touch on the topic of philosophical skepticism, which raises doubts about some or all claims to knowledge. These doubts are often based on the idea that knowledge requires absolute certainty and that humans are unable to acquire it. Ethics\nEthics, also known as moral philosophy, studies what constitutes right conduct. It is also concerned with the moral evaluation of character traits and institutions. It explores what the standards of morality are and how to live a good life. Philosophical ethics addresses such basic questions as \"Are moral obligations relative? \"; \"Which has priority: well-being or obligation? \"; and \"What gives life meaning?\"",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Immanuel_Kant_28",
        "text": "The Doctrine of Right, published in 1797, contains Kant's most mature and systematic contribution to political philosophy. It addresses duties according to law, which are \"concerned only with protecti...",
        "full_text": "The Doctrine of Right, published in 1797, contains Kant's most mature and systematic contribution to political philosophy. It addresses duties according to law, which are \"concerned only with protecting the external freedom of individuals\" and indifferent to incentives. Although there is a moral duty \"to limit ourselves to actions that are right, that duty is not part of [right] itself\". Its basic political idea is that \"each person's entitlement to be his or her own master is only consistent with the entitlements of others if public legal institutions are in place\". He formulates the universal principle of right as:\n\nAny action is right if it can coexist with everyone's freedom in accordance with a universal law, or if on its maxim the freedom of choice of each can coexist with everyone's freedom in accordance with a universal law. (MS 6:230). Religious writings\nStarting in the 20th century, commentators tended to see Kant as having a strained relationship with religion, although in the nineteenth century this had not been the prevalent view. Karl Leonhard Reinhold, whose letters helped make Kant famous, wrote: \"I believe that I may infer without reservation that the interest of religion, and of Christianity in particular, accords completely with the result of the Critique of Reason.\" According to Johann Friedrich Schultz, who wrote one of the first commentaries on Kant: \"And does not this system itself cohere most splendidly with the Christian religion? Do not the divinity and beneficence of the latter become all the more evident?\" The reason for these views was Kant's moral theology and the widespread belief that his philosophy was the great antithesis to Spinozism, which was widely seen as a form of sophisticated pantheism or even atheism. As Kant's philosophy disregarded the possibility of arguing for God through pure reason alone, for the same reasons it also disregarded the possibility of arguing against God through pure reason alone. Kant directs his strongest criticisms of the organization and practices of religious organizations at those that encourage what he sees as a religion of counterfeit service to God.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Immanuel_Kant",
        "title": "Immanuel Kant"
      }
    ]
  },
  {
    "question": "What is computer vision?",
    "answer": "AI field for analyzing visual input.",
    "source_id": "Computer vision",
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "category": "factual",
    "generated_answer": "an interdisciplinary field that deals with how computers can be made to gain high-level understanding",
    "response_time": "13.01s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_vision_0",
        "text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical ...",
        "full_text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Definition\nComputer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_3",
        "text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, s...",
        "full_text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields. Signal processing\nYet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. Robotic navigation\nRobot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n\nVisual computing\nOther fields\nBesides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view.",
        "rrf_score": 0.031544957774465976,
        "dense_rank": 6,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_4",
        "text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how...",
        "full_text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry. Distinctions\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted:\n\nImage processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images.",
        "rrf_score": 0.03149801587301587,
        "dense_rank": 4,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_5",
        "text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. Thi...",
        "full_text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology. Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap.",
        "rrf_score": 0.03149801587301587,
        "dense_rank": 3,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_1",
        "text": "The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to appl...",
        "full_text": "The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree. History\nIn the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through an undergraduate summer project, by attaching a camera to a computer and having it \"describe what it saw\". What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation. The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields. By the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques.",
        "rrf_score": 0.030834914611005692,
        "dense_rank": 2,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      }
    ]
  },
  {
    "question": "What is IoT?",
    "answer": "A network of internet-connected devices.",
    "source_id": "Internet of things",
    "url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "category": "factual",
    "generated_answer": "IoT describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communication",
    "response_time": "14.15s",
    "fused_chunks": [
      {
        "chunk_id": "Internet_of_things_19",
        "text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devi...",
        "full_text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users. Function\nIoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, shares information, monitors, navigates and groups with other IoT devices in the same or nearby network realizing SIoT and facilitating useful service compositions in order to help its users proactively in every day's life especially during emergency. Examples\nIoT-based smart home technology monitors health data of patients or aging adults by analyzing their physiological parameters and prompt the nearby health facilities when emergency medical services needed. In case emergency, automatically, ambulance of a nearest available hospital will be called with pickup location provided, ward assigned, patient's health data will be transmitted to the emergency department, and display on the doctor's computer immediately for further action. IoT sensors on the vehicles, road and traffic lights monitor the conditions of the vehicles and drivers and alert when attention needed and also coordinate themselves automatically to ensure autonomous driving is working normally. Unfortunately if an accident happens, IoT camera will inform the nearest hospital and police station for help. Challenges\nInternet of things is multifaceted and complicated. One of the main factors that hindering people from adopting and use Internet of things (IoT) based products and services is its complexity. Installation and setup is a challenge to people, therefore, there is a need for IoT devices to mix match and configure themselves automatically to provide different services at different situation. System security always a concern for any technology, and it is more crucial for SIoT as not only security of oneself need to be considered but also the mutual trust mechanism between collaborative IoT devices from time to time, from place to place. Another critical challenge for SIoT is the accuracy and reliability of the sensors.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_16",
        "text": "Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power ha...",
        "full_text": "Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the Internet to a server with sufficient processing power. Decentralized IoT\nDecentralized Internet of things, or decentralized IoT, is a modified IoT which utilizes fog computing to handle and balance requests of connected IoT devices in order to reduce loading on the cloud servers and improve responsiveness for latency-sensitive IoT applications like vital signs monitoring of patients, vehicle-to-vehicle communication of autonomous driving, and critical failure detection of industrial devices. Performance is improved, especially for huge IoT systems with millions of nodes. Conventional IoT is connected via a mesh network and led by a major head node (centralized controller). The head node decides how a data is created, stored, and transmitted. In contrast, decentralized IoT attempts to divide IoT systems into smaller divisions. The head node authorizes partial decision-making power to lower level sub-nodes under mutual agreed policy. Some approaches to decentralized IoT attempt to address the limited bandwidth and hashing capacity of battery-powered devices via blockchain, although traditional linear chains often struggle with the throughput required for billions of devices. To resolve this, recent research focuses on sharded distributed ledger protocols (such as Cerberus) that utilize braided synchronization. These protocols theoretically allow network throughput to scale linearly with the number of nodes, enabling the massive concurrency required for autonomous machine-to-machine transactions. Complexity\nIn semi-open or closed loops (i.e., value chains, whenever a global finality can be settled) the IoT will often be considered and studied as a complex system due to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a chaotic environment (since systems always have finality). As a practical approach, not all elements on the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability.",
        "rrf_score": 0.03007688828584351,
        "dense_rank": 7,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_18",
        "text": "On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted ...",
        "full_text": "On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role on the Internet and the Web, geo-spatial standards will play a key role on the Internet of things. A solution to \"basket of remotes\"\nMany IoT devices have the potential to take a piece of this market. Jean-Louis Gassée (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note, where he predicts that the most likely problem will be what he calls the \"basket of remotes\" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another. For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, \"where collected data is used to predict and trigger actions on the specific devices\" while making them work together. Social Internet of things\nSocial Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices. SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services, and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering. Social Network for IoT Devices (Not Human)\nIoT defines a device with an identity like a citizen in a community and connect them to the Internet to provide services to its users. SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human. How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_0",
        "text": "The Internet of things (IoT) describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and syste...",
        "full_text": "The Internet of things (IoT) describes physical objects that are embedded with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communication networks. The IoT encompasses electronics, communication, and computer science engineering. \"Internet of things\" has been considered a misnomer because devices do not need to be connected to the public Internet; they only need to be connected to a network and be individually addressable. The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning. Traditional fields of embedded systems, wireless sensor networks, and control systems independently and collectively enable the Internet of Things. While in the consumer market, IoT technology is most synonymous with \"smart home\" products—including devices and appliances like thermostats and smart speakers—the technology's largest applications are in the business and industrial sectors. Commercial asset tracking and fleet management represent the largest single application of IoT, accounting for 22% of the total market, driven by the need to monitor mobile assets like vehicles and shipping containers. Other major applications include industrial monitoring, smart metering in utilities, and connected healthcare. However, several concerns exist regarding the risks associated with the growth and diffusion of IoT technologies and products, particularly in the areas of privacy and security. Consequently, several industries, technology companies, and governments (or their branches, ministries, bureaus, departments, etc.) of many countries have taken multiple steps and implemented a variety of precautionary measures to address these concerns adequately and minimize safety risks, including the development and implementation of international and local standards, guidelines, and regulatory frameworks. Due to their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly creates regulatory ambiguities, complicating jurisdictional boundaries of the data transfer.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_11",
        "text": "Between 2006 and January 2024, there were over 440 living labs (though not all are currently active) that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative an...",
        "full_text": "Between 2006 and January 2024, there were over 440 living labs (though not all are currently active) that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. When companies intend to implement and develop IoT services for smart cities, they need to have economic incentives. The US government plays a key role in smart city projects; changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the US government provides tax incentives and affordable rent, improves public transport, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share a common infrastructure and labor markets, and take advantage of locally embedded technologies, production processes, and transaction costs. Military\nThe Internet of Military Things (IoMT) is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an urban environment and involves the use of sensors, munitions, vehicles, robots, human-wearable biometrics, and other smart technology that is relevant on the battlefield. One of the examples of IOT devices used in the military is the Xaver 1000 system. The Xaver 1000 was developed by Israel's Camero Tech, which is the latest in the company's line of \"through-wall imaging systems.\" The Xaver line uses millimeter wave (MMW) radar, or radar in the range of 30-300 gigahertz. It is equipped with an AI-based life target tracking system as well as its own 3D 'sense-through-the-wall' technology. Internet of Battlefield Things\nThe Internet of Battlefield Things (IoBT) is a project initiated and executed by the U.S. Army Research Laboratory (ARL) that focuses on the basic science related to the IoT that enhances the capabilities of Army soldiers. In 2017, ARL launched the Internet of Battlefield Things Collaborative Research Alliance (IoBT-CRA), establishing a working collaboration between industry, university, and Army researchers to advance the theoretical foundations of IoT technologies and their applications to Army operations.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      }
    ]
  },
  {
    "question": "What is rendering in graphics?",
    "answer": "Producing a 2D image from a 3D model.",
    "source_id": "Computer graphics",
    "url": "https://en.wikipedia.org/wiki/Computer_graphics",
    "category": "factual",
    "generated_answer": "Graphics are visual representations on a surface, such as a computer screen. Examples are photographs, drawing, graphics designs, maps, engineering drawings, or other images",
    "response_time": "13.65s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_graphics_20",
        "text": "Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel ...",
        "full_text": "Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel typically has three subpixels such as red, green, and blue. Graphics are visual representations on a surface, such as a computer screen. Examples are photographs, drawing, graphics designs, maps, engineering drawings, or other images. Graphics often combine text and illustration. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style. Primitives\nPrimitives are basic units which a graphics system may combine to create more complex images or models. Examples would be sprites and character maps in 2D video games, geometric primitives in CAD, or polygons or triangles in 3D rendering. Primitives may be supported in hardware for efficient rendering, or the building blocks provided by a graphics application. Rendering\nRendering is the generation of a 2D image from a 3D model by means of computer programs. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texturing, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The rendering program is usually built into the computer graphics software, though others are available as plug-ins or entirely separate programs. The term \"rendering\" may be by analogy with an \"artist's rendering\" of a scene. Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_18",
        "text": "The most effective models have generally been trained on massive amounts of image and text data scraped from the web. By 2022, the best of these models, for example Dall-E 2 and Stable Diffusion, are ...",
        "full_text": "The most effective models have generally been trained on massive amounts of image and text data scraped from the web. By 2022, the best of these models, for example Dall-E 2 and Stable Diffusion, are able to create images in a range of styles, ranging from imitations of living artists to near-photorealistic, in a matter of seconds, given powerful enough hardware. Three-dimensional\n3D graphics, compared to 2D graphics, are graphics that use a three-dimensional representation of geometric data. For the purpose of performance, this is stored in the computer. This includes images that may be for later display or for real-time viewing. Despite these differences, 3D computer graphics rely on similar algorithms as 2D computer graphics do in the frame and raster graphics (like in 2D) in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques. 3D computer graphics are the same as 3D models. The model is contained within the graphical data file, apart from the rendering. However, there are differences that include the 3D model being the representation of any 3D object. Until visually displayed, a model is not graphic. Due to printing, 3D models are not only confined to virtual space. 3D rendering is how a model can be displayed. Also can be used in non-graphical computer simulations and calculations. Computer animation\nComputer animation is the art of creating moving images via the use of computers. It is a subfield of computer graphics and animation. Increasingly it is created by means of 3D computer graphics, though 2D computer graphics are still widely used for stylistic, low bandwidth, and faster real-time rendering needs. Sometimes the target of the animation is the computer itself, but sometimes the target is another medium, such as film. It is also referred to as CGI (Computer-generated imagery or computer-generated imaging), especially when used in films.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 3,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_21",
        "text": "Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeli...",
        "full_text": "Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU. A GPU is a device able to assist the CPU in calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the rendering equation. The rendering equation does not account for all lighting phenomena, but is a general lighting model for computer-generated imagery. 'Rendering' is also used to describe the process of calculating effects in a video editing file to produce final video output. 3D projection\n3D projection is a method of mapping three dimensional points to a two dimensional plane. As most current methods for displaying graphical data are based on planar two dimensional media, the use of this type of projection is widespread. This method is used in most real-time 3D applications and typically uses rasterization to produce the final image. Ray tracing\n\nRay tracing is a technique from the family of image order algorithms for generating an image by tracing the path of light through pixels in an image plane. The technique is capable of producing a high degree of photorealism; usually higher than that of typical scanline rendering methods, but at a greater computational cost. Shading\n\nShading refers to depicting depth in 3D models or illustrations by varying levels of darkness. It is a process used in drawing for depicting levels of darkness on paper by applying media more densely or with a darker shade for darker areas, and less densely or with a lighter shade for lighter areas. There are various techniques of shading including cross hatching where perpendicular lines of varying closeness are drawn in a grid pattern to shade an area. The closer the lines are together, the darker the area appears. Likewise, the farther apart the lines are, the lighter the area appears. The term has been recently generalized to mean that shaders are applied. Texture mapping\nTexture mapping is a method for adding detail, surface texture, or colour to a computer-generated graphic or 3D model.",
        "rrf_score": 0.03131881575727918,
        "dense_rank": 1,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_10",
        "text": "Modern computers, dating from the 1980s, often use graphical user interfaces (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key ...",
        "full_text": "Modern computers, dating from the 1980s, often use graphical user interfaces (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key elements of multimedia technology. In the field of realistic rendering, Japan's Osaka University developed the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, in 1982, for the purpose of rendering realistic 3D computer graphics. According to the Information Processing Society of Japan: \"The core of 3D image rendering is calculating the luminance of each pixel making up a rendered surface from the given viewpoint, light source, and object position. The LINKS-1 system was developed to realize an image rendering methodology in which each pixel could be parallel processed independently using ray tracing. By developing a new software methodology specifically for high-speed image rendering, LINKS-1 was able to rapidly render highly realistic images.\" The LINKS-1 was the world's most powerful computer, as of 1984. Also in the field of realistic rendering, the general rendering equation of David Immel and James Kajiya was developed in 1986 – an important step towards implementing global illumination, which is necessary to pursue photorealism in computer graphics. The continuing popularity of Star Wars and other science fiction franchises were relevant in cinematic CGI at this time, as Lucasfilm and Industrial Light & Magic became known as the \"go-to\" house by many other studios for topnotch computer graphics in film. Important advances in chroma keying (\"bluescreening\", etc.) were made for the later films of the original trilogy. Two other pieces of video would also outlast the era as historically relevant: Dire Straits' iconic, near-fully-CGI video for their song \"Money for Nothing\" in 1985, which popularized CGI among music fans of that era, and a scene from Young Sherlock Holmes the same year featuring the first fully CGI character in a feature movie (an animated stained-glass knight).",
        "rrf_score": 0.031024531024531024,
        "dense_rank": 6,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_17",
        "text": "Sprite graphics\nA sprite is a two-dimensional image or animation that is integrated into a larger scene. Initially including just graphical objects handled separately from the memory bitmap of a video...",
        "full_text": "Sprite graphics\nA sprite is a two-dimensional image or animation that is integrated into a larger scene. Initially including just graphical objects handled separately from the memory bitmap of a video display, this now includes various manners of graphical overlays. Originally, sprites were a method of integrating unrelated bitmaps so that they appeared to be part of the normal bitmap on a screen, such as creating an animated character that can be moved on a screen without altering the data defining the overall screen. Such sprites can be created by either electronic circuitry or software. In circuitry, a hardware sprite is a hardware construct that employs custom DMA channels to integrate visual elements with the main screen in that it super-imposes two discrete video sources. Software can simulate this through specialized rendering methods. Vector graphics\nVector graphics formats are complementary to raster graphics. Raster graphics is the representation of images as an array of pixels and is typically used for the representation of photographic images. Vector graphics consists of encoding information about shapes and colors that comprise the image, which can allow for more flexibility in rendering. There are instances when working with vector tools and formats is best practice, and instances when working with raster tools and formats is best practice. There are times when both formats come together. An understanding of the advantages and limitations of each technology and the relationship between them is most likely to result in efficient and effective use of tools. Generative machine-learning models\nSince the mid-2010s, as a result of advances in deep neural networks, models have been created which take as input a natural language description and produces as output an image matching that description. Text-to-image models generally combine a language model, which transforms the input text into a latent representation, and a generative image model, which produces an image conditioned on that representation. The most effective models have generally been trained on massive amounts of image and text data scraped from the web.",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 5,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      }
    ]
  },
  {
    "question": "What is qualitative research?",
    "answer": "Research using non-numeric data for meanings and themes.",
    "source_id": "Research",
    "url": "https://en.wikipedia.org/wiki/Research",
    "category": "factual",
    "generated_answer": "much more subjective non-quantitative, use different methods of collecting data, analyzing data, interpreting data",
    "response_time": "13.06s",
    "fused_chunks": [
      {
        "chunk_id": "Research_7",
        "text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen und...",
        "full_text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\n\nExploratory research, which helps to identify and define a problem or question. Constructive research, which tests theories and proposes solutions to a problem or question. Empirical research, which tests the feasibility of a solution using empirical evidence. There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\nQualitative research\nQualitative research refers to much more subjective non-quantitative, use different methods of collecting data, analyzing data, interpreting data for meanings, definitions, characteristics, symbols metaphors of things. Qualitative research further classified into the following types: Ethnography: This research mainly focus on culture of group of people which includes share attributes, language, practices, structure, value, norms and material things, evaluate human lifestyle. Ethno: people, Grapho: to write, this disciple may include ethnic groups, ethno genesis, composition, resettlement and social welfare characteristics. Phenomenology: It is very powerful strategy for demonstrating methodology to health professions education as well as best suited for exploring challenging problems in health professions educations. In addition, PMP researcher Mandy Sha argued that a project management approach is necessary to control the scope, schedule, and cost related to qualitative research design, participant recruitment, data collection, reporting, as well as stakeholder engagement. Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 4,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Research_8",
        "text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting num...",
        "full_text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive). Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism. The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that can be summarized, compared, and generalized to larger populations if the data are collected using proper sampling and data collection strategies. Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest. If the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment). If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants. In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible. Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.",
        "rrf_score": 0.03125763125763126,
        "dense_rank": 3,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Sociology_20",
        "text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to est...",
        "full_text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims. Sociologists are often divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. Quantitative methodologies hold the dominant position in sociology, especially in the United States. In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two. (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective, and the very term \"methodology\" is often used synonymously with \"statistics\". Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the general public, though this judgment continues to be challenged by antipositivists. The choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to obtain statistical patterns on a target sample, and then combined with a qualitative interview to determine the play of agency. Sampling\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population.",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 5,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Psychology_43",
        "text": "Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understa...",
        "full_text": "Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations are made. Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identified five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers sometimes aim to enrich our understanding of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's application of psychological and sociological theories, in his book Escape from Freedom, to understanding why many ordinary Germans supported Hitler. Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out. Program evaluation\nProgram evaluation involves the systematic collection, analysis, and application of information to answer questions about projects, policies and programs, particularly about their effectiveness. In both the public and private sectors, stakeholders often want to know the extent which the programs they are funding, implementing, voting for, receiving, or objecting to are producing the intended effects. While program evaluation first focuses on effectiveness, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful. Contemporary issues\nMetascience\nMetascience involves the application of scientific methodology to study science itself. The field of metascience has revealed problems in psychological research. Some psychological research has suffered from bias, problematic reproducibility, and misuse of statistics. These findings have led to calls for reform from within and from outside the scientific community.",
        "rrf_score": 0.030621785881252923,
        "dense_rank": 2,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Psychology",
        "title": "Psychology"
      }
    ]
  },
  {
    "question": "What is quantitative research?",
    "answer": "Research using numerical data and statistics.",
    "source_id": "Research",
    "url": "https://en.wikipedia.org/wiki/Research",
    "category": "factual",
    "generated_answer": "systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods",
    "response_time": "13.57s",
    "fused_chunks": [
      {
        "chunk_id": "Research_8",
        "text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting num...",
        "full_text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive). Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism. The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that can be summarized, compared, and generalized to larger populations if the data are collected using proper sampling and data collection strategies. Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest. If the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment). If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants. In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible. Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 1,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Sociology_20",
        "text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to est...",
        "full_text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims. Sociologists are often divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. Quantitative methodologies hold the dominant position in sociology, especially in the United States. In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two. (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective, and the very term \"methodology\" is often used synonymously with \"statistics\". Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the general public, though this judgment continues to be challenged by antipositivists. The choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to obtain statistical patterns on a target sample, and then combined with a qualitative interview to determine the play of agency. Sampling\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population.",
        "rrf_score": 0.03128054740957967,
        "dense_rank": 2,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Research_7",
        "text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen und...",
        "full_text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\n\nExploratory research, which helps to identify and define a problem or question. Constructive research, which tests theories and proposes solutions to a problem or question. Empirical research, which tests the feasibility of a solution using empirical evidence. There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\nQualitative research\nQualitative research refers to much more subjective non-quantitative, use different methods of collecting data, analyzing data, interpreting data for meanings, definitions, characteristics, symbols metaphors of things. Qualitative research further classified into the following types: Ethnography: This research mainly focus on culture of group of people which includes share attributes, language, practices, structure, value, norms and material things, evaluate human lifestyle. Ethno: people, Grapho: to write, this disciple may include ethnic groups, ethno genesis, composition, resettlement and social welfare characteristics. Phenomenology: It is very powerful strategy for demonstrating methodology to health professions education as well as best suited for exploring challenging problems in health professions educations. In addition, PMP researcher Mandy Sha argued that a project management approach is necessary to control the scope, schedule, and cost related to qualitative research design, participant recruitment, data collection, reporting, as well as stakeholder engagement. Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).",
        "rrf_score": 0.030886196246139225,
        "dense_rank": 9,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Sociology_19",
        "text": "Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is...",
        "full_text": "Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\"). A perennial question within this debate is that of \"social reproduction\": how are structures (specifically, structures producing inequality) reproduced through the choices of individuals? Synchrony and diachrony\nSynchrony and diachrony (or statics and dynamics) within social theory are terms that refer to a distinction that emerged through the work of Levi-Strauss who inherited it from the linguistics of Ferdinand de Saussure. Synchrony slices moments of time for analysis, thus it is an analysis of static social reality. Diachrony, on the other hand, attempts to analyse dynamic sequences. Following Saussure, synchrony would refer to social phenomena as a static concept like a language, while diachrony would refer to unfolding processes like actual speech. In Anthony Giddens' introduction to Central Problems in Social Theory, he states that, \"in order to show the interdependence of action and structure…we must grasp the time space relations inherent in the constitution of all social interaction.\" And like structure and agency, time is integral to discussion of social reproduction. In terms of sociology, historical sociology is often better positioned to analyse social life as diachronic, while survey research takes a snapshot of social life and is thus better equipped to understand social life as synchronized. Some argue that the synchrony of social structure is a methodological perspective rather than an ontological claim. Nonetheless, the problem for theory is how to integrate the two manners of recording and thinking about social data. Research methodology\nSociological research methods may be divided into two broad, though often supplementary, categories:\n\nQualitative designs emphasize understanding of social phenomena through direct observation, communication with participants, or analysis of texts, and may stress contextual and subjective accuracy over generality. Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Research_0",
        "text": "Research is creative and systematic work undertaken to increase the stock of knowledge. It involves the collection, organization, and analysis of evidence to increase understanding of a topic, charact...",
        "full_text": "Research is creative and systematic work undertaken to increase the stock of knowledge. It involves the collection, organization, and analysis of evidence to increase understanding of a topic, characterized by a particular attentiveness to controlling sources of bias and error. These activities are characterized by accounting and controlling for biases. A research project may be an expansion of past work in the field. To test the validity of instruments, procedures, or experiments, research may replicate elements of prior projects or the project as a whole. The primary purposes of basic research (as opposed to applied research) are documentation, discovery, interpretation, and the research and development (R&D) of methods and systems for the advancement of human knowledge. Approaches to research depend on epistemologies, which vary considerably both within and between humanities and sciences. There are several forms of research: scientific, humanities, artistic, economic, social, business, marketing, practitioner research, life, technological, etc. The scientific study of research practices is known as meta-research. A researcher is a person who conducts research. Etymology\nThe word research is derived from the Middle French \"recherche\", which means \"to go about seeking\", the term itself being derived from the Old French term \"recerchier\", a compound word from \"re-\" + \"cerchier\", or \"sercher\", meaning 'search'. The earliest recorded use of the term was in 1577. Definitions\nResearch, in its simplest terms, is a intentional search for knowledge. John W. Creswell states that \"research is a process of steps used to collect and analyze information to increase our understanding of a topic or issue\". The Merriam-Webster Online Dictionary defines research to also include studying already existing knowledge: \"studious inquiry or examination; especially: investigation or experimentation aimed at the discovery and interpretation of facts, revision of accepted theories or laws in the light of new facts, or practical application of such new or revised theories or laws\".",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      }
    ]
  },
  {
    "question": "What is an export?",
    "answer": "A good sent to another country.",
    "source_id": "International trade",
    "url": "https://en.wikipedia.org/wiki/International_trade",
    "category": "factual",
    "generated_answer": "the act of obtaining the files from the repositories",
    "response_time": "9.35s",
    "fused_chunks": [
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Version_control_12",
        "text": "Conflict\nA conflict occurs when different parties make changes to the same document, and the system is unable to reconcile the changes. A user must resolve the conflict by combining the changes, or by...",
        "full_text": "Conflict\nA conflict occurs when different parties make changes to the same document, and the system is unable to reconcile the changes. A user must resolve the conflict by combining the changes, or by selecting one change in favour of the other. Delta compression\nMost revision control software uses delta compression, which retains only the differences between successive versions of files. This allows for more efficient storage of many different versions of files. Dynamic stream\nA stream in which some or all file versions are mirrors of the parent stream's versions. Export\nExporting is the act of obtaining the files from the repository. It is similar to checking out except that it creates a clean directory tree without the version-control metadata used in a working copy. This is often used prior to publishing the contents, for example. Fetch\nSee pull. Forward integration\nThe process of merging changes made in the main trunk into a development (feature or team) branch. Head\nAlso sometimes called tip, this refers to the most recent commit, either to the trunk or to a branch. The trunk and each branch have their own head, though HEAD is sometimes loosely used to refer to the trunk. Import\nImporting is the act of copying a local directory tree (that is not currently a working copy) into the repository for the first time. Initialize\nTo create a new, empty repository. Interleaved deltas\nSome revision control software uses Interleaved deltas, a method that allows storing the history of text based files in a more efficient way than by using Delta compression. Label\nSee tag. Locking\nWhen a developer locks a file, no one else can update that file until it is unlocked. Locking can be supported by the version control system, or via informal communications between developers (aka social locking). Mainline\nSimilar to trunk, but there can be a mainline for each branch. Merge\nA merge or integration is an operation in which two sets of changes are applied to a file or set of files.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Version_control",
        "title": "Version control"
      },
      {
        "chunk_id": "International_trade_1",
        "text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally re...",
        "full_text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not. However, in practical terms, carrying out trade at an international level is typically a more complex process than domestic trade. The main difference is that international trade is typically more costly than domestic trade. This is due to the fact that cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural differences, product safety, the legal system, and so on. Another difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries. Thus, international trade is mostly restricted to trade in goods and services, and only to a lesser extent to trade in capital, labour, or other factors of production. Trade in goods and services can serve as a substitute for trade in factors of production. Instead of importing a factor of production, a country can import goods that make intensive use of that factor of production and thus embody it. An example of this is the import of labor-intensive goods by the United States from China. Instead of importing Chinese labor, the United States imports goods that were produced with Chinese labor. One report in 2010, suggested that international trade was increased when a country hosted a network of immigrants, but the trade effect was weakened when the immigrants became assimilated into their new country. History\nThe history of international trade chronicles notable events that have affected trading among various economies. Theories and models\nThere are several models that seek to explain the factors behind international trade, the welfare consequences of trade and the pattern of trade. Most traded export products\nLargest countries or regions by total international trade\nThe following table is a list of the 30 largest trading states according to the World Trade Organization in 2024.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "Philosophy_3",
        "text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the ...",
        "full_text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: \"What can I know? \"; \"What should I do? \"; \"What may I hope? \"; and \"What is the human being?\" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as \"the midwife of the sciences\". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a \"rigorous science\" investigating essences.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Sociology_19",
        "text": "Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is...",
        "full_text": "Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\"). A perennial question within this debate is that of \"social reproduction\": how are structures (specifically, structures producing inequality) reproduced through the choices of individuals? Synchrony and diachrony\nSynchrony and diachrony (or statics and dynamics) within social theory are terms that refer to a distinction that emerged through the work of Levi-Strauss who inherited it from the linguistics of Ferdinand de Saussure. Synchrony slices moments of time for analysis, thus it is an analysis of static social reality. Diachrony, on the other hand, attempts to analyse dynamic sequences. Following Saussure, synchrony would refer to social phenomena as a static concept like a language, while diachrony would refer to unfolding processes like actual speech. In Anthony Giddens' introduction to Central Problems in Social Theory, he states that, \"in order to show the interdependence of action and structure…we must grasp the time space relations inherent in the constitution of all social interaction.\" And like structure and agency, time is integral to discussion of social reproduction. In terms of sociology, historical sociology is often better positioned to analyse social life as diachronic, while survey research takes a snapshot of social life and is thus better equipped to understand social life as synchronized. Some argue that the synchrony of social structure is a methodological perspective rather than an ontological claim. Nonetheless, the problem for theory is how to integrate the two manners of recording and thinking about social data. Research methodology\nSociological research methods may be divided into two broad, though often supplementary, categories:\n\nQualitative designs emphasize understanding of social phenomena through direct observation, communication with participants, or analysis of texts, and may stress contextual and subjective accuracy over generality. Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      }
    ]
  },
  {
    "question": "What is a knowledge base?",
    "answer": "Structured knowledge usable by a program.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "body of knowledge represented in a form that can be used by a program",
    "response_time": "8.98s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_2",
        "text": "A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowl...",
        "full_text": "A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. Planning and decision-making\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be.",
        "rrf_score": 0.03131881575727918,
        "dense_rank": 1,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Database_14",
        "text": "The World Wide Web is thus a large distributed hypertext database. A knowledge base (abbreviated KB, kb or Δ) is a special kind of database for knowledge management, providing the means for the comput...",
        "full_text": "The World Wide Web is thus a large distributed hypertext database. A knowledge base (abbreviated KB, kb or Δ) is a special kind of database for knowledge management, providing the means for the computerized collection, organization, and retrieval of knowledge. Also a collection of data representing problems with their solutions and related experiences. A mobile database can be carried on or synchronized from a mobile computing device. Operational databases store detailed data about the operations of an organization. They typically process relatively high volumes of updates using transactions. Examples include customer databases that record contact, credit, and demographic information about a business's customers, personnel databases that hold information such as salary, benefits, skills data about employees, enterprise resource planning systems that record details about product components, parts inventory, and financial databases that keep track of the organization's money, accounting and financial dealings. A parallel database seeks to improve performance through parallelization for tasks such as loading data, building indexes and evaluating queries. The major parallel DBMS architectures which are induced by the underlying hardware architecture are:\nShared memory architecture, where multiple processors share the main memory space, as well as other data storage. Shared disk architecture, where each processing unit (typically consisting of multiple processors) has its own main memory, but all units share the other storage. Shared-nothing architecture, where each processing unit has its own main memory and other storage. Probabilistic databases employ fuzzy logic to draw inferences from imprecise data. Real-time databases process transactions fast enough for the result to come back and be acted on right away. A spatial database can store the data with multidimensional features. The queries on such data include location-based queries, like \"Where is the closest hotel in my area?\". A temporal database has built-in time aspects, for example a temporal data model and a temporal version of SQL. More specifically the temporal aspects usually include valid-time and transaction-time. A terminology-oriented database builds upon an object-oriented database, often customized for a specific field.",
        "rrf_score": 0.0304147465437788,
        "dense_rank": 2,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Database",
        "title": "Database"
      },
      {
        "chunk_id": "Plato_9",
        "text": "Indeed, the apprehension of Forms may be at the base of the account required for justification, in that it offers foundational knowledge which itself needs no account, thereby avoiding an infinite reg...",
        "full_text": "Indeed, the apprehension of Forms may be at the base of the account required for justification, in that it offers foundational knowledge which itself needs no account, thereby avoiding an infinite regression. Ethics\nSeveral dialogues discuss ethics including virtue and vice, pleasure and pain, crime and punishment, and justice and medicine. Socrates presents the famous Euthyphro dilemma in the dialogue of the same name: \"Is the pious (τὸ ὅσιον) loved by the gods because it is pious, or is it pious because it is loved by the gods?\" (10a) In the Protagoras dialogue it is argued through Socrates that virtue is innate and cannot be learned, that no one does bad on purpose, and to know what is good results in doing what is good; that knowledge is virtue. In the Republic, Plato poses the question, \"What is justice?\" and by examining both individual justice and the justice that informs societies, Plato is able not only to inform metaphysics, but also ethics and politics with the question: \"What is the basis of moral and social obligation?\" Plato's well-known answer rests upon the fundamental responsibility to seek wisdom, wisdom which leads to an understanding of the Form of the Good. Plato views \"The Good\" as the supreme Form, somehow existing even \"beyond being\". In this manner, justice is obtained when knowledge of how to fulfill one's moral and political function in society is put into practice. Politics\nThe dialogues also discuss politics. Some of Plato's most famous doctrines are contained in the Republic as well as in the Laws and the Statesman. Because these opinions are not spoken directly by Plato and vary between dialogues, they cannot be straightforwardly assumed as representing Plato's own views. Socrates asserts that societies have a tripartite class structure corresponding to the appetite/spirit/reason structure of the individual soul. The appetite/spirit/reason are analogous to the castes of society. Productive (Workers) – the labourers, carpenters, plumbers, masons, merchants, farmers, ranchers, etc. These correspond to the \"appetite\" part of the soul.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Plato",
        "title": "Plato"
      },
      {
        "chunk_id": "Philosophy_9",
        "text": "Early African philosophy was primarily conducted and transmitted orally. It focused on community, morality, and ancestral ideas, encompassing folklore, wise sayings, religious ideas, and philosophical...",
        "full_text": "Early African philosophy was primarily conducted and transmitted orally. It focused on community, morality, and ancestral ideas, encompassing folklore, wise sayings, religious ideas, and philosophical concepts like Ubuntu. Systematic African philosophy emerged at the beginning of the 20th century. It discusses topics such as ethnophilosophy, négritude, pan-Africanism, Marxism, postcolonialism, the role of cultural identity, relativism, African epistemology, and the critique of Eurocentrism. Core branches\nPhilosophical questions can be grouped into several branches. These groupings allow philosophers to focus on a set of similar topics and interact with other thinkers who are interested in the same questions. Epistemology, ethics, logic, and metaphysics are sometimes listed as the main branches. There are many other subfields besides them and the different divisions are neither exhaustive nor mutually exclusive. For example, political philosophy, ethics, and aesthetics are sometimes linked under the general heading of value theory as they investigate normative or evaluative aspects. Furthermore, philosophical inquiry sometimes overlaps with other disciplines in the natural and social sciences, religion, and mathematics. Epistemology\nEpistemology is the branch of philosophy that studies knowledge. It is also known as theory of knowledge and aims to understand what knowledge is, how it arises, what its limits are, and what value it has. It further examines the nature of truth, belief, justification, and rationality. Some of the questions addressed by epistemologists include \"By what method(s) can one acquire knowledge? \"; \"How is truth established? \"; and \"Can we prove causal relations?\" Epistemology is primarily interested in declarative knowledge or knowledge of facts, like knowing that Princess Diana died in 1997. But it also investigates practical knowledge, such as knowing how to ride a bicycle, and knowledge by acquaintance, for example, knowing a celebrity personally. One area in epistemology is the analysis of knowledge. It assumes that declarative knowledge is a combination of different parts and attempts to identify what those parts are. An influential theory in this area claims that knowledge has three components: it is a belief that is justified and true.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Knowledge_management_6",
        "text": "Knowledge repositories (databases, bookmarking engines, etc.) Measuring and reporting intellectual capital (a way of making explicit knowledge for companies)\nKnowledge brokers (some organisational mem...",
        "full_text": "Knowledge repositories (databases, bookmarking engines, etc.) Measuring and reporting intellectual capital (a way of making explicit knowledge for companies)\nKnowledge brokers (some organisational members take on responsibility for a specific \"field\" and act as first reference on a specific subject)\nKnowledge farming (using note-taking software to cultivate a knowledge graph, part of knowledge agriculture)\nKnowledge capturing (refers to a process where trained people extract valuable or else desired knowledge from experts and embed it in databases)\n\nMotivations\nMultiple motivations lead organisations to undertake KM. Typical considerations include:\n\nMaking available increased knowledge content in the development and provision of products and services\nAchieving shorter development cycles\nImproving consistency of knowledge and standardized expert skills among staff\nFacilitating and managing innovation and organisational learning\nLeveraging expertise across the organisation\nIncreasing network connectivity between internal and external individuals\nManaging business environments and allowing employees to obtain relevant insights and ideas appropriate to their work\nSolving intractable or wicked problems\nManaging intellectual capital and assets in the workforce (such as the expertise and know-how possessed by key individuals or stored in repositories)\n\nKM technologies\nKnowledge management (KM) technology can be categorised:\n\nCollaborative software(Groupware)—Software that facilitates collaboration and sharing of organisational information. Such applications provide tools for threaded discussions, document sharing, organisation-wide uniform email, and other collaboration-related features. Workflow systems—Systems that allow the representation of processes associated with the creation, use and maintenance of organisational knowledge, such as the process of creating and utilise forms and documents. Content management and document management systems—Software systems that automate the process of creating web content and/or documents. Roles such as editors, graphic designers, writers and producers can be explicitly modeled along with the tasks in the process and validation criteria. Commercial vendors started either to support documents or to support web content but as the Internet grew these functions merged and vendors now perform both functions. Enterprise portals—Software that aggregates information across the entire organisation or for groups such as project teams. eLearning—Software that enables organisations to create customised training and education. This can include lesson plans, monitoring progress and online classes. Planning and scheduling software—Software that automates schedule creation and maintenance.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Knowledge_management",
        "title": "Knowledge management"
      }
    ]
  },
  {
    "question": "What is an ontology in AI?",
    "answer": "A formal set of concepts and relations in a domain.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "set of objects, relations, concepts, and properties used by a particular domain of knowledge",
    "response_time": "9.11s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_2",
        "text": "A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowl...",
        "full_text": "A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge. Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications. Planning and decision-making\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be.",
        "rrf_score": 0.03131881575727918,
        "dense_rank": 1,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Natural_language_processing_12",
        "text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantic...",
        "full_text": "The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization. Document AI\nA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants. Dialogue management\nComputer systems intended to converse with a human. Question answering\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada? \"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Text-to-image generation\nGiven a description of an image, generate an image that matches the description. Text-to-scene generation\nGiven a description of a scene, generate a 3D model of the scene. Text-to-video\nGiven a description of a video, generate a video that matches the description. General tendencies and (possible) future directions\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:\n\nInterest on increasingly abstract, \"cognitive\" aspects of natural language (1999–2001: shallow parsing, 2002–03: named entity recognition, 2006–09/2017–18: dependency syntax, 2004–05/2008–09 semantic role labelling, 2011–12 coreference, 2015–16: discourse parsing, 2019: semantic parsing).",
        "rrf_score": 0.029857397504456328,
        "dense_rank": 8,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.028991596638655463,
        "dense_rank": 10,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Philosophy_13",
        "text": "\"; \"Of what does reality ultimately consist? \"; and \"Are humans free?\" Metaphysics is sometimes divided into general metaphysics and specific or special metaphysics. General metaphysics investigates b...",
        "full_text": "\"; \"Of what does reality ultimately consist? \"; and \"Are humans free?\" Metaphysics is sometimes divided into general metaphysics and specific or special metaphysics. General metaphysics investigates being as such. It examines the features that all entities have in common. Specific metaphysics is interested in different kinds of being, the features they have, and how they differ from one another. An important area in metaphysics is ontology. Some theorists identify it with general metaphysics. Ontology investigates concepts like being, becoming, and reality. It studies the categories of being and asks what exists on the most fundamental level. Another subfield of metaphysics is philosophical cosmology. It is interested in the essence of the world as a whole. It asks questions including whether the universe has a beginning and an end and whether it was created by something else. A key topic in metaphysics concerns the question of whether reality only consists of physical things like matter and energy. Alternative suggestions are that mental entities (such as souls and experiences) and abstract entities (such as numbers) exist apart from physical things. Another topic in metaphysics concerns the problem of identity. One question is how much an entity can change while still remaining the same entity. According to one view, entities have essential and accidental features. They can change their accidental features but they cease to be the same entity if they lose an essential feature. A central distinction in metaphysics is between particulars and universals. Universals, like the color red, can exist at different locations at the same time. This is not the case for particulars including individual persons or specific objects. Other metaphysical questions are whether the past fully determines the present and what implications this would have for the existence of free will. Other major branches\nThere are many other subfields of philosophy besides its core branches. Some of the most prominent are aesthetics, philosophy of language, philosophy of mind, philosophy of religion, philosophy of science, and political philosophy. Aesthetics in the philosophical sense is the field that studies the nature and appreciation of beauty and other aesthetic properties, like the sublime.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "What is gradient descent?",
    "answer": "An optimization method minimizing loss.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "Does it converge?",
    "response_time": "8.41s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_6",
        "text": "Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is t...",
        "full_text": "Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal. Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position. Local search\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally. Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm. Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation. Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails). Logic\nFormal logic is used for reasoning and knowledge representation. Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\"). Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Deep_learning_4",
        "text": "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep net...",
        "full_text": "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\". The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 2,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_6",
        "text": "In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training. Sepp ...",
        "full_text": "In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. The \"P\" in ChatGPT refers to such pre-training. Sepp Hochreiter's diploma thesis (1991) implemented the neural history compressor, and identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995. LSTM can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999, which became the standard RNN architecture. In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs). During 1985–1995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine, restricted Boltzmann machine, Helmholtz machine, and the wake-sleep algorithm. These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p. 112 ). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics. Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 5,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Machine_learning_0",
        "text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus...",
        "full_text": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework. History\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used during this time period. The earliest machine learning program was introduced in the 1950s when Arthur Samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "What is Moore’s law?",
    "answer": "Transistor density doubling about every 18 months.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "The transistor density in integrated circuits has been observed to roughly double every 18 months",
    "response_time": "11.66s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_11",
        "text": "Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant. The transistor density in integrated cir...",
        "full_text": "Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant. The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang. Applications\nAI and machine learning technology is used in most of the essential applications of the 2020s, including:\nsearch engines (such as Google Search)\ntargeting online advertisements\nrecommendation systems (offered by Netflix, YouTube or Amazon) driving internet traffic\ntargeted advertising (AdSense, Facebook)\nvirtual assistants (such as Siri or Alexa)\nautonomous vehicles (including drones, ADAS and self-driving cars)\nautomatic language translation (Microsoft Translator, Google Translate)\nfacial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet)\nimage labeling (used by Facebook, Apple's Photos and TikTok). The deployment of AI may be overseen by a chief automation officer (CAO). Health and medicine\nIt has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold. Games\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy!",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Arab_Caliphate_31",
        "text": "Accountability of rulers\nAl-Mawardi said that if the rulers meet their Islamic responsibilities to the public the people must obey their laws, but a caliph or ruler who becomes either unjust or severe...",
        "full_text": "Accountability of rulers\nAl-Mawardi said that if the rulers meet their Islamic responsibilities to the public the people must obey their laws, but a caliph or ruler who becomes either unjust or severely ineffective must be impeached via the Majlis al-Shura. Al-Juwayni argued that Islam is the goal of the ummah, so any ruler who deviates from this goal must be impeached. Al-Ghazali believed that oppression by a caliph is sufficient grounds for impeachment. Rather than just relying on impeachment, Ibn Hajar al-Asqalani stated that the people have an obligation to rebel if the caliph begins to act with no regard for Islamic law. Ibn Hajar al-Asqalani said that to ignore such a situation is haraam and those who cannot revolt from inside the caliphate should launch a struggle from outside. Al-Asqalani used two ayahs from the Qur'an to justify this:\n\nAnd they (the sinners on qiyama) will say, \"Our Lord! We obeyed our leaders and elite, but they led us astray from the ˹Right˺ Way. Our Lord! Give them double ˹our˺ punishment, and condemn them tremendously.\" Islamic lawyers commented that when the rulers refuse to step down after being impeached through the Majlis, becoming dictators through the support of a corrupt army, if the majority is in agreement they have the option to launch a revolution. Some noted that this option is to be exercised only after factoring in the potential cost of life. Rule of law\nThe following hadith establishes the principle of rule of law in relation to nepotism and accountability\n\nNarrated ‘Aisha: The people of Quraish worried about the lady from Bani Makhzum who had committed theft. They asked, \"Who will intercede for her with Allah's Apostle?\" Some said, \"No one dare to do so except Usama bin Zaid the beloved one to Allah's Apostle.\" When Usama spoke about that to Allah's Apostle; Allah's Apostle said: \"Do you try to intercede for somebody in a case connected with Allah’s Prescribed Punishments?\"",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Arab_Caliphate",
        "title": "Arab Caliphate"
      },
      {
        "chunk_id": "Arab_Caliphate_32",
        "text": "When Usama spoke about that to Allah's Apostle; Allah's Apostle said: \"Do you try to intercede for somebody in a case connected with Allah’s Prescribed Punishments?\" Then he got up and delivered a ser...",
        "full_text": "When Usama spoke about that to Allah's Apostle; Allah's Apostle said: \"Do you try to intercede for somebody in a case connected with Allah’s Prescribed Punishments?\" Then he got up and delivered a sermon saying, \"What destroyed the nations preceding you, was that if a noble amongst them stole, they would forgive him, and if a poor person amongst them stole, they would inflict Allah's Legal punishment on him. By Allah, if Fatima, the daughter of Muhammad (my daughter) stole, I would cut off her hand.\" Various Islamic lawyers, however, place multiple conditions and stipulations on the execution of such a law, making it difficult to implement. For example, the poor cannot be penalised for stealing out of poverty, and during a time of drought in the Rashidun Caliphate, capital punishment was suspended until the effects of the drought passed. Islamic jurists later formulated the concept that all classes were subject to the law of the land, and no person is above the law; officials and private citizens alike have a duty to obey the same law. Furthermore, a Qadi (Islamic judge) was not allowed to discriminate on the grounds of religion, race, colour, kinship or prejudice. In a number of cases, caliphs had to appear before judges as they prepared to render their verdict. According to Noah Feldman, a law professor at Harvard University, the system of legal scholars and jurists responsible for the rule of law was replaced by the codification of Sharia by the Ottoman Empire in the early nineteenth century:\n\nEconomy\nDuring the Muslim Agricultural Revolution, the caliphate understood that real incentives were needed to increase productivity and wealth and thus enhance tax revenues. A social transformation took place as a result of changing land ownership giving individuals of any gender, ethnic or religious background the right to buy, sell, mortgage and inherit land for farming or any other purpose. Signatures were required on contracts for every major financial transaction concerning agriculture, industry, commerce and employment. Copies of the contract were usually kept by both parties involved.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Arab_Caliphate",
        "title": "Arab Caliphate"
      },
      {
        "chunk_id": "Utilitarianism_16",
        "text": "Such assumptions, for example, the existence of a personal God who rewards and punishes the agent in the afterlife, could reconcile egoism and utilitarianism. But without them, we have to admit a \"dua...",
        "full_text": "Such assumptions, for example, the existence of a personal God who rewards and punishes the agent in the afterlife, could reconcile egoism and utilitarianism. But without them, we have to admit a \"dualism of practical reason\" that constitutes a \"fundamental contradiction\" in our moral consciousness. Developments in the 20th century\nIdeal utilitarianism\nThe description of ideal utilitarianism was first used by Hastings Rashdall in The Theory of Good and Evil (1907), but it is more often associated with G. E. Moore. In Ethics (1912), Moore rejects a purely hedonistic utilitarianism and argues that there is a range of values that might be maximized. Moore's strategy was to show that it is intuitively implausible that pleasure is the sole measure of what is good. He says that such an assumption:\n\ninvolves our saying, for instance, that a world in which absolutely nothing except pleasure existed—no knowledge, no love, no enjoyment of beauty, no moral qualities—must yet be intrinsically better—better worth creating—provided only the total quantity of pleasure in it were the least bit greater, than one in which all these things existed as well as pleasure. It involves our saying that, even if the total quantity of pleasure in each was exactly equal, yet the fact that all the beings in the one possessed, in addition knowledge of many different kinds and a full appreciation of all that was beautiful or worthy of love in their world, whereas none of the beings in the other possessed any of these things, would give us no reason whatever for preferring the former to the latter. Moore admits that it is impossible to prove the case either way, but he believed that it was intuitively obvious that even if the amount of pleasure stayed the same a world that contained such things as beauty and love would be a better world. He adds that, if a person was to take the contrary view, then \"I think it is self-evident that he would be wrong.\" Act and rule utilitarianism\nIn the mid-20th century, a number of philosophers focused on the place of rules in utilitarian thought.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Utilitarianism",
        "title": "Utilitarianism"
      },
      {
        "chunk_id": "Operating_system_11",
        "text": "With cooperative memory management, it takes only one misbehaved program to crash the system. Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods ...",
        "full_text": "With cooperative memory management, it takes only one misbehaved program to crash the system. Memory protection enables the kernel to limit a process' access to the computer's memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which does not exist in all computers. In both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt, which causes the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error. Windows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway. Virtual memory\nThe use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks. If a program tries to access memory that is not accessible memory, but nonetheless has been allocated to it, the kernel is interrupted (see § Memory management). This kind of interrupt is typically a page fault. When the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application's memory is stored, or even whether or not it has been allocated yet. In modern operating systems, memory which is accessed less frequently can be temporarily stored on a disk or other media to make that space available for use by other programs.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Operating_system",
        "title": "Operating system"
      }
    ]
  },
  {
    "question": "What is affective computing?",
    "answer": "AI that recognizes or simulates emotions.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "the design of systems and devices that can recognize, interpret, and process human emotions",
    "response_time": "8.60s",
    "fused_chunks": [
      {
        "chunk_id": "Emotion_24",
        "text": "Computer science\nIn the 2000s, research in computer science, engineering, psychology and neuroscience has been aimed at developing devices that recognize human affect display and model emotions. In co...",
        "full_text": "Computer science\nIn the 2000s, research in computer science, engineering, psychology and neuroscience has been aimed at developing devices that recognize human affect display and model emotions. In computer science, affective computing is a branch of the study and development of artificial intelligence that deals with the design of systems and devices that can recognize, interpret, and process human emotions. It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science. While the origins of the field may be traced as far back as to early philosophical enquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing. Detecting emotional information begins with passive sensors which capture data about the user's physical state or behavior without interpreting the input. The data gathered is analogous to the cues humans use to perceive emotions in others. Another area within affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities or that are capable of convincingly simulating emotions. Emotional speech processing recognizes the user's emotional state by analyzing speech patterns. The detection and processing of facial expression or body gestures is achieved through detectors and sensors. The role of artificial intelligence in relation to emotions in education is examined. Effects on memory\nEmotion affects the way autobiographical memories are encoded and retrieved. Emotional memories are reactivated more, they are remembered better and have more attention devoted to them. Through remembering our past achievements and failures, autobiographical memories affect how we perceive and feel about ourselves. Notable theorists\nIn the late 19th century, the most influential theorists were William James (1842–1910) and Carl Lange (1834–1900). James was an American psychologist and philosopher who wrote about educational psychology, psychology of religious experience/mysticism, and the philosophy of pragmatism. Lange was a Danish physician and psychologist. Working independently, they developed the James–Lange theory, a hypothesis on the origin and nature of emotions.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Emotion",
        "title": "Emotion"
      },
      {
        "chunk_id": "Artificial_intelligence_5",
        "text": "In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE tes...",
        "full_text": "In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications. Perception\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input. The field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception. Social intelligence\nAffective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction. However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject. General intelligence\nA machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence. Techniques\nAI research uses a wide variety of techniques to accomplish the goals above. Search and optimization\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search. State space search\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 2,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Leadership_13",
        "text": "Leaders transmit their moods to other group members through the mechanism of emotional contagion. Mood contagion may be one of the psychological mechanisms by which charismatic leaders influence follo...",
        "full_text": "Leaders transmit their moods to other group members through the mechanism of emotional contagion. Mood contagion may be one of the psychological mechanisms by which charismatic leaders influence followers. The affective tone of the group\nGroup affective tone represents the consistent or homogeneous affective reactions within a group. Group affective tone is an aggregate of the moods of the individual members of the group and refers to mood at the group level of analysis. Groups with leaders in a positive mood have a more positive affective tone than do groups with leaders in a negative mood. Group processes like coordination, effort expenditure, and task strategy\nPublic expressions of mood impact how group members think and act. When people experience and express mood, they send signals to others. Leaders signal their goals, intentions, and attitudes through their expressions of moods. For example, expressions of positive moods by leaders signal that leaders deem progress toward goals to be good. The group members respond to those signals cognitively and behaviorally in ways that are reflected in the group processes. In research about client service, it was found that expressions of positive mood by the leader improve the performance of the group, although in other sectors there were other findings. Beyond the leader's mood, her/his behavior is a source for employee positive and negative emotions at work. The leader's behavior creates situations and events that lead to emotional response, for example by giving feedback, allocating tasks, and distributing resources. Since employee behavior and productivity are affected by their emotional states, it is imperative to consider employee emotional responses to organizational leaders. Emotional intelligence – the ability to understand and manage moods and emotions in the self and others – contributes to effective leadership within organizations. Neo-emergent theory\nThe neo-emergent leadership theory (from the Oxford Strategic Leadership Programme)\nsees leadership as an impression formed through the communication of information by the leader or by other stakeholders, not through the actions of the leader. In other words, the reproduction of information or stories form the basis of the perception of leadership by the majority.",
        "rrf_score": 0.029631255487269532,
        "dense_rank": 7,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Leadership",
        "title": "Leadership"
      },
      {
        "chunk_id": "Immanuel_Kant_10",
        "text": "In 2010, the university was again renamed to Immanuel Kant Baltic Federal University. Philosophy\nLike many of his contemporaries, Kant was greatly impressed with the scientific advances made by Sir Is...",
        "full_text": "In 2010, the university was again renamed to Immanuel Kant Baltic Federal University. Philosophy\nLike many of his contemporaries, Kant was greatly impressed with the scientific advances made by Sir Isaac Newton and others. This new evidence of the power of human reason called into question for many the traditional authority of politics and religion. In particular, the modern mechanistic view of the world called into question the very possibility of morality; for, if there is no agency, there cannot be any responsibility. The aim of Kant's critical project is to secure human autonomy, the basis of religion and morality, from this threat of mechanism—and to do so in a way that preserves the advances of modern science. In the Critique of Pure Reason, Kant summarizes his philosophical concerns in the following three questions:\n\nWhat can I know? What should I do? What may I hope? The Critique of Pure Reason focuses upon the first question and opens a conceptual space for an answer to the second question. It argues that even though we cannot strictly know that we are free, we can—and for practical purposes, must—think of ourselves as free. In Kant's own words, \"I had to deny knowledge in order to make room for faith.\" Our rational faith in morality is further developed in the Groundwork of the Metaphysics of Morals and the Critique of Practical Reason. The Critique of the Power of Judgment argues we may rationally hope for the harmonious unity of the theoretical and practical domains treated in the first two Critiques on the basis, not only of its conceptual possibility, but also on the basis of our affective experience of natural beauty and, more generally, the organization of the natural world. In Religion within the Bounds of Mere Reason, Kant endeavors to complete his answer to this third question. These works all place the active, rational human subject at the center of the cognitive and moral worlds.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Immanuel_Kant",
        "title": "Immanuel Kant"
      },
      {
        "chunk_id": "Emotion_0",
        "text": "Emotions are physical and mental states brought on by neurophysiological changes, variously associated with thoughts, feelings, behavioral responses, and a degree of pleasure or displeasure. There is ...",
        "full_text": "Emotions are physical and mental states brought on by neurophysiological changes, variously associated with thoughts, feelings, behavioral responses, and a degree of pleasure or displeasure. There is no scientific consensus on a definition. Emotions are often intertwined with mood, temperament, personality, disposition, or creativity. Research on emotion has increased over the past two decades, with many fields contributing, including psychology, medicine, history, sociology of emotions, computer science and philosophy. The numerous attempts to explain the origin, function, and other aspects of emotions have fostered intense research on this topic. Theorizing about the evolutionary origin and possible purpose of emotion dates back to Charles Darwin. Current areas of research include the neuroscience of emotion, using tools like PET and fMRI scans to study the affective picture processes in the brain. From a mechanistic perspective, emotions can be defined as \"a positive or negative experience that is associated with a particular pattern of physiological activity\". Emotions are complex, involving multiple different components, such as subjective experience, cognitive processes, expressive behavior, psychophysiological changes, and instrumental behavior. At one time, academics attempted to identify the emotion with one of the components: William James with a subjective experience, behaviorists with instrumental behavior, psychophysiologists with physiological changes, and so on. More recently, emotion has been said to consist of all the components. The different components of emotion are categorized somewhat differently depending on the academic discipline. In psychology and philosophy, emotion typically includes a subjective, conscious experience characterized primarily by psychophysiological expressions, biological reactions, and mental states. A similar multi-componential description of emotion is found in sociology. For example, Peggy Thoits described emotions as involving physiological components, cultural or emotional labels (anger, surprise, etc. ), expressive body actions, and the appraisal of situations and contexts. Cognitive processes, like reasoning and decision-making, are often regarded as separate from emotional processes, making a division between \"thinking\" and \"feeling\". However, not all theories of emotion regard this separation as valid.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Emotion",
        "title": "Emotion"
      }
    ]
  },
  {
    "question": "What is sentiment analysis?",
    "answer": "Classifying emotional tone in text or media.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "Sentiment analysis involves identifying and classifying the emotional tone expressed in text",
    "response_time": "10.73s",
    "fused_chunks": [
      {
        "chunk_id": "Natural_language_processing_8",
        "text": "For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scrip...",
        "full_text": "For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives. This task is also referred to as token classification. Sentiment analysis (see also Multimodal sentiment analysis)\nSentiment analysis involves identifying and classifying the emotional tone expressed in text. This technique involves analyzing text to determine whether the expressed sentiment is positive, negative, or neutral. Models for sentiment classification typically utilize inputs such as word n-grams, Term Frequency-Inverse Document Frequency (TF-IDF) features, hand-generated features, or employ deep learning models designed to recognize both long-term and short-term dependencies in text sequences. The applications of sentiment analysis are diverse, extending to tasks such as categorizing customer reviews on various online platforms. Terminology extraction\nThe goal of terminology extraction is to automatically extract relevant terms from a given corpus. Word-sense disambiguation (WSD)\nMany words have more than one meaning; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet. Entity linking\nMany words—typically proper names—refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context. Relational semantics (semantics of individual sentences)\nRelationship extraction\nGiven a chunk of text, identify the relationships among named entities (e.g. who is married to whom). Semantic parsing\nGiven a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing).",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "title": "Natural language processing"
      },
      {
        "chunk_id": "Artificial_intelligence_5",
        "text": "In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE tes...",
        "full_text": "In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications. Perception\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input. The field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception. Social intelligence\nAffective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction. However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject. General intelligence\nA machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence. Techniques\nAI research uses a wide variety of techniques to accomplish the goals above. Search and optimization\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search. State space search\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers.",
        "rrf_score": 0.0315136476426799,
        "dense_rank": 2,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.02857142857142857,
        "dense_rank": 10,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Sociology_19",
        "text": "Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is...",
        "full_text": "Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\"). A perennial question within this debate is that of \"social reproduction\": how are structures (specifically, structures producing inequality) reproduced through the choices of individuals? Synchrony and diachrony\nSynchrony and diachrony (or statics and dynamics) within social theory are terms that refer to a distinction that emerged through the work of Levi-Strauss who inherited it from the linguistics of Ferdinand de Saussure. Synchrony slices moments of time for analysis, thus it is an analysis of static social reality. Diachrony, on the other hand, attempts to analyse dynamic sequences. Following Saussure, synchrony would refer to social phenomena as a static concept like a language, while diachrony would refer to unfolding processes like actual speech. In Anthony Giddens' introduction to Central Problems in Social Theory, he states that, \"in order to show the interdependence of action and structure…we must grasp the time space relations inherent in the constitution of all social interaction.\" And like structure and agency, time is integral to discussion of social reproduction. In terms of sociology, historical sociology is often better positioned to analyse social life as diachronic, while survey research takes a snapshot of social life and is thus better equipped to understand social life as synchronized. Some argue that the synchrony of social structure is a methodological perspective rather than an ontological claim. Nonetheless, the problem for theory is how to integrate the two manners of recording and thinking about social data. Research methodology\nSociological research methods may be divided into two broad, though often supplementary, categories:\n\nQualitative designs emphasize understanding of social phenomena through direct observation, communication with participants, or analysis of texts, and may stress contextual and subjective accuracy over generality. Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Philosophy_3",
        "text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the ...",
        "full_text": "Some definitions characterize philosophy in relation to its method, like pure reasoning. Others focus on its topic, for example, as the study of the biggest patterns of the world as a whole or as the attempt to answer the big questions. Such an approach is pursued by Immanuel Kant, who holds that the task of philosophy is united by four questions: \"What can I know? \"; \"What should I do? \"; \"What may I hope? \"; and \"What is the human being?\" Both approaches have the problem that they are usually either too wide, by including non-philosophical disciplines, or too narrow, by excluding some philosophical sub-disciplines. Many definitions of philosophy emphasize its intimate relation to science. In this sense, philosophy is sometimes understood as a proper science in its own right. According to some naturalistic philosophers, such as W. V. O. Quine, philosophy is an empirical yet abstract science that is concerned with wide-ranging empirical patterns instead of particular observations. Science-based definitions usually face the problem of explaining why philosophy in its long history has not progressed to the same extent or in the same way as the sciences. This problem is avoided by seeing philosophy as an immature or provisional science whose subdisciplines cease to be philosophy once they have fully developed. In this sense, philosophy is sometimes described as \"the midwife of the sciences\". Other definitions focus on the contrast between science and philosophy. A common theme among many such conceptions is that philosophy is concerned with meaning, understanding, or the clarification of language. According to one view, philosophy is conceptual analysis, which involves finding the necessary and sufficient conditions for the application of concepts. Another definition characterizes philosophy as thinking about thinking to emphasize its self-critical, reflective nature. A further approach presents philosophy as a linguistic therapy. According to Ludwig Wittgenstein, for instance, philosophy aims at dispelling misunderstandings to which humans are susceptible due to the confusing structure of ordinary language. Phenomenologists, such as Edmund Husserl, characterize philosophy as a \"rigorous science\" investigating essences.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Philosophy",
        "title": "Philosophy"
      }
    ]
  },
  {
    "question": "What is transfer learning?",
    "answer": "Applying learned knowledge to a new task.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "when the knowledge gained from one problem is applied to a new problem",
    "response_time": "8.87s",
    "fused_chunks": [
      {
        "chunk_id": "Learning_14",
        "text": "First, it is speculated that different types of transfer exist, including: near transfer, the application of skill to solve a novel problem in a similar context; and far transfer, the application of s...",
        "full_text": "First, it is speculated that different types of transfer exist, including: near transfer, the application of skill to solve a novel problem in a similar context; and far transfer, the application of skill to solve a novel problem presented in a different context. Furthermore, Perkins and Salomon (1992) suggest that positive transfer in cases when learning supports novel problem solving, and negative transfer occurs when prior learning inhibits performance on highly correlated tasks, such as second or third-language learning. Concepts of positive and negative transfer have a long history; researchers in the early 20th century described the possibility that \"...habits or mental acts developed by a particular kind of training may inhibit rather than facilitate other mental activities\". Finally, Schwarz, Bransford and Sears (2005) have proposed that transferring knowledge into a situation may differ from transferring knowledge out to a situation as a means to reconcile findings that transfer may both be frequent and challenging to promote. A significant and long research history has also attempted to explicate the conditions under which transfer of learning might occur. Early research by Ruger, for example, found that the \"level of attention\", \"attitudes\", \"method of attack\" (or method for tackling a problem), a \"search for new points of view\", a \"careful testing of hypothesis\" and \"generalization\" were all valuable approaches for promoting transfer. To encourage transfer through teaching, Perkins and Salomon recommend aligning (\"hugging\") instruction with practice and assessment, and \"bridging\", or encouraging learners to reflect on past experiences or make connections between prior knowledge and current content. Factors affecting learning\nInstructional techniques\nCertain techniques and factors can affect long-term retention:\n\nThe spacing effect means that lessons or studying spaced out over time (spaced repetition) are better than cramming due to the forgetting curve. Desirable difficulty\nLearning by teaching\n\"Self-explaining\" (paraphrasing material to oneself) rather than passive reading\nTesting effect and low-stakes quizzing\n\nGenetics\nSome aspects of intelligence are inherited genetically, so different learners to some degree have different abilities with regard to learning and speed of learning.",
        "rrf_score": 0.031544957774465976,
        "dense_rank": 1,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Learning",
        "title": "Learning"
      },
      {
        "chunk_id": "Artificial_intelligence_4",
        "text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other gui...",
        "full_text": "It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input). In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning. Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization. Natural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering. Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure. Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.",
        "rrf_score": 0.031054405392392875,
        "dense_rank": 2,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Learning_13",
        "text": "This type of learning occurs in part as a product of social interaction and active involvement in both online and onsite courses. Research implies that some un-assessed aspects of onsite and online le...",
        "full_text": "This type of learning occurs in part as a product of social interaction and active involvement in both online and onsite courses. Research implies that some un-assessed aspects of onsite and online learning challenge the equivalency of education between the two modalities. Both onsite and online learning have distinct advantages with traditional on-campus students experiencing higher degrees of incidental learning in three times as many areas as online students. Additional research is called for to investigate the implications of these findings both conceptually and pedagogically. Domains\nBenjamin Bloom has suggested three domains of learning in his taxonomy which are:\n\nCognitive: To recall, calculate, discuss, analyze, problem solve, etc. Psychomotor: To dance, swim, ski, dive, drive a car, ride a bike, etc. Affective: To like something or someone, love, appreciate, fear, hate, worship, etc. These domains are not mutually exclusive. For example, in learning to play chess, the person must learn the rules (cognitive domain)—but must also learn how to set up the chess pieces and how to properly hold and move a chess piece (psychomotor). Furthermore, later in the game the person may even learn to love the game itself, value its applications in life, and appreciate its history (affective domain). Transfer\nTransfer of learning is the application of skill, knowledge or understanding to resolve a novel problem or situation that happens when certain conditions are fulfilled. Research indicates that learning transfer is infrequent; most common when \"... cued, primed, and guided...\" and has sought to clarify what it is, and how it might be promoted through instruction. Over the history of its discourse, various hypotheses and definitions have been advanced. First, it is speculated that different types of transfer exist, including: near transfer, the application of skill to solve a novel problem in a similar context; and far transfer, the application of skill to solve a novel problem presented in a different context.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Learning",
        "title": "Learning"
      },
      {
        "chunk_id": "Developmental_psychology_11",
        "text": "This suggests that humans have an inbuilt need to develop strong bonds with caregivers in order to survive and be healthy. Childhood attachment styles can have an impact on how people behave in adult ...",
        "full_text": "This suggests that humans have an inbuilt need to develop strong bonds with caregivers in order to survive and be healthy. Childhood attachment styles can have an impact on how people behave in adult social situations, including romantic partnerships. Nature vs nurture\nA significant concern of developmental psychology is the relationship between innateness and environmental influences on development. This is often referred to as \"nature and nurture\" or nativism versus empiricism. A nativist account of development would argue that the processes in question are innate, that is, they are specified by the organism's genes. What makes a person who they are? Is it their environment or their genetics? This is the debate of nature vs nurture. According to an empiricist viewpoint, those processes are learned through interaction with the environment. Today most developmental psychologists take a more holistic approach, emphasizing the interaction between genetic and environmental influences. One of the ways this relationship has been explored in recent years is through the emerging field of evolutionary developmental psychology. The dispute over innateness has been well represented in the field of language acquisition studies. A major question in this area is whether or not certain properties of human language are specified genetically or can be acquired through learning. The empiricist position on the issue of language acquisition suggests that the language input provides the necessary information required for learning the structure of language and that infants acquire language through a process of statistical learning. From this perspective, language can be acquired via general learning methods that also apply to other aspects of development, such as perceptual learning. The nativist position argues that the input from language is too impoverished for infants and children to acquire the structure of language. Linguist Noam Chomsky asserts that, evidenced by the lack of sufficient information in the language input, there is a universal grammar that applies to all human languages and is pre-specified. This has led to the idea that there is a special cognitive module suited for learning language, often called the language acquisition device.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Developmental_psychology",
        "title": "Developmental psychology"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      }
    ]
  },
  {
    "question": "What is differential privacy?",
    "answer": "A method to protect individual data in datasets.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "factual",
    "generated_answer": "data aggregation, de-identification and differential privacy",
    "response_time": "8.84s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_17",
        "text": "Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy. AI developers argue that ...",
        "full_text": "Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy. AI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\" Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners can indicate that they do not want their content scraped via a \"robots.txt\" file. However, some companies will scrape content regardless because the robots.txt file has no real authority. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors. Dominance by tech giants\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace. Power needs and environmental impacts\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Internet_of_things_23",
        "text": "The other issues pertain to consumer choice and ownership of data and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ow...",
        "full_text": "The other issues pertain to consumer choice and ownership of data and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ownership continue to develop. IoT regulation depends on the country. Some examples of legislation that is relevant to privacy and data collection are: the US Privacy Act of 1974, OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data of 1980, and the EU Directive 95/46/EC of 1995. Current regulatory environment:\nA report published by the Federal Trade Commission (FTC) in January 2015 made the following three recommendations:\n\nData security – At the time of designing IoT companies should ensure that data collection, storage and processing would be secure at all times. Companies should adopt a \"defense in depth\" approach and encrypt data at each stage. Data consent – users should have a choice as to what data they share with IoT companies and the users must be informed if their data gets exposed. Data minimisation – IoT companies should collect only the data they need and retain the collected information only for a limited time. However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights. A resolution passed by the Senate in March 2015, is already being considered by the Congress. This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT ecosystem, in March 2016, a bipartisan group of four Senators proposed a bill, The Developing Innovation and Growing the Internet of Things (DIGIT) Act, to direct the Federal Communications Commission to assess the need for more spectrum to connect IoT devices. Approved on 28 September 2018, California Senate Bill No.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 2,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_35",
        "text": "Privacy and security concerns\nAs for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the \"things\" around the user can cooperate to provide bet...",
        "full_text": "Privacy and security concerns\nAs for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the \"things\" around the user can cooperate to provide better services that fulfill personal preference. When the collected information which describes a user in detail travels through multiple hops in a network, due to a diverse integration of services, devices and network, the information stored on a device is vulnerable to privacy violation by compromising nodes existing in an IoT network. For example, on 21 October 2016, a multiple distributed denial of service (DDoS) attacks systems operated by domain name system provider Dyn, which caused the inaccessibility of several websites, such as GitHub, Twitter, and others. This attack is executed through a botnet consisting of a large number of IoT devices including IP cameras, gateways, and even baby monitors. Fundamentally there are 4 security objectives that the IoT system requires: (1) data confidentiality: unauthorised parties cannot have access to the transmitted and stored data; (2) data integrity: intentional and unintentional corruption of transmitted and stored data must be detected; (3) non-repudiation: the sender cannot deny having sent a given message; (4) data availability: the transmitted and stored data should be available to authorised parties even with the denial-of-service (DOS) attacks. Information privacy regulations also require organisations to practice \"reasonable security\". California's SB-327 Information privacy: connected devices \"would require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorised access, destruction, use, modification, or disclosure, as specified\". As each organisation's environment is unique, it can prove challenging to demonstrate what \"reasonable security\" is and what potential risks could be involved for the business.",
        "rrf_score": 0.030621785881252923,
        "dense_rank": 9,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Cloud_computing_6",
        "text": "These variations in abstraction quality depend on the cloud vendor, service and architecture. Mitigating leaky abstractions requires users to understand the implementation details and limitations of t...",
        "full_text": "These variations in abstraction quality depend on the cloud vendor, service and architecture. Mitigating leaky abstractions requires users to understand the implementation details and limitations of the cloud services they utilize. Service lock-in within the same vendor\nService lock-in within the same vendor occurs when a customer becomes dependent on specific services within a cloud vendor, making it challenging to switch to alternative services within the same vendor when their needs change. Security and privacy\nCloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time. It could accidentally or deliberately alter or delete information. Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored. Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access. Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity. The systems work by creating and describing identities, recording activities, and getting rid of unused identities. According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. \"There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into\".",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Cloud_computing",
        "title": "Cloud computing"
      },
      {
        "chunk_id": "Deep_learning_20",
        "text": "Financial fraud detection\nDeep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering. Materials science\nIn November 2023, researchers at...",
        "full_text": "Financial fraud detection\nDeep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering. Materials science\nIn November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds. Military\nThe United States Department of Defense applied deep learning to train robots in new tasks through observation. Partial differential equations\nPhysics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on. It is evident that geometric and physical constraints have a synergistic effect on neural PDE surrogates, thereby enhancing their efficacy in predicting stable and super long rollouts. Deep backward stochastic differential equation method\nDeep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings.",
        "rrf_score": 0.015625,
        "dense_rank": 0,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      }
    ]
  },
  {
    "question": "Which uses labels: supervised or unsupervised learning?",
    "answer": "Supervised learning.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "comparative",
    "generated_answer": "unsupervised",
    "response_time": "6.69s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_8",
        "text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the...",
        "full_text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Other types\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. Self-learning\nSelf-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.",
        "rrf_score": 0.03225806451612903,
        "dense_rank": 2,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_3",
        "text": "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of...",
        "full_text": "Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. Data compression\nData mining\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimisation: Many learning problems are formulated as minimisation of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). Generalization\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 4,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_6",
        "text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm...",
        "full_text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm works for all problems. Supervised learning\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised.",
        "rrf_score": 0.03149801587301587,
        "dense_rank": 3,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_7",
        "text": "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in...",
        "full_text": "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself. Dimensionality reduction\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation. Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data).",
        "rrf_score": 0.03131881575727918,
        "dense_rank": 1,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_10",
        "text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervi...",
        "full_text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary.",
        "rrf_score": 0.030536130536130537,
        "dense_rank": 5,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "Which is numeric: qualitative or quantitative research?",
    "answer": "Quantitative.",
    "source_id": "Research",
    "url": "https://en.wikipedia.org/wiki/Research",
    "category": "comparative",
    "generated_answer": "Quantitative research Quantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods",
    "response_time": "15.70s",
    "fused_chunks": [
      {
        "chunk_id": "Research_8",
        "text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting num...",
        "full_text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive). Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism. The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that can be summarized, compared, and generalized to larger populations if the data are collected using proper sampling and data collection strategies. Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest. If the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment). If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants. In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible. Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 1,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Research_7",
        "text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen und...",
        "full_text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\n\nExploratory research, which helps to identify and define a problem or question. Constructive research, which tests theories and proposes solutions to a problem or question. Empirical research, which tests the feasibility of a solution using empirical evidence. There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\nQualitative research\nQualitative research refers to much more subjective non-quantitative, use different methods of collecting data, analyzing data, interpreting data for meanings, definitions, characteristics, symbols metaphors of things. Qualitative research further classified into the following types: Ethnography: This research mainly focus on culture of group of people which includes share attributes, language, practices, structure, value, norms and material things, evaluate human lifestyle. Ethno: people, Grapho: to write, this disciple may include ethnic groups, ethno genesis, composition, resettlement and social welfare characteristics. Phenomenology: It is very powerful strategy for demonstrating methodology to health professions education as well as best suited for exploring challenging problems in health professions educations. In addition, PMP researcher Mandy Sha argued that a project management approach is necessary to control the scope, schedule, and cost related to qualitative research design, participant recruitment, data collection, reporting, as well as stakeholder engagement. Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 3,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Sociology_20",
        "text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to est...",
        "full_text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims. Sociologists are often divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. Quantitative methodologies hold the dominant position in sociology, especially in the United States. In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two. (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective, and the very term \"methodology\" is often used synonymously with \"statistics\". Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the general public, though this judgment continues to be challenged by antipositivists. The choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to obtain statistical patterns on a target sample, and then combined with a qualitative interview to determine the play of agency. Sampling\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population.",
        "rrf_score": 0.0315136476426799,
        "dense_rank": 2,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.03149801587301587,
        "dense_rank": 4,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Psychology_39",
        "text": "The measurement and operationalization of important constructs is an essential part of these research designs. Although this type of psychological research is much less abundant than quantitative rese...",
        "full_text": "The measurement and operationalization of important constructs is an essential part of these research designs. Although this type of psychological research is much less abundant than quantitative research, some psychologists conduct qualitative research. This type of research can involve interviews, questionnaires, and first-hand observation. While hypothesis testing is rare, virtually impossible, in qualitative research, qualitative studies can be helpful in theory and hypothesis generation, interpreting seemingly contradictory quantitative findings, and understanding why some interventions fail and others succeed. Controlled experiments\nA true experiment with random assignment of research participants (sometimes called subjects) to rival conditions allows researchers to make strong inferences about causal relationships. When there are large numbers of research participants, the random assignment (also called random allocation) of those participants to rival conditions ensures that the individuals in those conditions will, on average, be similar on most characteristics, including characteristics that went unmeasured. In an experiment, the researcher alters one or more variables of influence, called independent variables, and measures resulting changes in the factors of interest, called dependent variables. Prototypical experimental research is conducted in a laboratory with a carefully controlled environment. A quasi-experiment is a situation in which different conditions are being studied, but random assignment to the different conditions is not possible. Investigators must work with preexisting groups of people. Researchers can use common sense to consider how much the nonrandom assignment threatens the study's validity. For example, in research on the best way to affect reading achievement in the first three grades of school, school administrators may not permit educational psychologists to randomly assign children to phonics and whole language classrooms, in which case the psychologists must work with preexisting classroom assignments. Psychologists will compare the achievement of children attending phonics and whole language classes and, perhaps, statistically adjust for any initial differences in reading level. Experimental researchers typically use a statistical hypothesis testing model which involves making predictions before conducting the experiment, then assessing how well the data collected are consistent with the predictions. These predictions are likely to originate from one or more abstract scientific hypotheses about how the phenomenon under study actually works.",
        "rrf_score": 0.02964426877470356,
        "dense_rank": 9,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Psychology",
        "title": "Psychology"
      }
    ]
  },
  {
    "question": "Which is older: AI or ML?",
    "answer": "AI.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "comparative",
    "generated_answer": "ML",
    "response_time": "7.05s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.030834914611005692,
        "dense_rank": 2,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_37",
        "text": "Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence...",
        "full_text": "Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence. As a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal, political and academic discourses. Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\". There has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text. Evaluating approaches to AI\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers. Symbolic AI and its limits\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\" However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_18",
        "text": "Risks\nExistential risks\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic...",
        "full_text": "Risks\nExistential risks\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\". The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench them, preventing moral progress. Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create an entrenched repressive worldwide totalitarian regime. There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass-created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe. Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\". Risk of loss of control and human extinction\nThe thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman. In 2014, Stephen Hawking criticized widespread indifference:\n\nSo, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_4",
        "text": "An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was e...",
        "full_text": "An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was echoed by Scott Aaronson during his time at OpenAI. AI-complete problems\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that AGI would be needed to solve it, because the solution is beyond the capabilities of a purpose-specific algorithm. Many problems have been conjectured to require general intelligence to solve. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance. However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning. History\nClassical AI\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's fictional character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\". Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "Which handles images: NLP or computer vision?",
    "answer": "Computer vision.",
    "source_id": "Computer vision",
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "category": "comparative",
    "generated_answer": "computer vision",
    "response_time": "7.33s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_vision_5",
        "text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. Thi...",
        "full_text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology. Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_4",
        "text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how...",
        "full_text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry. Distinctions\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted:\n\nImage processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 3,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_0",
        "text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical ...",
        "full_text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Definition\nComputer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.",
        "rrf_score": 0.03149801587301587,
        "dense_rank": 4,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_6",
        "text": "The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refe...",
        "full_text": "The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\n\nAutomatic inspection, e.g., in manufacturing applications;\nAssisting humans in identification tasks, e.g., a species identification system;\nControlling processes, e.g., an industrial robot;\nDetecting events, e.g., for visual surveillance or people counting, e.g., in the restaurant industry;\nInteraction, e.g., as the input to a device for computer-human interaction;\nMediaPipe, an open-source framework from Google for AI edge device computing, e.g., face detection, image classification, object detection;\nmonitoring agricultural crops, e.g. an open-source vision transformers model  has been developed to help farmers automatically detect strawberry diseases with 98.4% accuracy. Modeling objects or environments, e.g., medical image analysis or topographical modeling;\nNavigation, e.g., by an autonomous vehicle or mobile robot;\nOrganizing information, e.g., for indexing databases of images and image sequences. Tracking surfaces or planes in 3D coordinates for allowing Augmented Reality experiences. Analyzing the condition of facilities in industry or construction. Automatic real-time lip-reading for devices and apps to assist people with disabilities. For 2024, the leading areas of computer vision were industry (market size US$5.22 billion), medicine (market size US$2.6 billion), military (market size US$996.2 million). Medicine\nOne of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example.",
        "rrf_score": 0.031099324975891997,
        "dense_rank": 1,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Robotics_18",
        "text": "As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and...",
        "full_text": "As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras. In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities. There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology. Environmental interaction and navigation\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.",
        "rrf_score": 0.02877846790890269,
        "dense_rank": 9,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Robotics",
        "title": "Robotics"
      }
    ]
  },
  {
    "question": "Which is more costly: domestic or international trade?",
    "answer": "International trade.",
    "source_id": "International trade",
    "url": "https://en.wikipedia.org/wiki/International_trade",
    "category": "comparative",
    "generated_answer": "international trade",
    "response_time": "6.12s",
    "fused_chunks": [
      {
        "chunk_id": "International_trade_1",
        "text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally re...",
        "full_text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not. However, in practical terms, carrying out trade at an international level is typically a more complex process than domestic trade. The main difference is that international trade is typically more costly than domestic trade. This is due to the fact that cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural differences, product safety, the legal system, and so on. Another difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries. Thus, international trade is mostly restricted to trade in goods and services, and only to a lesser extent to trade in capital, labour, or other factors of production. Trade in goods and services can serve as a substitute for trade in factors of production. Instead of importing a factor of production, a country can import goods that make intensive use of that factor of production and thus embody it. An example of this is the import of labor-intensive goods by the United States from China. Instead of importing Chinese labor, the United States imports goods that were produced with Chinese labor. One report in 2010, suggested that international trade was increased when a country hosted a network of immigrants, but the trade effect was weakened when the immigrants became assimilated into their new country. History\nThe history of international trade chronicles notable events that have affected trading among various economies. Theories and models\nThere are several models that seek to explain the factors behind international trade, the welfare consequences of trade and the pattern of trade. Most traded export products\nLargest countries or regions by total international trade\nThe following table is a list of the 30 largest trading states according to the World Trade Organization in 2024.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "International_trade_0",
        "text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade repr...",
        "full_text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade represents a significant share of gross domestic product (GDP). While international trade has existed throughout history (for example Uttarapatha, Silk Road, Amber Road, salt roads), its economic, social, and political importance has been on the rise in recent centuries. Carrying out trade at an international level is a complex process when compared to domestic trade. When trade takes place between two or more states, factors like currency, government policies, economy, judicial system, laws, and markets influence trade. To ease and justify the process of trade between countries of different economic standing in the modern era, some international economic organizations were formed, such as the World Trade Organization. These organizations work towards the facilitation and growth of international trade. Statistical services of intergovernmental and supranational organizations and governmental statistical agencies publish official statistics on international trade. Characteristics of global trade\nA product that is transferred or sold from a party in one country to a party in another country is an export from the originating country, and an import to the country receiving that product. Imports and exports are accounted for in a country's current account in the balance of payments. Trading globally may give consumers and countries the opportunity to be exposed to new markets and  products. Almost every kind of product can be found in the international market, for example: food, clothes, spare parts, oil, jewellery, wine, stocks, currencies, and water. Services are also traded, such as in tourism, banking, consulting, and transportation. Advanced technology (including transportation), globalization, industrialization, outsourcing and multinational corporations have major impacts on the international trade systems. Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not.",
        "rrf_score": 0.03225806451612903,
        "dense_rank": 2,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "Monetary_policy_20",
        "text": "This view rests on two implicit assumptions: a high responsiveness of import prices to the exchange rate, i.e. producer currency pricing (PCP), and frictionless international financial markets support...",
        "full_text": "This view rests on two implicit assumptions: a high responsiveness of import prices to the exchange rate, i.e. producer currency pricing (PCP), and frictionless international financial markets supporting the efficiency of flexible price allocation. The violation or distortion of these assumptions found in empirical research is the subject of a substantial part of the international optimal monetary policy literature. The policy trade-offs specific to this international perspective are threefold:\nFirst, research suggests only a weak reflection of exchange rate movements in import prices, lending credibility to the opposed theory of local currency pricing (LCP). The consequence is a departure from the classical view in the form of a trade-off between output gaps and misalignments in international relative prices, shifting monetary policy to CPI inflation control and real exchange rate stabilization. Second, another specificity of international optimal monetary policy is the issue of strategic interactions and competitive devaluations, which is due to cross-border spillovers in quantities and prices. Therein, the national authorities of different countries face incentives to manipulate the terms of trade to increase national welfare in the absence of international policy coordination. Even though the gains of international policy coordination might be small, such gains may become very relevant if balanced against incentives for international noncooperation. Third, open economies face policy trade-offs if asset market distortions prevent global efficient allocation. Even though the real exchange rate absorbs shocks in current and expected fundamentals, its adjustment does not necessarily result in a desirable allocation and may even exacerbate the misallocation of consumption and employment at both the domestic and global level. This is because, relative to the case of complete markets, both the Phillips curve and the loss function include a welfare-relevant measure of cross-country imbalances. Consequently, this results in domestic goals, e.g. output gaps or inflation, being traded-off against the stabilization of external variables such as the terms of trade or the demand gap. Hence, the optimal monetary policy in this case consists of redressing demand imbalances and/or correcting international relative prices at the cost of some inflation.",
        "rrf_score": 0.031746031746031744,
        "dense_rank": 3,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Monetary_policy",
        "title": "Monetary policy"
      },
      {
        "chunk_id": "Globalization_12",
        "text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never reco...",
        "full_text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never recovered since the Great Recession. New opposing views from some economists have argued such trends are a result of price drops and in actuality, trade volume is increasing, especially with agricultural products, natural resources and refined petroleum. The 21st century melting of the Arctic will also affect global trade, as it is paving the way for shorter trade routes. Economic globalization\nEconomic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital. Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market. Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes competition, technology, and corporations and industries. Current globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration. International standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations.",
        "rrf_score": 0.029631255487269532,
        "dense_rank": 8,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Globalization",
        "title": "Globalization"
      },
      {
        "chunk_id": "Globalization_13",
        "text": "Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the ...",
        "full_text": "Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations. A multinational corporation, or worldwide enterprise, is an organization that owns or controls the production of goods or services in one or more countries other than their home country. It can also be referred to as an international corporation, a transnational corporation, or a stateless corporation. A free-trade area is the region encompassing a trade bloc whose member countries have signed a free-trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers –  import quotas and tariffs –  and to increase trade of goods and services with each other. If people are also free to move between the countries, in addition to a free-trade agreement, it would also be considered an open border. Arguably, the most significant free-trade area in the world is the European Union, a politico-economic union of 27 member states that are primarily located in Europe. The EU has developed European Single Market through a standardized system of laws that apply in all member states. EU policies aim to ensure the free movement of people, goods, services, and capital within the internal market,\n\nTrade facilitation looks at how procedures and controls governing the movement of goods across national borders can be improved to reduce associated cost burdens and maximize efficiency while safeguarding legitimate regulatory objectives. Global trade in services is also significant. For example, in India, business process outsourcing has been described as the \"primary engine of the country's development over the next few decades, contributing broadly to GDP growth, employment growth, and poverty alleviation\". William I. Robinson's theoretical approach to globalization is a critique of Wallerstein's World Systems Theory. He believes that the global capital experienced today is due to a new and distinct form of globalization which began in the 1980s. Robinson argues not only are economic activities expanded across national boundaries but also there is a transnational fragmentation of these activities.",
        "rrf_score": 0.029631255487269532,
        "dense_rank": 7,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Globalization",
        "title": "Globalization"
      }
    ]
  },
  {
    "question": "Which uses rewards: reinforcement or supervised learning?",
    "answer": "Reinforcement learning.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "comparative",
    "generated_answer": "Reinforcement",
    "response_time": "7.51s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_8",
        "text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the...",
        "full_text": "Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Other types\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example, topic modelling, meta-learning. Self-learning\nSelf-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 1,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_6",
        "text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm...",
        "full_text": "As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise. Although each algorithm has advantages and limitations, no single algorithm works for all problems. Supervised learning\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 2,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Deep_learning_23",
        "text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFa...",
        "full_text": "Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. Commercial activity\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories. As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\". Criticism and comment\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.",
        "rrf_score": 0.03007688828584351,
        "dense_rank": 7,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Machine_learning_9",
        "text": "Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence ...",
        "full_text": "Emotion is used as a state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion. The self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n\nin situation s act a\nreceive a consequence situation s'\ncompute emotion of being in the consequence situation v(s')\nupdate crossbar memory  w'(a,s) = w(a,s) + v(s')\nIt is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour in an environment that contains both desirable and undesirable situations. Feature learning\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data.",
        "rrf_score": 0.03007688828584351,
        "dense_rank": 6,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_1",
        "text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early...",
        "full_text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think? \", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\". Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms. Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "Which is for interaction design: HCI or graphics?",
    "answer": "HCI.",
    "source_id": "Human–computer interaction",
    "url": "https://en.wikipedia.org/wiki/Human–computer_interaction",
    "category": "comparative",
    "generated_answer": "HCI",
    "response_time": "7.13s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_graphics_20",
        "text": "Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel ...",
        "full_text": "Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel typically has three subpixels such as red, green, and blue. Graphics are visual representations on a surface, such as a computer screen. Examples are photographs, drawing, graphics designs, maps, engineering drawings, or other images. Graphics often combine text and illustration. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style. Primitives\nPrimitives are basic units which a graphics system may combine to create more complex images or models. Examples would be sprites and character maps in 2D video games, geometric primitives in CAD, or polygons or triangles in 3D rendering. Primitives may be supported in hardware for efficient rendering, or the building blocks provided by a graphics application. Rendering\nRendering is the generation of a 2D image from a 3D model by means of computer programs. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texturing, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The rendering program is usually built into the computer graphics software, though others are available as plug-ins or entirely separate programs. The term \"rendering\" may be by analogy with an \"artist's rendering\" of a scene. Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU.",
        "rrf_score": 0.03252247488101534,
        "dense_rank": 2,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_1",
        "text": "Many tools have been developed to visualize data. Computer-generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As te...",
        "full_text": "Many tools have been developed to visualize data. Computer-generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As technology has improved, 3D computer graphics have become more common, but 2D computer graphics are still widely used. Computer graphics has emerged as a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Over the past decade, other specialized fields have been developed like information visualization, and scientific visualization more concerned with \"the visualization of three dimensional phenomena (architectural, meteorological, medical, biological, etc. ), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component\". History\nThe precursor sciences to the development of modern computer graphics were the advances in electrical engineering, electronics, and television that took place during the first half of the twentieth century. Screens could display art since the Lumiere brothers' use of mattes to create special effects for the earliest films dating from 1895, but such displays were limited and not interactive. The first cathode ray tube, the Braun tube, was invented in 1897 – it in turn would permit the oscilloscope and the military control panel – the more direct precursors of the field, as they provided the first two-dimensional electronic displays that responded to programmatic or user input. Nevertheless, computer graphics remained relatively unknown as a discipline until the 1950s and the post-World War II period – during which time the discipline emerged from a combination of both pure university and laboratory academic research into more advanced computers and the United States military's further development of technologies like radar, aviation, and rocketry developed during the war. New kinds of displays were needed to process the wealth of information resulting from such projects, leading to the development of computer graphics as a discipline. 1950s\nEarly projects like the Whirlwind and SAGE Projects introduced the CRT as a viable display and interaction interface and introduced the light pen as an input device.",
        "rrf_score": 0.02967032967032967,
        "dense_rank": 10,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_0",
        "text": "Computer graphics deals with generating images and art with the aid of computers. Computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer...",
        "full_text": "Computer graphics deals with generating images and art with the aid of computers. Computer graphics is a core technology in digital photography, film, video games, digital art, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research. Computer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. It is also used for processing image data received from the physical world, such as photo and video content. Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, and video games in general. Overview\nThe term computer graphics has been used in a broad sense to describe \"almost everything on computers that is not text or sound\". Typically, the term computer graphics refers to several different things:\n\nthe representation and manipulation of image data by a computer\nthe various technologies used to create and manipulate images\nmethods for digitally synthesizing and manipulating visual content, see study of computer graphics\nToday, computer graphics is widespread. Such imagery is found in and on television, newspapers, weather reports, and in a variety of medical investigations and surgical procedures. A well-constructed graph can present complex statistics in a form that is easier to understand and interpret. In the media \"such graphs are used to illustrate papers, reports, theses\", and other presentation material. Many tools have been developed to visualize data. Computer-generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_25",
        "text": "He developed the Bresenham's line algorithm, his most well-known invention in 1962, and the midpoint circle algorithm. He retired from 27 years of service at IBM as a Senior Technical Staff Member, ta...",
        "full_text": "He developed the Bresenham's line algorithm, his most well-known invention in 1962, and the midpoint circle algorithm. He retired from 27 years of service at IBM as a Senior Technical Staff Member, taught for 16 years at Winthrop University and has nine patents. Other pioneers\nPierre Bézier\nJim Blinn\nJohn Carmack\nPaul de Casteljau\nEd Catmull\nFrank Crow\nJames D. Foley\nWilliam Fetter\nHenry Fuchs\nHenri Gouraud\nCharles Loop\nNadia Magnenat Thalmann\nBenoit Mandelbrot\nMartin Newell\nFred Parke\nBui Tuong Phong\nSteve Russell\nDaniel J. Sandin\nAlvy Ray Smith\nBob Sproull\nIvan Sutherland\nDaniel Thalmann\nJohnson K. Yan\nAndries van Dam\nJohn Warnock\nJ. Turner Whitted\nLance Williams\nJim Kajiya\nJames H. Clark\n\nOrganizations\nSIGGRAPH\nGDC\nBell Telephone Laboratories\nLink Flight Simulation, division of Singer Company\nUnited States Armed Forces, particularly the Whirlwind computer and SAGE Project\nBoeing\nEurographics\nEvans and Sutherland\nIBM\nRenault\nNYIT\nThe computer science department of the University of Utah\nLucasfilm and Industrial Light & Magic\nAutodesk\nAdobe Systems\nPixar\nSilicon Graphics, Khronos Group & OpenGL\nThe DirectX division at Microsoft\nNvidia\nAMD (ATI Technologies)\n\nStudy of computer graphics\nThe study of computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing. As an academic discipline, computer graphics studies the manipulation of visual and geometric information using computational techniques. It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues. Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities. Applications\nComputer graphics may be used in the following areas:\n\nAugmented reality\nComputational biology\nComputational photography\nComputational physics\nComputer-aided design\nComputing\nDesign\nDigital art\nEducation\nExtended reality\nGraphic design\nInfographics\nInformation visualization\nMixed reality\nMotion graphics\nRational drug design\nSimulation\nScientific visualization\nSpecial effects for cinema\nVideo games\nVirtual reality\nWeb3D\n\nSee also\nComputer representation of surfaces\nGlossary of computer graphics\n\nNotes\nReferences\nFurther reading\nL. Ammeraal and K. Zhang (2007). Computer Graphics for Java Programmers, Second Edition, John-Wiley & Sons, ISBN 978-0-470-03160-5. David Rogers (1998). Procedural Elements for Computer Graphics. McGraw-Hill.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      },
      {
        "chunk_id": "Computer_graphics_11",
        "text": "In 1988, the first shaders – small programs designed specifically to do shading as a separate algorithm – were developed by Pixar, which had already spun off from Industrial Light & Magic as a separat...",
        "full_text": "In 1988, the first shaders – small programs designed specifically to do shading as a separate algorithm – were developed by Pixar, which had already spun off from Industrial Light & Magic as a separate entity – though the public would not see the results of such technological progress until the next decade. In the late 1980s, Silicon Graphics (SGI) computers were used to create some of the first fully computer-generated short films at Pixar, and Silicon Graphics machines were considered a high-water mark for the field during the decade. The 1980s is also called the golden era of videogames; millions-selling systems from Atari, Nintendo and Sega, among other companies, exposed computer graphics for the first time to a new, young, and impressionable audience – as did MS-DOS-based personal computers, Apple IIs, Macs, and Amigas, all of which also allowed users to program their own games if skilled enough. For the arcades, advances were made in commercial, real-time 3D graphics. In 1988, the first dedicated real-time 3D graphics boards were introduced for arcades, with the Namco System 21 and Taito Air System. On the professional side, Evans & Sutherland and SGI developed 3D raster graphics hardware that directly influenced the later single-chip graphics processing unit (GPU), a technology where a separate and very powerful chip is used in parallel processing with a CPU to optimize graphics. The decade also saw computer graphics applied to many additional professional markets, including location-based entertainment and education with the E&S Digistar, vehicle design, vehicle simulation, and chemistry. 1990s\nThe 1990s' highlight was the emergence of 3D modeling on a mass scale and an rise in the quality of CGI generally. Home computers became able to take on rendering tasks that previously had been limited to workstations costing thousands of dollars; as 3D modelers became available for home systems, the popularity of Silicon Graphics workstations declined and powerful Microsoft Windows and Apple Macintosh machines running Autodesk products like 3D Studio or other home rendering software ascended in importance. By the end of the decade, the GPU would begin its rise to the prominence it still enjoys today.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Computer_graphics",
        "title": "Computer graphics"
      }
    ]
  },
  {
    "question": "Which is variable: wind or nuclear power?",
    "answer": "Wind power.",
    "source_id": "Wind power",
    "url": "https://en.wikipedia.org/wiki/Wind_power",
    "category": "comparative",
    "generated_answer": "wind",
    "response_time": "7.52s",
    "fused_chunks": [
      {
        "chunk_id": "Wind_power_6",
        "text": "Homes and businesses can also be programmed to vary electricity demand, for example by remotely turning up water heater thermostats. Variability\nWind power is variable, and during low wind periods, it...",
        "full_text": "Homes and businesses can also be programmed to vary electricity demand, for example by remotely turning up water heater thermostats. Variability\nWind power is variable, and during low wind periods, it may need to be replaced by other power sources. Transmission networks presently cope with outages of other generation plants and daily changes in electrical demand, but the variability of intermittent power sources such as wind power is more frequent than those of conventional power generation plants which, when scheduled to be operating, may be able to deliver their nameplate capacity around 95% of the time. Electric power generated from wind power can be highly variable at several different timescales: hourly, daily, or seasonally. Annual variation also exists but is not as significant. Because instantaneous electrical generation and consumption must remain in balance to maintain grid stability, this variability can present substantial challenges to incorporating large amounts of wind power into a grid system. Intermittency and the non-dispatchable nature of wind energy production can raise costs for regulation, incremental operating reserve, and (at high penetration levels) could require an increase in the already existing energy demand management, load shedding, storage solutions, or system interconnection with HVDC cables. Fluctuations in load and allowance for the failure of large fossil-fuel generating units require operating reserve capacity, which can be increased to compensate for the variability of wind generation. Utility-scale batteries are often used to balance hourly and shorter timescale variation, but car batteries may gain ground from the mid-2020s. Wind power advocates argue that periods of low wind can be dealt with by simply restarting existing power stations that have been held in readiness, or interlinking with HVDC. The combination of diversifying variable renewables by type and location, forecasting their variation, and integrating them with dispatchable renewables, flexible fueled generators, and demand response can create a power system that has the potential to meet power supply needs reliably. Integrating ever-higher levels of renewables is being successfully demonstrated in the real world. Solar power tends to be complementary to wind.",
        "rrf_score": 0.031099324975891997,
        "dense_rank": 8,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_0",
        "text": "Wind power is the use of wind energy to generate useful work. Historically, wind power was used by sails, windmills and windpumps, but today it is mostly used to generate electricity. This article dea...",
        "full_text": "Wind power is the use of wind energy to generate useful work. Historically, wind power was used by sails, windmills and windpumps, but today it is mostly used to generate electricity. This article deals only with wind power for electricity generation. Today, wind power is generated almost completely using wind turbines, generally grouped into wind farms and connected to the electrical grid. In 2024, wind supplied about 2,500 TWh of electricity, which was over 8% of world electricity. With about 100 GW added during 2021, mostly in China and the United States, global installed wind power capacity exceeded 800 GW. 30 countries generated more than a tenth of their electricity from wind power in 2024 and wind generation has nearly tripled since 2015. To help meet the Paris Agreement goals to limit climate change, analysts say it should expand much faster – by over 1% of electricity generation per year. Wind power is a sustainable, renewable energy source, and has a much smaller impact on the environment than burning fossil fuels. Wind power is variable, so it needs energy storage or other dispatchable generation energy sources to attain a reliable supply of electricity. Land-based (onshore) wind farms have a greater visual impact on the landscape than most other power stations per energy produced. Wind farms sited offshore have less visual impact and have higher capacity factors, although they are generally more expensive. Offshore wind power currently has a share of about 10% of new installations. Wind power is one of the lowest-cost electricity sources per unit of energy produced. In many locations, new onshore wind farms are cheaper than new coal or gas plants. Regions in the higher northern and southern latitudes have the highest potential for wind power. In most regions, wind power generation is higher in nighttime, and in winter when solar power output is low. So combinations of wind and solar power are suitable in many countries. Wind energy resources\nWind is air movement in the Earth's atmosphere.",
        "rrf_score": 0.030776515151515152,
        "dense_rank": 4,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_3",
        "text": "At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system. Generator characteristics and st...",
        "full_text": "At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system. Generator characteristics and stability\nMost modern turbines use variable speed generators combined with either a partial or full-scale power converter between the turbine generator and the collector system, which generally have more desirable properties for grid interconnection and have low-voltage ride-through capabilities. Modern turbines use either doubly fed electric machines with partial-scale converters or squirrel-cage induction generators or synchronous generators (both permanently and electrically excited) with full-scale converters. Black start is possible and is being further developed for places (such as Iowa) which generate most of their electricity from wind. Transmission system operators will supply a wind farm developer with a grid code to specify the requirements for interconnection to the transmission grid. This will include the power factor, the constancy of frequency, and the dynamic behaviour of the wind farm turbines during a system fault. Offshore wind power\nOffshore wind power is wind farms in large bodies of water, usually the sea. These installations can use the more frequent and powerful winds that are available in these locations and have less visual impact on the landscape than land-based projects. However, the construction and maintenance costs are considerably higher. As of November 2021, the Hornsea Wind Farm in the United Kingdom is the largest offshore wind farm in the world at 1,218 MW. Collection and transmission network\nNear offshore wind farms may be connected by AC and far offshore by HVDC. Wind power resources are not always located near areas with a high population density. As transmission lines become longer, the losses associated with power transmission increase, as modes of losses at lower lengths are exacerbated and new modes of losses are no longer negligible as the length is increased; making it harder to transport large loads over large distances. When the transmission capacity does not meet the generation capacity, wind farms are forced to produce below their full potential or stop running altogether, in a process known as curtailment. While this leads to potential renewable generation left untapped, it prevents possible grid overload or risk to reliable service.",
        "rrf_score": 0.030536130536130537,
        "dense_rank": 6,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_9",
        "text": "Northern Eurasia, Canada, some parts of the United States, and Patagonia in Argentina are the best areas for onshore wind: whereas in other parts of the world solar power, or a combination of wind and...",
        "full_text": "Northern Eurasia, Canada, some parts of the United States, and Patagonia in Argentina are the best areas for onshore wind: whereas in other parts of the world solar power, or a combination of wind and solar, tend to be cheaper. Electric power cost and trends\nWind power is capital intensive but has no fuel costs. The price of wind power is therefore much more stable than the volatile prices of fossil fuel sources. However, the estimated average cost per unit of electric power must incorporate the cost of construction of the turbine and transmission facilities, borrowed funds, return to investors (including the cost of risk), estimated annual production, and other components, averaged over the projected useful life of the equipment, which may be more than 20 years. Energy cost estimates are highly dependent on these assumptions so published cost figures can differ substantially. The presence of wind energy, even when subsidized, can reduce costs for consumers (€5 billion/yr in Germany) by reducing the marginal price and by minimizing the use of expensive peaking power plants. The cost has decreased as wind turbine technology has improved. There are now longer and lighter wind turbine blades, improvements in turbine performance, and increased power generation efficiency. Also, wind project capital expenditure costs and maintenance costs have continued to decline. In 2021, a Lazard study of unsubsidized electricity said that wind power levelized cost of electricity continues to fall but more slowly than before. The study estimated new wind-generated electricity cost from $26 to $50/MWh, compared to new gas power from $45 to $74/MWh. The median cost of fully deprecated existing coal power was $42/MWh, nuclear $29/MWh and gas $24/MWh. The study estimated offshore wind at around $83/MWh. Compound annual growth rate was 4% per year from 2016 to 2021, compared to 10% per year from 2009 to 2021.",
        "rrf_score": 0.029877369007803793,
        "dense_rank": 5,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_7",
        "text": "Integrating ever-higher levels of renewables is being successfully demonstrated in the real world. Solar power tends to be complementary to wind. On daily to weekly timescales, high-pressure areas ten...",
        "full_text": "Integrating ever-higher levels of renewables is being successfully demonstrated in the real world. Solar power tends to be complementary to wind. On daily to weekly timescales, high-pressure areas tend to bring clear skies and low surface winds, whereas low-pressure areas tend to be windier and cloudier. On seasonal timescales, solar energy peaks in summer, whereas in many areas wind energy is lower in summer and higher in winter. Thus the seasonal variation of wind and solar power tend to cancel each other somewhat. Wind hybrid power systems are becoming more popular. Predictability\nFor any particular generator, there is an 80% chance that wind output will change less than 10% in an hour and a 40% chance that it will change 10% or more in 5 hours. In summer 2021, wind power in the United Kingdom fell due to the lowest winds in seventy years, In the future, smoothing peaks by producing green hydrogen may help when wind has a larger share of generation. While the output from a single turbine can vary greatly and rapidly as local wind speeds vary, as more turbines are connected over larger and larger areas the average power output becomes less variable and more predictable. Weather forecasting permits the electric-power network to be readied for the predictable variations in production that occur. It is thought that the most reliable low-carbon electricity systems will include a large share of wind power. Energy storage\nTypically, conventional hydroelectricity complements wind power very well. When the wind is blowing strongly, nearby hydroelectric stations can temporarily hold back their water. When the wind drops they can, provided they have the generation capacity, rapidly increase production to compensate. This gives a very even overall power supply and virtually no loss of energy and uses no more water. Alternatively, where a suitable head of water is not available, pumped-storage hydroelectricity or other forms of grid energy storage such as compressed air energy storage and thermal energy storage can store energy developed by high-wind periods and release it when needed.",
        "rrf_score": 0.029211087420042643,
        "dense_rank": 7,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      }
    ]
  },
  {
    "question": "Which is biological inspiration: CNNs or SVMs?",
    "answer": "CNNs.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "comparative",
    "generated_answer": "SVMs are computing systems inspired by the biological neural networks that constitute animal brains",
    "response_time": "10.29s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_16",
        "text": "This random selection of RFR for training enables the model to reduce biased predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single-ou...",
        "full_text": "This random selection of RFR for training enables the model to reduce biased predictions and achieve a higher degree of accuracy. RFR generates independent decision trees, and it can work on single-output data as well as multiple regressor tasks. This makes RFR compatible to be use in various applications. Support-vector machines\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Regression analysis\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space. Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 4,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Deep_learning_11",
        "text": "but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural ne...",
        "full_text": "but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\". Neural networks\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Computer_vision_3",
        "text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, s...",
        "full_text": "These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology. The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex. Some strands of computer vision research are closely related to the study of biological vision—indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields. Signal processing\nYet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision. Robotic navigation\nRobot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n\nVisual computing\nOther fields\nBesides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Creativity_2",
        "text": "Conceptual history\nAncient\nMost ancient cultures, including Ancient Greece, Ancient China, and Ancient India, lacked the concept of creativity, seeing art as a form of discovery and not creation. The ...",
        "full_text": "Conceptual history\nAncient\nMost ancient cultures, including Ancient Greece, Ancient China, and Ancient India, lacked the concept of creativity, seeing art as a form of discovery and not creation. The ancient Greeks had no terms for \"to create\" or \"creator\" except for the expression poiein (to make), which only applied to poiesis (poetry) and to the poietes (poet, or \"maker\", who made it). Plato did not believe in art as a form of creation. He asks in the Republic, \"Will we say of a painter that he makes something?\" He answers, \"Certainly not, he merely imitates.\" It is commonly argued that the notion of \"creativity\" originated in Western cultures through Christianity, as a matter of divine inspiration. According to scholars, the earliest Western conception of creativity was the Biblical story of the creation given in Genesis. However, this is not creativity in the modern sense, which did not arise until the Renaissance. In the Judeo-Christian-Islamic tradition, creativity was the sole province of God; humans were not considered to have the ability to create something new except as an expression of God's work. A similar concept existed in Greek culture, where the Muses were seen as mediating inspiration from the gods. Romans and Greeks invoked the concept of an external creative \"daemon\" (Greek) or \"genius\" (Latin), linked to the sacred or the divine. However, none of these views are similar to the modern concept of creativity, and the rejection of creativity in favor of discovery and the belief that individual creation was a conduit of the divine would dominate the West until the Renaissance and even later. Renaissance\nIt was during the Renaissance that creativity was first conceived not as a conduit from the divine, but as arising from the abilities of \"great men.\" This could be attributed to the leading intellectual movement of the time, aptly named humanism, which developed an intensely anthropocentric outlook on the world, valuing the intellect and achievement of the individual.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Creativity",
        "title": "Creativity"
      },
      {
        "chunk_id": "Deep_learning_17",
        "text": "Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database ...",
        "full_text": "Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. Visual art processing\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\n\nidentifying the style period of a given painting\nNeural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video\ngenerating striking imagery based on random visual input fields. Natural language processing\nNeural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling. Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others. Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\". It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      }
    ]
  },
  {
    "question": "Which needs labels: classification or clustering?",
    "answer": "Classification.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "comparative",
    "generated_answer": "classification",
    "response_time": "6.95s",
    "fused_chunks": [
      {
        "chunk_id": "Kubernetes_9",
        "text": "Labels and selectors\nKubernetes enables clients (users or internal components) to attach keys called labels to any API object in the system, such as pods and nodes. Correspondingly, label selectors ar...",
        "full_text": "Labels and selectors\nKubernetes enables clients (users or internal components) to attach keys called labels to any API object in the system, such as pods and nodes. Correspondingly, label selectors are queries against labels that resolve to matching objects. When a service is defined, one can define the label selectors that will be used by the service router/load balancer to select the pod instances that the traffic will be routed to. Thus, simply changing the labels of the pods or changing the label selectors on the service can be used to control which pods get traffic and which don't, which can be used to support various deployment patterns like blue–green deployments or A/B testing. This capability to dynamically control how services utilize implementing resources provides a loose coupling within the infrastructure. For example, if an application's pods have labels for a system tier (with values such as frontend, backend, for example) and a release_track (with values such as canary, production, for example), then an operation on all of backend and canary nodes can use a label selector, such as:\n\ntier=backend AND release_track=canary\nJust like labels, field selectors also let one select Kubernetes resources. Unlike labels, the selection is based on the attribute values inherent to the resource being selected, rather than user-defined categorization. metadata.name and metadata.namespace are field selectors that will be present on all Kubernetes objects. Other selectors that can be used depend on the object/resource type. Add-ons\nAdd-ons are additional features of the Kubernetes cluster implemented as applications running within it. The pods may be managed by Deployments, ReplicationControllers, and so on. There are many add-ons. Some of the more important are:\n\nDNS\nCluster DNS is a DNS server, in addition to the other DNS server(s) in the environment, which serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server in their DNS searches. Web UI\nThis is a general purpose, web-based UI for Kubernetes clusters. It allows administrators to manage and troubleshoot applications running in the cluster, as well as the cluster itself.",
        "rrf_score": 0.03225806451612903,
        "dense_rank": 2,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Kubernetes",
        "title": "Kubernetes"
      },
      {
        "chunk_id": "Machine_learning_1",
        "text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early...",
        "full_text": "Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nils Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question, \"Can machines think? \", is replaced with the question, \"Can machines do what we (as thinking entities) can do?\". Modern-day Machine Learning algorithms are broken into 3 algorithm types: Supervised Learning Algorithms, Unsupervised Learning Algorithms, and Reinforcement Learning Algorithms. Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_7",
        "text": "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in...",
        "full_text": "It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself. Dimensionality reduction\nDimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation. Semi-supervised learning\nSemi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data).",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_11",
        "text": "In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is asso...",
        "full_text": "In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image denoising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot. Anomaly detection\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations that raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions. In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns. Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance being generated by the model.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Machine_learning_10",
        "text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervi...",
        "full_text": "This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      }
    ]
  },
  {
    "question": "Why did deep learning advance after 2012?",
    "answer": "GPUs and large datasets enabled it.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "hardware improvements (faster computers, graphics processing units, cloud computing and access to large amounts of data",
    "response_time": "9.47s",
    "fused_chunks": [
      {
        "chunk_id": "Deep_learning_9",
        "text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision ...",
        "full_text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. Deep learning revolution\nThe deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning. A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Deep_learning_24",
        "text": "(e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically. In f...",
        "full_text": "(e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website. With the support of Innovation Diffusion Theory (IDT), a study analyzed the diffusion of Deep Learning in BRICS and OECD countries using data from Google Trends. Errors\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). Cyber threat\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target.",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 2,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Artificial_general_intelligence_9",
        "text": "Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances...",
        "full_text": "Timescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs. In the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert. In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave. In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27. In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training.",
        "rrf_score": 0.029910714285714284,
        "dense_rank": 4,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_34",
        "text": "Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began t...",
        "full_text": "Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks. AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect). However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s. Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. For many specific tasks, other methods were abandoned. Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.",
        "rrf_score": 0.029418126757516764,
        "dense_rank": 7,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Deep_learning_4",
        "text": "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep net...",
        "full_text": "They regarded it as a form of polynomial regression, or a generalization of Rosenblatt's perceptron to handle more complex, nonlinear, and hierarchical relationships. A 1971 paper described a deep network with eight layers trained by this method, which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\". The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for deep learning. Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation. Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970). G.M. Ostrovski et al. republished it in 1971. Paul Werbos applied backpropagation to neural networks in 1982 (his 1974 PhD thesis, reprinted in a 1994 book, did not yet describe the algorithm). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work. 1980s-2000s\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.",
        "rrf_score": 0.02919863597612958,
        "dense_rank": 9,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      }
    ]
  },
  {
    "question": "Why can recommender systems spread misinformation?",
    "answer": "They optimize engagement.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content",
    "response_time": "9.46s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_20",
        "text": "Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuc...",
        "full_text": "Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban. Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI. On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. According to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors. In 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it. Misinformation\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Consumer_behavior_40",
        "text": "The three categories are: \"trackers\", \"hunters\", and \"explorers\". \"Trackers\" are the online consumers who are exactly looking for a product that they definitely wish to buy. They use the Internet for ...",
        "full_text": "The three categories are: \"trackers\", \"hunters\", and \"explorers\". \"Trackers\" are the online consumers who are exactly looking for a product that they definitely wish to buy. They use the Internet for the information about its price, delivery methods, post-purchase service, and so on. Once they have found the proper information, little effort is needed to let them do the business. \"Hunters\" just know the categories of the product that they need, for instance, a novel for leisure time. However, they have not made specific decision on whose novel to buy. They use the Internet to find a list of product of their needed categories to make comparison. This kind of online consumer needs advice and help to do their business. \"Explorers\" do not even have the categories of product on their minds. In fact, they just want to buy something online. There is more uncertainty of this type of online consumers. Influence of the Internet on buying process\nAs the preceding table shows, the first row indicates the process of a consumer buying a new product, while the second and third row illustrates the positive influences the Internet could have on buying process by creating effective communications with online consumers. For example, suppose a consumer carelessly sees an advertisement about laptops on Wechat, a popular Chinese social media developed by Tencent. He begins to feel that his laptop is a bit out of date and wants to buy a new one, which is the outcome of good advertisement placed on a daily Internet tool. He does not know anything about how to buy a new one as business changes so fast today, so he searches on Google to find an answer. On the results page he finds promotional ads which mainly come from JD.com and Taobao, two competing Chinese online retailers in this field. He prefers to use JD.com, which provides detailed comparisons of brands, prices, locations, and methods of payment and delivery. After careful selection, he makes his order through JD.com via Wechat payment.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Consumer_behavior",
        "title": "Consumer behavior"
      },
      {
        "chunk_id": "Consumer_behavior_39",
        "text": "Or they may become a brand advocate by posting a product review online, or providing brand referrals via social media. Some e-commerce providers have encountered a challenge in courting consumers who ...",
        "full_text": "Or they may become a brand advocate by posting a product review online, or providing brand referrals via social media. Some e-commerce providers have encountered a challenge in courting consumers who seek information online, but still prefer to turn to bricks and mortar retailers for their purchase. To understand the needs and habits of these and other kinds of online shoppers, online marketers have segmented consumers into different kinds of online behaviour in accordance with their online behavioural characteristics. Lewis and Lewis (1997) identified five market segments based on the way that consumers use the Internet in the purchase decision process:\n\n\"Directed Information-seekers\" are users that primarily look for information about a product or service online, but there is no guarantee that they may be converted into online buyers. \"Undirected Information-seekers\" are newcomers to a product or service. They are more likely to interact with online prompts, and click through to web pages linked in advertising. \"Directed Buyers\" have a predetermined mindset and wish to purchase a specific product or service online. \"Bargain Hunters\" are price-sensitive users that like to discover products during sales promotions. For these users, discounts are a major attraction to online sales conversion. \"Entertainment Seekers\" are online consumers that are attracted to marketing delivered as a fun activity. Interactive online games could be useful in attracting this kind of customer. A typology of online consumer behaviour\nWendy Moe (2003) argues that in the offline environment, consumers who are shopping in stores can be easily classified by experienced sales employees only by watching their shopping behaviours. Such classification may not appear online, but Moe and Fader argued that it is feasible to predict practical buying, surfing, and searching action online by investigating click patterns and repetition of visit within online behaviour. In addition, a report of E-consultancy about \"benchmarking of user experience\" outlined three kinds of online consuming behaviour as a valuable classification for the research of design of web pages to better serve different kinds of consuming behaviour. The three categories are: \"trackers\", \"hunters\", and \"explorers\".",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Consumer_behavior",
        "title": "Consumer behavior"
      },
      {
        "chunk_id": "Democracy_37",
        "text": "The internet and social media have provided powerful new tools for democracy movements in developing countries and emerging democracies, enabling them to bypass censorship, voice their opinions, and o...",
        "full_text": "The internet and social media have provided powerful new tools for democracy movements in developing countries and emerging democracies, enabling them to bypass censorship, voice their opinions, and organize protests. A serious problem with the social media is that they have no truth filters. The established news media have to guard their reputation as trustworthy, while ordinary citizens may post unreliable information. In fact, studies show that false stories are going more viral than true stories. The proliferation of false stories and conspiracy theories may undermine public trust in the political system and public officials. Reliable information sources are essential for the democratic process. Less democratic governments rely heavily on censorship, propaganda, and misinformation in order to stay in power, while independent sources of information are able to undermine their legitimacy. Democracy promotion\nDemocracy promotion can increase the quality of already existing democracies, reduce political apathy, and the chance of democratic backsliding. Democracy promotion measures include voting advice applications, participatory democracy, increasing youth suffrage, increasing civic education, reducing barriers to entry for new political parties, increasing proportionality and reducing presidentialism. See also\nFootnotes\nReferences\nWorks cited\nFurther reading\nCartledge, Paul (2016). Democracy: A Life. Oxford University Press. ISBN 978-0199697670. Provost, Claire; Kennard, Matt (2023). Silent Coup: How Corporations Overthrew Democracy. Bloomsbury Academic. ISBN 978-1-350-26998-9. Biagini, Eugenio (general editor). 2021. A Cultural History of Democracy, 6 Volumes New York : Bloomsbury Academic. Taylor, Astra (2019). Democracy May Not Exist, but We'll Miss It When It's Gone. Metropolitan Books. ISBN 978-1-250-17984-5. Przeworski, Adam (2018) Why Bother With Elections? Cambridge, UK: Polity Press. Munck, Gerardo L. (2016) \"What is Democracy? A Reconceptualization of the Quality of Democracy\". Democratization 23(1): 1–26. Fuller, Roslyn (2015). Beasts and Gods: How Democracy Changed Its Meaning and Lost its Purpose. London: Zed Books. p. 371. ISBN 978-1-78360-542-2. Votingsystem, Nu (2024).",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Democracy",
        "title": "Democracy"
      },
      {
        "chunk_id": "Social_media_24",
        "text": "Under these rules, the only way to intellectually prevail is by infinite filibustering.\" Trustworthiness and reliability\nSocial media has become a regular source of news and information. A 2021 Pew Re...",
        "full_text": "Under these rules, the only way to intellectually prevail is by infinite filibustering.\" Trustworthiness and reliability\nSocial media has become a regular source of news and information. A 2021 Pew Research Center poll reported roughly 70% of users regularly get news from social media, despite the presence of fake news and misinformation. Platforms typically do not take responsibility for content accuracy, and many do not vet content at all, although in some cases, content the platform finds problematic is deleted or access to it is reduced. Content distribution algorithms otherwise typically ignore substance, responding instead to the contents' virality. In 2018, researchers reported that fake news spread almost 70% faster than truthful news on X. Social media bots on social media increase the reach of both true and false content and if wielded by bad actors misinformation can reach many more users. Some platforms attempt to discover and block bots, with limited success. Fake news seems to receive more user engagement, possibly because it is relatively novel, engaging users' curiosity and increasing spread. Fake news often propagates in the immediate aftermath of an event, before conventional media are prepared to publish. Data harvesting and data mining\nCritique of activism\nCanadian journalist Malcolm Gladwell considers the role of social media in revolutions and protests to be overstated. He concluded that while social media makes it easier for activists to express themselves, that expression likely has no impact beyond social media. What he called \"high-risk activism\" involves strong relationships, coordination, commitment, high risks, and sacrifice. Gladwell claimed that social media are built around weak ties and argues that \"social networks are effective at increasing participation—by lessening the level of motivation that participation requires.\" According to him, \"Facebook activism succeeds not by motivating people to make a real sacrifice, but by motivating them to do the things that people do when they are not motivated enough to make a real sacrifice.\" Disputing Gladwell's theory, a 2018 survey reported that people who are politically expressive on social media are more likely to participate in offline political activity. Content ownership\nSocial media content is generated by users.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Social_media",
        "title": "Social media"
      }
    ]
  },
  {
    "question": "Why is big data called dirty data?",
    "answer": "Error rates grow with scale.",
    "source_id": "Big data",
    "url": "https://en.wikipedia.org/wiki/Big_data",
    "category": "inferential",
    "generated_answer": "fraction of data inaccuracies",
    "response_time": "6.92s",
    "fused_chunks": [
      {
        "chunk_id": "Big_data_11",
        "text": "Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearabl...",
        "full_text": "Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality. \"Big data very often means 'dirty data' and the fraction of data inaccuracies increases with data volume growth.\" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed. While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use. The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust. Big data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research. Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research. A related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine. For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily. Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data. These are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance. Education\nA McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers and a number of universities including University of Tennessee and UC Berkeley, have created masters programs to meet this demand.",
        "rrf_score": 0.031099324975891997,
        "dense_rank": 1,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Big_data",
        "title": "Big data"
      },
      {
        "chunk_id": "Big_data_2",
        "text": "For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\" Definition\nThe term big data has been in use since the 1990s, with some giving credit to Jo...",
        "full_text": "For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\" Definition\nThe term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term. Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time. Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data. Big data \"size\" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data. Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale. Variability is often included as an additional quality of big data. A 2018 definition states \"Big data is where parallel computing tools are needed to handle data\", and notes, \"This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model.\" In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait. Instead of focusing on the intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed. Big data vs. business intelligence\nThe growing maturity of the concept more starkly delineates the difference between \"big data\" and \"business intelligence\":\n\nBusiness intelligence uses applied mathematics tools and descriptive statistics with data with high information density to measure things, detect trends, etc.",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 4,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Big_data",
        "title": "Big data"
      },
      {
        "chunk_id": "Big_data_13",
        "text": "It is controversial whether these predictions are currently being used for pricing. Internet of things (IoT)\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mappin...",
        "full_text": "It is controversial whether these predictions are currently being used for pricing. Internet of things (IoT)\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts. Kevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: \"If we had computers that knew everything there was to know about things—using data they gathered without any help from us—we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.\" Information technology\nEspecially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them. ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data. Survey science\nCompared to survey-based data collection, big data has low cost per data point, applies analysis techniques via machine learning and data mining, and includes diverse and new data sources, e.g., registers, social media, apps, and other forms digital data. Since 2018, survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality.",
        "rrf_score": 0.030798389007344232,
        "dense_rank": 7,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Big_data",
        "title": "Big data"
      },
      {
        "chunk_id": "Big_data_3",
        "text": "Big data uses mathematical analysis, optimization, inductive statistics, and concepts from nonlinear system identification to infer laws (regressions, nonlinear relationships, and causal effects) from...",
        "full_text": "Big data uses mathematical analysis, optimization, inductive statistics, and concepts from nonlinear system identification to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors. Characteristics\nBig data can be described by the following characteristics:\n\nVolume\nThe quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not. The size of big data is usually larger than terabytes and petabytes. Variety\nThe type and nature of the data. Earlier technologies like RDBMSs were capable to handle structured data efficiently and effectively. However, the change in type and nature from structured to semi-structured or unstructured challenged the existing tools and technologies. Big data technologies evolved with the prime intention to capture, store, and process the semi-structured and unstructured (variety) data generated with high speed (velocity), and huge in size (volume). Later, these tools and technologies were explored and used for handling structured data also but preferable for storage. Eventually, the processing of structured data was still kept as optional, either using big data or traditional RDBMSs. This helps in analyzing data towards effective usage of the hidden insights exposed from the data collected via social media, log files, sensors, etc. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion. Velocity\nThe speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time. Compared to small data, big data is produced more continually. Two kinds of velocity related to big data are the frequency of generation and the frequency of handling, recording, and publishing. Veracity\nThe truthfulness or reliability of the data, which refers to the data quality and the data value. Big data must not only be large in size, but also must be reliable in order to achieve value in the analysis of it.",
        "rrf_score": 0.03036576949620428,
        "dense_rank": 3,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Big_data",
        "title": "Big data"
      },
      {
        "chunk_id": "Big_data_23",
        "text": "Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and us...",
        "full_text": "Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting. Critique\nCritiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies. Critiques of the big data paradigm\n\"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data.\" In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory: focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts. Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by \"big judgment\", according to an article in the Harvard Business Review. Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\". Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the system's dynamics of the future change (if it is not a stationary process), the past can say little about the future.",
        "rrf_score": 0.02857142857142857,
        "dense_rank": 10,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Big_data",
        "title": "Big data"
      }
    ]
  },
  {
    "question": "Why is AI energy-intensive?",
    "answer": "Large data centers require high power.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "unanswerable",
    "response_time": "6.18s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_39",
        "text": "Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI\nAI researchers are divide...",
        "full_text": "Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks. Narrow vs. general AI\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively. Machine consciousness, sentience, and mind\nThere is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction. Consciousness\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain.",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 4,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_0",
        "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...",
        "full_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_4",
        "text": "An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was e...",
        "full_text": "An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was echoed by Scott Aaronson during his time at OpenAI. AI-complete problems\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that AGI would be needed to solve it, because the solution is beyond the capabilities of a purpose-specific algorithm. Many problems have been conjectured to require general intelligence to solve. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance. However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning. History\nClassical AI\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's fictional character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\". Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "Why can AI cause job loss?",
    "answer": "Automation replaces tasks.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "unanswerable",
    "response_time": "6.53s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_26",
        "text": "It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mas...",
        "full_text": "It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China. There are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours. Technological unemployment\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment. In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 1,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_28",
        "text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, la...",
        "full_text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Geoffrey Hinton, Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"",
        "rrf_score": 0.029211087420042643,
        "dense_rank": 7,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_35",
        "text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In ...",
        "full_text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. Philosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_27",
        "text": "Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. In July 2025, Ford CE...",
        "full_text": "Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. In July 2025, Ford CEO Jim Farley predicted that \"artificial intelligence is going to replace literally half of all white-collar workers in the U.S.\"\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement. Existential risk\nRecent public debates in artificial intelligence have increasingly focused on its broader societal and ethical implications. It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways. First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of an automated paperclip factory that destroys the world to get more iron for paperclips). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\". Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "Why is explainability needed?",
    "answer": "People deserve reasons for decisions.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "The better an explanation is at making predictions, the more useful it frequently can be and the more likely it will continue to explain a body of evidence better than its alternatives at explaining the evidence",
    "response_time": "10.94s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_21",
        "text": "Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine ex...",
        "full_text": "Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the data. The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes. In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users. Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research itself. Explainability\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. Overfitting\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is. Other limitations and vulnerabilities\nLearners can also be disappointed by \"learning the wrong lesson\".",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Scientific_method_23",
        "text": "The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it will continue to explain a body of evidence better than its alternatives. The most succ...",
        "full_text": "The better an explanation is at making predictions, the more useful it frequently can be, and the more likely it will continue to explain a body of evidence better than its alternatives. The most successful explanations – those that explain and make accurate predictions in a wide range of circumstances – are often called scientific theories. Most experimental results do not produce large changes in human understanding; improvements in theoretical scientific understanding typically result from a gradual process of development over time, sometimes across different domains of science. Scientific models vary in the extent to which they have been experimentally tested and for how long, and in their acceptance in the scientific community. In general, explanations become accepted over time as evidence accumulates on a given topic, and the explanation in question proves more powerful than its alternatives at explaining the evidence. Often subsequent researchers re-formulate the explanations over time, or combined explanations to produce new explanations. Properties of scientific inquiry\nScientific knowledge is closely tied to empirical findings and can remain subject to falsification if new experimental observations are incompatible with what is found. That is, no theory can ever be considered final since new problematic evidence might be discovered. If such evidence is found, a new theory may be proposed, or (more commonly) it is found that modifications to the previous theory are sufficient to explain the new evidence. The strength of a theory relates to how long it has persisted without major alteration to its core principles. Theories can also become subsumed by other theories. For example, Newton's laws explained thousands of years of scientific observations of the planets almost perfectly. However, these laws were then determined to be special cases of a more general theory (relativity), which explained both the (previously unexplained) exceptions to Newton's laws and predicted and explained other observations such as the deflection of light by gravity. Thus, in certain cases independent, unconnected, scientific observations can be connected, unified by principles of increasing explanatory power.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Scientific_method",
        "title": "Scientific method"
      },
      {
        "chunk_id": "Ethical_business_3",
        "text": "Statutes and regulations passed by legislative bodies and administrative boards set forth the \"law\". Slavery was once legal in the US, but one certainly would not say enslaving another was an \"ethical...",
        "full_text": "Statutes and regulations passed by legislative bodies and administrative boards set forth the \"law\". Slavery was once legal in the US, but one certainly would not say enslaving another was an \"ethical\" act. Economist Milton Friedman wrote that corporate executives' \"responsibility ... generally will be to make as much money as possible while conforming to their basic rules of the society, both those embodied in law and those embodied in ethical custom\". Friedman also said, \"the only entities who can have responsibilities are individuals ... A business cannot have responsibilities. So the question is, do corporate executives, provided they stay within the law, have responsibilities in their business activities other than to make as much money for their stockholders as possible? And my answer to that is, no, they do not.\" This view is known as the Friedman doctrine. A multi-country 2011 survey found support for this view among the \"informed public\" ranging from 30 to 80%. Ronald Duska and Jacques Cory have described Friedman's argument as consequentialist or utilitarian rather than pragmatic: Friedman's argument implies that unrestrained corporate freedom would benefit the most people in the long term. Duska argued that Friedman failed to differentiate two very different aspects of business: (1) the motive of individuals, who are generally motivated by profit to participate in business, and (2) the socially sanctioned purpose of business, or the reason why people allow businesses to exist, which is to provide goods and services to people. So Friedman was wrong that making a profit is the only concern of business, Duska argued. Peter Drucker once said, \"There is neither a separate ethics of business nor is one needed\", implying that standards of personal ethics cover all business situations. However, Drucker in another instance said that the ultimate responsibility of company directors is not to harm—primum non nocere. Philosopher and author Ayn Rand has put forth her idea of rational egoism, which also applies to business ethics.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Ethical_business",
        "title": "Ethical business"
      },
      {
        "chunk_id": "Scientific_method_7",
        "text": "This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong...",
        "full_text": "This hypothesis was also considered by Francis Crick and James D. Watson but discarded. When Watson and Crick learned of Pauling's hypothesis, they understood from existing data that Pauling was wrong. and that Pauling would soon admit his difficulties with that structure. A hypothesis is a suggested explanation of a phenomenon, or alternately a reasoned proposal suggesting a possible correlation between or among a set of phenomena. Normally, hypotheses have the form of a mathematical model. Sometimes, but not always, they can also be formulated as existential statements, stating that some particular instance of the phenomenon being studied has some characteristic and causal explanations, which have the general form of universal statements, stating that every instance of the phenomenon has a particular characteristic. Scientists are free to use whatever resources they have – their own creativity, ideas from other fields, inductive reasoning, Bayesian inference, and so on – to imagine possible explanations for a phenomenon under study. Albert Einstein once observed that \"there is no logical bridge between phenomena and their theoretical principles.\" Charles Sanders Peirce, borrowing a page from Aristotle (Prior Analytics, 2.25) described the incipient stages of inquiry, instigated by the \"irritation of doubt\" to venture a plausible guess, as abductive reasoning. The history of science is filled with stories of scientists claiming a \"flash of inspiration\", or a hunch, which then motivated them to look for evidence to support or refute their idea. Michael Polanyi made such creativity the centerpiece of his discussion of methodology. William Glen observes that\n\nthe success of a hypothesis, or its service to science, lies not simply in its perceived \"truth\", or power to displace, subsume or reduce a predecessor idea, but perhaps more in its ability to stimulate the research that will illuminate ... bald suppositions and areas of vagueness. In general, scientists tend to look for theories that are \"elegant\" or \"beautiful\". Scientists often use these terms to refer to a theory that is following the known facts but is nevertheless relatively simple and easy to handle.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Scientific_method",
        "title": "Scientific method"
      },
      {
        "chunk_id": "Scientific_method_17",
        "text": "When a narrative is constructed its elements become easier to believe. Fleck (1979), p. 27 notes \"Words and ideas are originally phonetic and mental equivalences of the experiences coinciding with the...",
        "full_text": "When a narrative is constructed its elements become easier to believe. Fleck (1979), p. 27 notes \"Words and ideas are originally phonetic and mental equivalences of the experiences coinciding with them. ... Such proto-ideas are at first always too broad and insufficiently specialized. ... Once a structurally complete and closed system of opinions consisting of many details and relations has been formed, it offers enduring resistance to anything that contradicts it\". Sometimes, these relations have their elements assumed a priori, or contain some other logical or methodological flaw in the process that ultimately produced them. Donald M. MacKay has analyzed these elements in terms of limits to the accuracy of measurement and has related them to instrumental elements in a category of measurement. Deductive and inductive reasoning\nThe idea of there being two opposed justifications for truth has shown up throughout the history of scientific method as analysis versus synthesis, non-ampliative/ampliative, or even confirmation and verification. (And there are other kinds of reasoning.) One to use what is observed to build towards fundamental truths – and the other to derive from those fundamental truths more specific principles. Deductive reasoning derives specific conclusions from established general principles—if the premises are true, the conclusion must be true. Inductive reasoning builds general principles from observations—conclusions are probable but not guaranteed. Scientific inquiry employs both: induction generates hypotheses from observations; deduction predicts testable consequences. This process requires stringent scepticism regarding observed phenomena, because cognitive assumptions can distort the interpretation of initial perceptions. An example for how inductive and deductive reasoning works can be found in the history of gravitational theory. It took thousands of years of measurements, from the Chaldean, Indian, Persian, Greek, Arabic, and European astronomers, to fully record the motion of planet Earth. Kepler(and others) were then able to build their early theories by generalizing the collected data inductively, and Newton was able to unify prior theory and measurements into the consequences of his laws of motion in 1727. Another common example of inductive reasoning is the observation of a counterexample to current theory inducing the need for new ideas.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 3,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Scientific_method",
        "title": "Scientific method"
      }
    ]
  },
  {
    "question": "Why regulate AI?",
    "answer": "To manage risks and ethics.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation",
    "response_time": "9.46s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_31",
        "text": "It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. Regulation\nThe regulation of artificial intelligence is the developme...",
        "full_text": "It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities. Regulation\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 1,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_28",
        "text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, la...",
        "full_text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Geoffrey Hinton, Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 4,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_32",
        "text": "It was adopted by the European Union, the United States, the United Kingdom, and other signatories. In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but...",
        "full_text": "It was adopted by the European Union, the United States, the United Kingdom, and other signatories. In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\". In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI. History\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.",
        "rrf_score": 0.03055037313432836,
        "dense_rank": 7,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "Why does bias appear in AI?",
    "answer": "Models learn from biased data.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "inferential",
    "generated_answer": "because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women",
    "response_time": "9.25s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_38",
        "text": "However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"inte...",
        "full_text": "However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him. The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches. Neat vs. scruffy\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both. Soft vs. hard computing\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.",
        "rrf_score": 0.03125763125763126,
        "dense_rank": 5,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_23",
        "text": "Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data tha...",
        "full_text": "Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive. Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women. There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws. At the 2022 ACM Conference on Fairness, Accountability, and Transparency a paper reported that a CLIP‑based (Contrastive Language-Image Pre-training) robotic system reproduced harmful gender‑ and race‑linked stereotypes in a simulated manipulation task. The authors recommended robot‑learning methods which physically manifest such harms be \"paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just.\" Lack of transparency\nMany AI systems are so complex that their designers cannot explain how they reach their decisions.",
        "rrf_score": 0.031099324975891997,
        "dense_rank": 1,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_14",
        "text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AG...",
        "full_text": "Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things. Consciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts. Self-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks. These traits have a moral dimension.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Decision-making_13",
        "text": "In one demonstration of this effect, the discounting of arguments with which one disagrees (by judging them as untrue or irrelevant) was decreased by selective activation of the right prefrontal corte...",
        "full_text": "In one demonstration of this effect, the discounting of arguments with which one disagrees (by judging them as untrue or irrelevant) was decreased by selective activation of the right prefrontal cortex. Wishful thinking is a tendency to want to see things in a certain – usually positive – light, which can distort perception and thinking. Choice-supportive bias occurs when people distort their memories of chosen and rejected options to make the chosen options seem more attractive. Recency: People tend to place more attention on more recent information and either ignore or forget more distant information (see Semantic priming). The opposite effect in the first set of data or other information is termed primacy effect. Repetition bias is a willingness to believe what one has been told most often and by the greatest number of different sources. Anchoring and adjustment: Decisions are unduly influenced by initial information that shapes our view of subsequent information. Groupthink is peer pressure to conform to the opinions held by the group. Source credibility bias is a tendency to reject a person's statement on the basis of a bias against the person, organization, or group to which the person belongs. People preferentially accept statements by others that they like (see also Prejudice). Incremental decision-making and escalating commitment: People look at a decision as a small step in a process, and this tends to perpetuate a series of similar decisions. This can be contrasted with zero-based decision-making (see Slippery slope). Attribution asymmetry: People tend to attribute their own success to internal factors, including abilities and talents, but explain their failures in terms of external factors such as bad luck. The reverse bias is shown when people explain others' success or failure. Role fulfillment is a tendency to conform to others' decision-making expectations. Underestimating uncertainty and the illusion of control: People tend to underestimate future uncertainty because of a tendency to believe they have more control over events than they really do. Framing bias: This is best avoided by increasing numeracy and presenting data in several formats (for example, using both absolute and relative scales).",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Decision-making",
        "title": "Decision-making"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      }
    ]
  },
  {
    "question": "Why is CV tied to biology?",
    "answer": "Inspired by visual systems.",
    "source_id": "Computer vision",
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "category": "inferential",
    "generated_answer": "unanswerable",
    "response_time": "7.03s",
    "fused_chunks": [
      {
        "chunk_id": "Conservation_biology_0",
        "text": "Conservation biology is the study of the conservation of nature and of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and th...",
        "full_text": "Conservation biology is the study of the conservation of nature and of Earth's biodiversity with the aim of protecting species, their habitats, and ecosystems from excessive rates of extinction and the erosion of biotic interactions. It is an interdisciplinary subject drawing on natural and social sciences, and the practice of natural resource management. The conservation ethic is based on the findings of conservation biology. Origins\nThe term conservation biology and its conception as a new field originated with the convening of \"The First International Conference on Research in Conservation Biology\" held at the University of California, San Diego in La Jolla, California, in 1978 led by American biologists Bruce A. Wilcox and Michael E. Soulé with a group of leading university and zoo researchers and conservationists including Kurt Benirschke, Sir Otto Frankel, Thomas Lovejoy, and Jared Diamond. The meeting was prompted due to concern over tropical deforestation, disappearing species, and eroding genetic diversity within species. The conference and proceedings that resulted sought to initiate the bridging of a gap between theory in ecology and evolutionary genetics on the one hand and conservation policy and practice on the other. Conservation biology and the concept of biological diversity (biodiversity) emerged together, helping crystallize the modern era of conservation science and policy. The inherent multidisciplinary basis for conservation biology has led to new subdisciplines including conservation social science, conservation behavior and conservation physiology. It stimulated further development of conservation genetics which Otto Frankel had originated first but is now often considered a subdiscipline as well. Description\nThe rapid decline of established biological systems around the world means that conservation biology is often referred to as a \"Discipline with a deadline\". Conservation biology is tied closely to ecology in researching the population ecology (dispersal, migration, demographics, effective population size, inbreeding depression, and minimum population viability) of rare or endangered species. Conservation biology is concerned with phenomena that affect the maintenance, loss, and restoration of biodiversity and the science of sustaining evolutionary processes that engender genetic, population, species, and ecosystem diversity.",
        "rrf_score": 0.03177805800756621,
        "dense_rank": 5,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Conservation_biology",
        "title": "Conservation biology"
      },
      {
        "chunk_id": "Physiology_1",
        "text": "Genetics is the study of heredity and the variation of traits within and between populations. It provides insights into the genetic basis of physiological processes and the ways in which genes interac...",
        "full_text": "Genetics is the study of heredity and the variation of traits within and between populations. It provides insights into the genetic basis of physiological processes and the ways in which genes interact with the environment to influence an organism's phenotype. Evolutionary biology is the study of the processes that have led to the diversity of life on Earth. It helps to explain the origin and adaptive significance of physiological processes and the ways in which organisms have evolved to cope with their environment. Subdisciplines\nThere are many ways to categorize the subdisciplines of physiology:\n\nbased on the taxa studied: human physiology, animal physiology, plant physiology, microbial physiology, viral physiology\nbased on the level of organization: cell physiology, molecular physiology, systems physiology, organismal physiology, ecological physiology, integrative physiology\nbased on the process that causes physiological variation: developmental physiology, environmental physiology, evolutionary physiology\nbased on the ultimate goals of the research: applied physiology (e.g., medical physiology), non-applied (e.g., comparative physiology)\n\nSubdisciplines by level of organisation\nCell physiology\nAlthough there are differences between animal, plant, and microbial cells, the basic physiological functions of cells can be divided into the processes of cell division, cell signaling, cell growth, and cell metabolism. Subdisciplines by taxa\nPlant physiology\nPlant physiology is a subdiscipline of botany concerned with the functioning of plants. Closely related fields include plant morphology, plant ecology, phytochemistry, cell biology, genetics, biophysics, and molecular biology. Fundamental processes of plant physiology include photosynthesis, respiration, plant nutrition, tropisms, nastic movements, photoperiodism, photomorphogenesis, circadian rhythms, seed germination, dormancy, and stomata function and transpiration. Absorption of water by roots, production of food in the leaves, and growth of shoots towards light are examples of plant physiology. Animal physiology\nHuman physiology\nHuman physiology is the study of how the human body's systems and functions work together to maintain a stable internal environment. It includes the study of the nervous, endocrine, cardiovascular, respiratory, digestive, and urinary systems, as well as cellular and exercise physiology.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Physiology",
        "title": "Physiology"
      },
      {
        "chunk_id": "Biology_0",
        "text": "Biology is the scientific study of life and living organisms. It is a broad natural science that encompasses a wide range of fields and unifying principles that explain the structure, function, growth...",
        "full_text": "Biology is the scientific study of life and living organisms. It is a broad natural science that encompasses a wide range of fields and unifying principles that explain the structure, function, growth, origin, evolution, and distribution of life. Central to biology are five fundamental themes: the cell as the basic unit of life, genes and heredity as the basis of inheritance, evolution as the driver of biological diversity, energy transformation for sustaining life processes, and the maintenance of internal stability (homeostasis). Biology examines life across multiple levels of organization, from molecules and cells to organisms, populations, and ecosystems. Subdisciplines include molecular biology, physiology, ecology, evolutionary biology, developmental biology, and systematics, among others. Each of these fields applies a range of methods to investigate biological phenomena, including observation, experimentation, and mathematical modeling. Modern biology is grounded in the theory of evolution by natural selection, first articulated by Charles Darwin, and in the molecular understanding of genes encoded in DNA. The discovery of the structure of DNA and advances in molecular genetics have transformed many areas of biology, leading to applications in medicine, agriculture, biotechnology, and environmental science. Life on Earth is believed to have originated over 3.7 billion years ago. Today, it includes a vast diversity of organisms—from single-celled archaea and bacteria to complex multicellular plants, fungi, and animals. Biologists classify organisms based on shared characteristics and evolutionary relationships, using taxonomic and phylogenetic frameworks. These organisms interact with each other and with their environments in ecosystems, where they play roles in energy flow and nutrient cycling. As a constantly evolving field, biology incorporates new discoveries and technologies that enhance the understanding of life and its processes, while contributing to solutions for challenges such as disease, climate change, and biodiversity loss. Etymology\nFrom Greek βίος (bíos) 'life', (from Proto-Indo-European root *gwei-, to live) and λογία (logia) 'study of'.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Biology",
        "title": "Biology"
      },
      {
        "chunk_id": "Scientific_method_5",
        "text": "But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, start...",
        "full_text": "But the mechanism of storing genetic information (i.e., genes) in DNA was unclear. Researchers in Bragg's laboratory at Cambridge University made X-ray diffraction pictures of various molecules, starting with crystals of salt, and proceeding to more complicated substances. Using clues painstakingly assembled over decades, beginning with its chemical composition, it was determined that it should be possible to characterize the physical structure of DNA, and the X-ray images would be the vehicle. The scientific method depends upon increasingly sophisticated characterizations of the subjects of investigation. (The subjects can also be called unsolved problems or the unknowns.) For example, Benjamin Franklin conjectured, correctly, that St. Elmo's fire was electrical in nature, but it has taken a long series of experiments and theoretical changes to establish this. While seeking the pertinent properties of the subjects, careful thought may also entail some definitions and observations; these observations often demand careful measurements and/or counting can take the form of expansive empirical research. A scientific question can refer to the explanation of a specific observation, as in \"Why is the sky blue?\" but can also be open-ended, as in \"How can I design a drug to cure this particular disease?\" This stage frequently involves finding and evaluating evidence from previous experiments, personal scientific observations or assertions, as well as the work of other scientists. If the answer is already known, a different question that builds on the evidence can be posed. When applying the scientific method to research, determining a good question can be very difficult and it will affect the outcome of the investigation. The systematic, careful collection of measurements or counts of relevant quantities is often the critical difference between pseudo-sciences, such as alchemy, and science, such as chemistry or biology. Scientific measurements are usually tabulated, graphed, or mapped, and statistical manipulations, such as correlation and regression, performed on them. The measurements might be made in a controlled setting, such as a laboratory, or made on more or less inaccessible or unmanipulatable objects such as stars or human populations.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Scientific_method",
        "title": "Scientific method"
      },
      {
        "chunk_id": "Sociology_32",
        "text": "The ISA maintains a Research Committee on Science and Technology. Leisure\nSociology of leisure is the study of how humans organize their free time. Leisure includes a broad array of activities, such a...",
        "full_text": "The ISA maintains a Research Committee on Science and Technology. Leisure\nSociology of leisure is the study of how humans organize their free time. Leisure includes a broad array of activities, such as sport, tourism, and the playing of games. The sociology of leisure is closely tied to the sociology of work, as each explores a different side of the work–leisure relationship. More recent studies in the field move away from the work–leisure relationship and focus on the relation between leisure and culture. This area of sociology began with Thorstein Veblen's Theory of the Leisure Class. Peace, war, and conflict\nThis subfield of sociology studies, broadly, the dynamics of war, conflict resolution, peace movements, war refugees, conflict resolution and military institutions. As a subset of this subfield, military sociology aims towards the systematic study of the military as a social group rather than as an organization. It is a highly specialized sub-field which examines issues related to service personnel as a distinct group with coerced collective action based on shared interests linked to survival in vocation and combat, with purposes and values that are more defined and narrower than within civil society. Military sociology also concerns civilian-military relations and interactions between other groups or governmental agencies. Topics include the dominant assumptions held by those in the military, changes in military members' willingness to fight, military unionization, military professionalism, the increased utilization of women, the military industrial-academic complex, the military's dependence on research, and the institutional and organizational structure of military. Political sociology\nHistorically, political sociology concerned the relations between political organization and society. A typical research question in this area might be: \"Why do so few American citizens choose to vote?\" In this respect questions of political opinion formation brought about some of the pioneering uses of statistical survey research by Paul Lazarsfeld. A major subfield of political sociology developed in relation to such questions, which draws on comparative history to analyse socio-political trends. The field developed from the work of Max Weber and Moisey Ostrogorsky.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      }
    ]
  },
  {
    "question": "Why is trade complex internationally?",
    "answer": "Different laws and currencies.",
    "source_id": "International trade",
    "url": "https://en.wikipedia.org/wiki/International_trade",
    "category": "inferential",
    "generated_answer": "cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural",
    "response_time": "12.13s",
    "fused_chunks": [
      {
        "chunk_id": "International_trade_0",
        "text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade repr...",
        "full_text": "International trade is the exchange of capital, goods, and services across international borders or territories because there is a need or want of goods or services. In most countries, such trade represents a significant share of gross domestic product (GDP). While international trade has existed throughout history (for example Uttarapatha, Silk Road, Amber Road, salt roads), its economic, social, and political importance has been on the rise in recent centuries. Carrying out trade at an international level is a complex process when compared to domestic trade. When trade takes place between two or more states, factors like currency, government policies, economy, judicial system, laws, and markets influence trade. To ease and justify the process of trade between countries of different economic standing in the modern era, some international economic organizations were formed, such as the World Trade Organization. These organizations work towards the facilitation and growth of international trade. Statistical services of intergovernmental and supranational organizations and governmental statistical agencies publish official statistics on international trade. Characteristics of global trade\nA product that is transferred or sold from a party in one country to a party in another country is an export from the originating country, and an import to the country receiving that product. Imports and exports are accounted for in a country's current account in the balance of payments. Trading globally may give consumers and countries the opportunity to be exposed to new markets and  products. Almost every kind of product can be found in the international market, for example: food, clothes, spare parts, oil, jewellery, wine, stocks, currencies, and water. Services are also traded, such as in tourism, banking, consulting, and transportation. Advanced technology (including transportation), globalization, industrialization, outsourcing and multinational corporations have major impacts on the international trade systems. Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not.",
        "rrf_score": 0.030679156908665108,
        "dense_rank": 1,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "International_trade_1",
        "text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally re...",
        "full_text": "Differences from domestic trade\nInternational trade is, in principle, not different from domestic trade as the motivation and the behavior of parties involved in a trade do not change fundamentally regardless of whether trade is across a border or not. However, in practical terms, carrying out trade at an international level is typically a more complex process than domestic trade. The main difference is that international trade is typically more costly than domestic trade. This is due to the fact that cross-border trade typically incurs additional costs such as explicit tariffs as well as explicit or implicit non-tariff barriers such as time costs (due to border delays), language and cultural differences, product safety, the legal system, and so on. Another difference between domestic and international trade is that factors of production such as capital and labor are often more mobile within a country than across countries. Thus, international trade is mostly restricted to trade in goods and services, and only to a lesser extent to trade in capital, labour, or other factors of production. Trade in goods and services can serve as a substitute for trade in factors of production. Instead of importing a factor of production, a country can import goods that make intensive use of that factor of production and thus embody it. An example of this is the import of labor-intensive goods by the United States from China. Instead of importing Chinese labor, the United States imports goods that were produced with Chinese labor. One report in 2010, suggested that international trade was increased when a country hosted a network of immigrants, but the trade effect was weakened when the immigrants became assimilated into their new country. History\nThe history of international trade chronicles notable events that have affected trading among various economies. Theories and models\nThere are several models that seek to explain the factors behind international trade, the welfare consequences of trade and the pattern of trade. Most traded export products\nLargest countries or regions by total international trade\nThe following table is a list of the 30 largest trading states according to the World Trade Organization in 2024.",
        "rrf_score": 0.03057889822595705,
        "dense_rank": 3,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/International_trade",
        "title": "International trade"
      },
      {
        "chunk_id": "Language_25",
        "text": "Latin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word o...",
        "full_text": "Latin uses morphology to express the distinction between subject and object, whereas English uses word order. Another example of how syntactic rules contribute to meaning is the rule of inverse word order in questions, which exists in many languages. This rule explains why when in English, the phrase \"John is talking to Lucy\" is turned into a question, it becomes \"Who is John talking to? \", and not \"John is talking to who?\". The latter example may be used as a way of placing special emphasis on \"who\", thereby slightly altering the meaning of the question. Syntax also includes the rules for how complex sentences are structured by grouping words together in units, called phrases, that can occupy different places in a larger syntactic structure. Sentences can be described as consisting of phrases connected in a tree structure, connecting the phrases to each other at different levels. To the right is a graphic representation of the syntactic analysis of the English sentence \"the cat sat on the mat\". The sentence is analyzed as being constituted by a noun phrase, a verb, and a prepositional phrase; the prepositional phrase is further divided into a preposition and a noun phrase, and the noun phrases consist of an article and a noun. The reason sentences can be seen as being composed of phrases is because each phrase would be moved around as a single element if syntactic operations were carried out. For example, \"the cat\" is one phrase, and \"on the mat\" is another, because they would be treated as single units if a decision was made to emphasize the location by moving forward the prepositional phrase: \"[And] on the mat, the cat sat\". There are many different formalist and functionalist frameworks that propose theories for describing syntactic structures, based on different assumptions about what language is and how it should be described. Each of them would analyze a sentence such as this in a different manner. Typology and universals\nLanguages can be classified in relation to their grammatical types.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Language",
        "title": "Language"
      },
      {
        "chunk_id": "Globalization_12",
        "text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never reco...",
        "full_text": "Modern consensus for the last 15 years regards globalization as having run its course and gone into decline. A common argument for this is that trade has dropped since its peak in 2008, and never recovered since the Great Recession. New opposing views from some economists have argued such trends are a result of price drops and in actuality, trade volume is increasing, especially with agricultural products, natural resources and refined petroleum. The 21st century melting of the Arctic will also affect global trade, as it is paving the way for shorter trade routes. Economic globalization\nEconomic globalization is the increasing economic interdependence of national economies across the world through a rapid increase in cross-border movement of goods, services, technology, and capital. Whereas the globalization of business is centered around the diminution of international trade regulations as well as tariffs, taxes, and other impediments that suppresses global trade, economic globalization is the process of increasing economic integration between countries, leading to the emergence of a global marketplace or a single world market. Depending on the paradigm, economic globalization can be viewed as either a positive or a negative phenomenon. Economic globalization comprises: globalization of production; which refers to the obtainment of goods and services from a particular source from locations around the globe to benefit from difference in cost and quality. Likewise, it also comprises globalization of markets; which is defined as the union of different and separate markets into a massive global marketplace. Economic globalization also includes competition, technology, and corporations and industries. Current globalization trends can be largely accounted for by developed economies integrating with less developed economies by means of foreign direct investment, the reduction of trade barriers as well as other economic reforms, and, in many cases, immigration. International standards have made trade in goods and services more efficient. An example of such standard is the intermodal container. Containerization dramatically reduced the costs of transportation, supported the post-war boom in international trade, and was a major element in globalization. International standards are set by the International Organization for Standardization, which is composed of representatives from various national standards organizations.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Globalization",
        "title": "Globalization"
      },
      {
        "chunk_id": "Utilitarianism_39",
        "text": "He wrote in his System of Logic I iv. 3, of 'Caesar is dead and Brutus is alive', that 'we might as well call a street a complex house, as these two propositions a complex proposition'.\" Finally, whil...",
        "full_text": "He wrote in his System of Logic I iv. 3, of 'Caesar is dead and Brutus is alive', that 'we might as well call a street a complex house, as these two propositions a complex proposition'.\" Finally, whilst motives may not play a role in determining the morality of an action, this does not preclude utilitarians from fostering particular motives if doing so will increase overall happiness. Other sentient beings\nIn An Introduction to the Principles of Morals and Legislation, Bentham wrote \"the question is not, Can they reason? nor, Can they talk? but, Can they suffer?\" Mill's distinction between higher and lower pleasures might suggest that he gave more status to humans. However, in his essay \"Whewell on Moral Philosophy\", Mill defends Bentham's position, calling it a \"noble anticipation\", and writing: \"Granted that any practice causes more pain to animals than it gives pleasure to man; is that practice moral or immoral? And if, exactly in proportion as human beings raise their heads out of the slough of selfishness, they do not with one voice answer 'immoral', let the morality of the principle of utility be for ever condemned.\" Henry Sidgwick also considers the implications of utilitarianism for nonhuman animals. He writes:\"We have next to consider who the 'all' are, whose happiness is to be taken into account. Are we to extend our concern to all the beings capable of pleasure and pain whose feelings are affected by our conduct? or are we to confine our view to human happiness? The former view is the one adopted by Bentham and Mill, and (I believe) by the Utilitarian school generally: and is obviously most in accordance with the universality that is characteristic of their principle ... it seems arbitrary and unreasonable to exclude from the end, as so conceived, any pleasure of any sentient being. \"Among contemporary utilitarian philosophers, Peter Singer is especially known for arguing that the well-being of all sentient beings ought to be given equal consideration.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Utilitarianism",
        "title": "Utilitarianism"
      }
    ]
  },
  {
    "question": "How do GPUs and data together boost AI?",
    "answer": "They allow fast learning from large datasets.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "increase in computer power (including the hundred-fold increase in speed by switching",
    "response_time": "9.34s",
    "fused_chunks": [
      {
        "chunk_id": "Machine_learning_25",
        "text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financi...",
        "full_text": "It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\" Financial incentives\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. Hardware\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. Tensor Processing Units (TPUs)\nTensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specifically for machine learning workloads. Unlike general-purpose GPUs and FPGAs, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency. Since their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.",
        "rrf_score": 0.030776515151515152,
        "dense_rank": 4,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.0304147465437788,
        "dense_rank": 10,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_35",
        "text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In ...",
        "full_text": "Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019. In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study. In the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies. Philosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI. Defining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"",
        "rrf_score": 0.028991596638655463,
        "dense_rank": 8,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_7",
        "text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by t...",
        "full_text": "AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers. Feasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. An additional challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions? Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI. John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Deep_learning_9",
        "text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision ...",
        "full_text": "In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees. Deep learning revolution\nThe deep learning revolution started around CNN- and GPU-based computer vision. Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning. A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004. In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training. In 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. It then won more contests. They also showed how max-pooling CNNs on GPU improved performance significantly. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos. In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman and Google's Inceptionv3. The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs. In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      }
    ]
  },
  {
    "question": "How do AI agents improve?",
    "answer": "They learn from experience.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "through experience or training",
    "response_time": "6.55s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_14",
        "text": "Military\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting i...",
        "full_text": "Military\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous. AI has been used in military operations in Iraq, Syria, Israel and Ukraine. Generative AI\nAgents\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks. Web search\nMicrosoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search. For safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content. Google officially pushed its AI Search at its Google I/O event on 20 May 2025. It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 3,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_general_intelligence_4",
        "text": "An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was e...",
        "full_text": "An AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was echoed by Scott Aaronson during his time at OpenAI. AI-complete problems\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that AGI would be needed to solve it, because the solution is beyond the capabilities of a purpose-specific algorithm. Many problems have been conjectured to require general intelligence to solve. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance. However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning. History\nClassical AI\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's fictional character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\". Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "title": "Artificial general intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_28",
        "text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, la...",
        "full_text": "Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Geoffrey Hinton, Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI. In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI. In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Some other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_0",
        "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and dec...",
        "full_text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human. Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Artificial_intelligence_3",
        "text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent m...",
        "full_text": "It can then choose the action with the maximum expected utility. In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked. In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be. A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned. Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents. Learning\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      }
    ]
  },
  {
    "question": "How does RLHF reduce hallucinations?",
    "answer": "Human feedback corrects outputs.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "Reinforcing learning from human feedback",
    "response_time": "7.54s",
    "fused_chunks": [
      {
        "chunk_id": "Artificial_intelligence_10",
        "text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoreti...",
        "full_text": "The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet. GPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text. Current models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text. Hardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Religion_13",
        "text": "Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities. Hallucinations and delusion...",
        "full_text": "Scholars in this field seek to explain how human minds acquire, generate, and transmit religious thoughts, practices, and schemas by means of ordinary cognitive capacities. Hallucinations and delusions related to religious content occurs in about 60% of people with schizophrenia. While this number varies across cultures, this had led to theories about a number of influential religious phenomena and possible relation to psychotic disorders. A number of prophetic experiences are consistent with psychotic symptoms, although retrospective diagnoses are practically impossible. Schizophrenic episodes are also experienced by people who do not have belief in gods. Religious content is also common in temporal lobe epilepsy, and obsessive-compulsive disorder. Atheistic content is also found to be common with temporal lobe epilepsy. Comparativism\nComparative religion is the branch of the study of religions concerned with the systematic comparison of the doctrines and practices of the world's religions. In general, the comparative study of religion yields a deeper understanding of the fundamental philosophical concerns of religion such as ethics, metaphysics, and the nature and form of salvation. Studying such material is meant to give one a richer and more sophisticated understanding of human beliefs and practices regarding the sacred, numinous, spiritual and divine. In the field of comparative religion, a common geographical classification of the main world religions includes Middle Eastern religions (including Zoroastrianism and Iranian religions), Indian religions, East Asian religions, African religions, American religions, Oceanic religions, and classical Hellenistic religions. Classification\nIn the 19th and 20th centuries, the academic practice of comparative religion divided religious belief into philosophically defined categories called world religions. Some academics studying the subject have divided religions into three broad categories:\n\nWorld religions, a term which refers to transcultural, international religions;\nIndigenous religions, which refers to smaller, culture-specific or nation-specific religious groups; and\nNew religious movements, which refers to recently developed religions.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Religion",
        "title": "Religion"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Psychoanalysis_9",
        "text": "All attempts to develop a suitable neuronal treatment failed; in fact, the detailed examinations did not reveal any organic defects. In the monograph written on this cases, Freud documented his differ...",
        "full_text": "All attempts to develop a suitable neuronal treatment failed; in fact, the detailed examinations did not reveal any organic defects. In the monograph written on this cases, Freud documented his differential-diagnostically supported suspicion that neurotic symptoms probably would have psychological causes. Finishing the ineffective hypnosis, the idea of psychoanalysis began to receive serious attention; Freud initially called it free association. His first attempt to explain neurotical symptoms on this path was presented in Studies on Hysteria (1895). Co-authored with Josef Breuer, this is generally seen as the birth of psychoanalysis. The work based on their partly joint treatment of Bertha Pappenheim, referred to by the pseudonym \"Anna O.\" Bertha herself had dubbed the treatment talking cure. Breuer, a distinguished physician, was astonished but remained unspecific; while Freud formulated his hypothesis that Anna's hystera seemed to be caused by distressing but unconscious experiences related to sexuality, basing his assumption on corresponding free associations made by the young women. For example, she sometimes liked to jokingly rename her talking cure as chimney sweeping, an association about the fairy tale through which part of a pregnant woman's house 'the stork' gives birth to the baby – or in Lacan's words: \"The more Anna provided signifers, the more she chattered on, the better it went.\" See also Jokes and Their Relation to the Unconscious (1905). Around the same time, Freud had started to develop a neurological hypothesis about mental phenomena such as memory, but soon abandoned this attempt and left it unpublished. Insights into neuronal-biochemical processes that store experiences in the brain – like engraving the proverbial tabula rasa with some code – belongs to the physiological branch of science and lead in a different direction of research than the psychological question of what the differences between consciousness and unconsciousness are. After some thought about a suitable term, Freud called his new instrument and field of research psychoanalysis, introduced in his essay \"Inheritance and Etiology of Neuroses\", written in 1896.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Psychoanalysis",
        "title": "Psychoanalysis"
      },
      {
        "chunk_id": "Problem_solving_4",
        "text": "Such techniques can also be applied to a product or process prior to an actual failure event—to predict, analyze, and mitigate a potential problem in advance. Techniques such as failure mode and effec...",
        "full_text": "Such techniques can also be applied to a product or process prior to an actual failure event—to predict, analyze, and mitigate a potential problem in advance. Techniques such as failure mode and effects analysis can proactively reduce the likelihood of problems. In either the reactive or the proactive case, it is necessary to build a causal explanation through a process of diagnosis. In deriving an explanation of effects in terms of causes, abduction generates new ideas or hypotheses (asking \"how? \"); deduction evaluates and refines hypotheses based on other plausible premises (asking \"why? \"); and induction justifies a hypothesis with empirical data (asking \"how much?\"). The objective of abduction is to determine which hypothesis or proposition to test, not which one to adopt or assert. In the Peircean logical system, the logic of abduction and deduction contribute to our conceptual understanding of a phenomenon, while the logic of induction adds quantitative details (empirical substantiation) to our conceptual knowledge. Forensic engineering is an important technique of failure analysis that involves tracing product defects and flaws. Corrective action can then be taken to prevent further failures. Reverse engineering attempts to discover the original problem-solving logic used in developing a product by disassembling the product and developing a plausible pathway to creating and assembling its parts. Physics\nIn physics, problem solving refers to the process by which one transforms an initial physical situation into a goal state by applying physics-specific reasoning and analysis. This involves identifying the relevant physical principles, making assumptions, formulating and manipulating equations, and checking whether the result is reasonable. A physics problem is not simply application or recall of a formula, but requires understanding the underlying concepts and navigating through a \"problem space\" of possible knowledge states toward the goal. Military science\nIn military science, problem solving is linked to the concept of \"end-states\", the conditions or situations which are the aims of the strategy. Ability to solve problems is important at any military rank, but is essential at the command and control level. It results from deep qualitative and quantitative understanding of possible scenarios.",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Problem_solving",
        "title": "Problem solving"
      }
    ]
  },
  {
    "question": "How do filter bubbles form?",
    "answer": "Algorithms repeat similar content.",
    "source_id": "Artificial intelligence",
    "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "category": "multi-hop",
    "generated_answer": "they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust",
    "response_time": "10.56s",
    "fused_chunks": [
      {
        "chunk_id": "Virology_1",
        "text": "Virology is a broad subject covering biology, health, animal welfare, agriculture and ecology. History\nLouis Pasteur was unable to find a causative agent for rabies and speculated about a pathogen too...",
        "full_text": "Virology is a broad subject covering biology, health, animal welfare, agriculture and ecology. History\nLouis Pasteur was unable to find a causative agent for rabies and speculated about a pathogen too small to be detected by microscopes. In 1884, the French microbiologist Charles Chamberland invented the Chamberland filter (or Pasteur-Chamberland filter) with pores small enough to remove all bacteria from a solution passed through it. In 1892, the Russian biologist Dmitri Ivanovsky used this filter to study what is now known as the tobacco mosaic virus: crushed leaf extracts from infected tobacco plants remained infectious even after filtration to remove bacteria. Ivanovsky suggested the infection might be caused by a toxin produced by bacteria, but he did not pursue the idea. At the time it was thought that all infectious agents could be retained by filters and grown on a nutrient medium—this was part of the germ theory of disease. In 1898, the Dutch microbiologist Martinus Beijerinck repeated the experiments and became convinced that the filtered solution contained a new form of infectious agent. He observed that the agent multiplied only in cells that were dividing, but as his experiments did not show that it was made of particles, he called it a contagium vivum fluidum (soluble living germ) and reintroduced the word virus. Beijerinck maintained that viruses were liquid in nature, a theory later discredited by Wendell Stanley, who proved they were particulate. In the same year, Friedrich Loeffler and Paul Frosch passed the first animal virus, aphthovirus (the agent of foot-and-mouth disease), through a similar filter. In the early 20th century, the English bacteriologist Frederick Twort discovered a group of viruses that infect bacteria, now called bacteriophages (or commonly 'phages'), in 1915. The French-Canadian microbiologist Félix d'Herelle announced his independent discovery of bacteriophages in 1917. D'Herelle described viruses that, when added to bacteria on an agar plate, would produce areas of dead bacteria. He accurately diluted a suspension of these viruses and discovered that the highest dilutions (lowest virus concentrations), rather than killing all the bacteria, formed discrete areas of dead organisms.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 1,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Virology",
        "title": "Virology"
      },
      {
        "chunk_id": "Human_body_9",
        "text": "Emphysema, commonly a result of smoking, is caused by damage to connections between the alveoli. Urinary system\nThe urinary system consists of the two kidneys, two ureters, bladder, and urethra. It re...",
        "full_text": "Emphysema, commonly a result of smoking, is caused by damage to connections between the alveoli. Urinary system\nThe urinary system consists of the two kidneys, two ureters, bladder, and urethra. It removes waste materials from the blood through urine, which carries a variety of waste molecules and excess ions and water out of the body. First, the kidneys filter the blood through their respective nephrons, removing waste products like urea, creatinine and maintaining the proper balance of electrolytes and turning the waste products into urine by combining them with water from the blood. The kidneys filter about 150 quarts (170 liters) of blood daily, but most of it is returned to the blood stream with only 1-2 quarts (1-2 liters) ending up as urine, which passes from the kidneys through the ureters into the bladder. The smooth muscles lining the ureter walls continuously tighten and relax through a process called peristalsis, forcing small amounts of urine into the bladder every 10–15 seconds. The bladder is a hollow balloon shaped organ located in the pelvis. It stores urine until the brain signals it to relax the urinary sphincter and release the urine into the urethra starting urination. A normal bladder can hold up to 16 ounces (half a liter) for 3–5 hours comfortably. Numerous diseases affect the urinary system including kidney stones, which are formed when materials in the urine concentrate enough to form a solid mass, urinary tract infections, which are infections of the urinary tract and can cause pain when urinating, frequent urination and even death if left untreated. Renal failure occurs when the kidneys fail to adequately filter waste from the blood and can lead to death if not treated with dialysis or kidney transplantation. Cancer can affect the bladder, kidneys, urethra and ureters, with the latter two being far more rare. Anatomy\nHuman anatomy is the study of the shape and form of the human body. The human body has four limbs (two arms and two legs), a head and a neck, which connect to the torso.",
        "rrf_score": 0.031054405392392875,
        "dense_rank": 2,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Human_body",
        "title": "Human body"
      },
      {
        "chunk_id": "Artificial_intelligence_21",
        "text": "Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that t...",
        "full_text": "Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem. In the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer and Nobel Prize-winning computer scientist Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. The ability to influence electorates has been proved in at least one study. This same study shows more inaccurate statements from the models when they advocate for candidates of the political right. AI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models. Algorithmic bias and fairness\nMachine learning applications can be biased if they learn from biased data. The developers may not be aware that the bias exists. Discriminatory behavior by some LLMs can be observed in their output. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases. On 28 June 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "title": "Artificial intelligence"
      },
      {
        "chunk_id": "Semantics_1",
        "text": "Semantics is relevant to the fields of formal logic, computer science, and psychology. Definition and related fields\nSemantics is the study of meaning in languages. It is a systematic inquiry that exa...",
        "full_text": "Semantics is relevant to the fields of formal logic, computer science, and psychology. Definition and related fields\nSemantics is the study of meaning in languages. It is a systematic inquiry that examines what linguistic meaning is and how it arises. It investigates how expressions are built up from different layers of constituents, like morphemes, words, clauses, sentences, and texts, and how the meanings of the constituents affect one another. Semantics can focus on a specific language, like English, but in its widest sense, it investigates meaning structures relevant to all languages. As a descriptive discipline, it aims to determine how meaning works without prescribing what meaning people should associate with particular expressions. Some of its key questions are \"How do the meanings of words combine to create the meanings of sentences? \", \"How do meanings relate to the minds of language users, and to the things words refer to? \", and \"What is the connection between what a word means, and the contexts in which it is used?\". The main disciplines engaged in semantics are linguistics, semiotics, and philosophy. Besides its meaning as a field of inquiry, semantics can also refer to theories within this field, like truth-conditional semantics, and to the meaning of particular expressions, like the semantics of the word fairy. As a field of inquiry, semantics has both an internal and an external side. The internal side is interested in the connection between words and the mental phenomena they evoke, like ideas and conceptual representations. The external side examines how words refer to objects in the world and under what conditions a sentence is true. Many related disciplines investigate language and meaning. Semantics contrasts with other subfields of linguistics focused on distinct aspects of language. Phonology studies the different types of sounds used in languages and how sounds are connected to form words while syntax examines the rules that dictate how to arrange words to create sentences.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Semantics",
        "title": "Semantics"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      }
    ]
  },
  {
    "question": "How does wind variability affect grids?",
    "answer": "It challenges balance.",
    "source_id": "Wind power",
    "url": "https://en.wikipedia.org/wiki/Wind_power",
    "category": "multi-hop",
    "generated_answer": "can present substantial challenges to incorporating large amounts of wind power into a grid",
    "response_time": "8.46s",
    "fused_chunks": [
      {
        "chunk_id": "Wind_power_6",
        "text": "Homes and businesses can also be programmed to vary electricity demand, for example by remotely turning up water heater thermostats. Variability\nWind power is variable, and during low wind periods, it...",
        "full_text": "Homes and businesses can also be programmed to vary electricity demand, for example by remotely turning up water heater thermostats. Variability\nWind power is variable, and during low wind periods, it may need to be replaced by other power sources. Transmission networks presently cope with outages of other generation plants and daily changes in electrical demand, but the variability of intermittent power sources such as wind power is more frequent than those of conventional power generation plants which, when scheduled to be operating, may be able to deliver their nameplate capacity around 95% of the time. Electric power generated from wind power can be highly variable at several different timescales: hourly, daily, or seasonally. Annual variation also exists but is not as significant. Because instantaneous electrical generation and consumption must remain in balance to maintain grid stability, this variability can present substantial challenges to incorporating large amounts of wind power into a grid system. Intermittency and the non-dispatchable nature of wind energy production can raise costs for regulation, incremental operating reserve, and (at high penetration levels) could require an increase in the already existing energy demand management, load shedding, storage solutions, or system interconnection with HVDC cables. Fluctuations in load and allowance for the failure of large fossil-fuel generating units require operating reserve capacity, which can be increased to compensate for the variability of wind generation. Utility-scale batteries are often used to balance hourly and shorter timescale variation, but car batteries may gain ground from the mid-2020s. Wind power advocates argue that periods of low wind can be dealt with by simply restarting existing power stations that have been held in readiness, or interlinking with HVDC. The combination of diversifying variable renewables by type and location, forecasting their variation, and integrating them with dispatchable renewables, flexible fueled generators, and demand response can create a power system that has the potential to meet power supply needs reliably. Integrating ever-higher levels of renewables is being successfully demonstrated in the real world. Solar power tends to be complementary to wind.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_2",
        "text": "The Global Wind Atlas provided by the Technical University of Denmark in partnership with the World Bank provides a global assessment of wind power potential. Unlike 'static' wind resource atlases whi...",
        "full_text": "The Global Wind Atlas provided by the Technical University of Denmark in partnership with the World Bank provides a global assessment of wind power potential. Unlike 'static' wind resource atlases which average estimates of wind speed and power density across multiple years, tools such as Renewables.ninja provide time-varying simulations of wind speed and power output from different wind turbine models at an hourly resolution. More detailed, site-specific assessments of wind resource potential can be obtained from specialist commercial providers, and many of the larger wind developers have in-house modeling capabilities. The total amount of economically extractable power available from the wind is considerably more than present human power use from all sources. The strength of wind varies, and an average value for a given location does not alone indicate the amount of energy a wind turbine could produce there. To assess prospective wind power sites, a probability distribution function is often fit to the observed wind speed data. Different locations will have different wind speed distributions. The Weibull model closely mirrors the actual distribution of hourly/ten-minute wind speeds at many locations. The Weibull factor is often close to 2 and therefore a Rayleigh distribution can be used as a less accurate, but simpler model. Wind farms\nA wind farm is a group of wind turbines in the same location. A large wind farm may consist of several hundred individual wind turbines distributed over an extended area. The land between the turbines may be used for agricultural or other purposes. A wind farm may also be located offshore. Almost all large wind turbines have the same design — a horizontal axis wind turbine having an upwind rotor with 3 blades, attached to a nacelle on top of a tall tubular tower. In a wind farm, individual turbines are interconnected with a medium voltage (often 34.5 kV) power collection system and communications network. In general, a distance of 7D (7 times the rotor diameter of the wind turbine) is set between each turbine in a fully developed wind farm. At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system.",
        "rrf_score": 0.03036576949620428,
        "dense_rank": 3,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_5",
        "text": "Wind power's share of worldwide electricity usage in 2021 was almost 7%, up from 3.5% in 2015. There is no generally accepted maximum level of wind penetration. The limit for a particular grid will de...",
        "full_text": "Wind power's share of worldwide electricity usage in 2021 was almost 7%, up from 3.5% in 2015. There is no generally accepted maximum level of wind penetration. The limit for a particular grid will depend on the existing generating plants, pricing mechanisms, capacity for energy storage, demand management, and other factors. An interconnected electric power grid will already include reserve generating and transmission capacity to allow for equipment failures. This reserve capacity can also serve to compensate for the varying power generation produced by wind stations. Studies have indicated that 20% of the total annual electrical energy consumption may be incorporated with minimal difficulty. These studies have been for locations with geographically dispersed wind farms, some degree of dispatchable energy or hydropower with storage capacity, demand management, and interconnected to a large grid area enabling the export of electric power when needed. Electrical utilities continue to study the effects of large-scale penetration of wind generation on system stability. A wind energy penetration figure can be specified for different duration of time but is often quoted annually. To generate almost all electricity from wind annually requires substantial interconnection to other systems, for example some wind power in Scotland is sent to the rest of the British grid. On a monthly, weekly, daily, or hourly basis—or less—wind might supply as much as or more than 100% of current use, with the rest stored, exported or curtailed. The seasonal industry might then take advantage of high wind and low usage times such as at night when wind output can exceed normal demand. Such industry might include the production of silicon, aluminum, steel, or natural gas, and hydrogen, and using future long-term storage to facilitate 100% energy from variable renewable energy. Homes and businesses can also be programmed to vary electricity demand, for example by remotely turning up water heater thermostats. Variability\nWind power is variable, and during low wind periods, it may need to be replaced by other power sources.",
        "rrf_score": 0.030309988518943745,
        "dense_rank": 7,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Wind_power_7",
        "text": "Integrating ever-higher levels of renewables is being successfully demonstrated in the real world. Solar power tends to be complementary to wind. On daily to weekly timescales, high-pressure areas ten...",
        "full_text": "Integrating ever-higher levels of renewables is being successfully demonstrated in the real world. Solar power tends to be complementary to wind. On daily to weekly timescales, high-pressure areas tend to bring clear skies and low surface winds, whereas low-pressure areas tend to be windier and cloudier. On seasonal timescales, solar energy peaks in summer, whereas in many areas wind energy is lower in summer and higher in winter. Thus the seasonal variation of wind and solar power tend to cancel each other somewhat. Wind hybrid power systems are becoming more popular. Predictability\nFor any particular generator, there is an 80% chance that wind output will change less than 10% in an hour and a 40% chance that it will change 10% or more in 5 hours. In summer 2021, wind power in the United Kingdom fell due to the lowest winds in seventy years, In the future, smoothing peaks by producing green hydrogen may help when wind has a larger share of generation. While the output from a single turbine can vary greatly and rapidly as local wind speeds vary, as more turbines are connected over larger and larger areas the average power output becomes less variable and more predictable. Weather forecasting permits the electric-power network to be readied for the predictable variations in production that occur. It is thought that the most reliable low-carbon electricity systems will include a large share of wind power. Energy storage\nTypically, conventional hydroelectricity complements wind power very well. When the wind is blowing strongly, nearby hydroelectric stations can temporarily hold back their water. When the wind drops they can, provided they have the generation capacity, rapidly increase production to compensate. This gives a very even overall power supply and virtually no loss of energy and uses no more water. Alternatively, where a suitable head of water is not available, pumped-storage hydroelectricity or other forms of grid energy storage such as compressed air energy storage and thermal energy storage can store energy developed by high-wind periods and release it when needed.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Wind_power",
        "title": "Wind power"
      },
      {
        "chunk_id": "Renewable_energy_2",
        "text": "Role in addressing climate change\nAs part of the global effort to limit climate change, most countries have committed to net zero greenhouse gas emissions. In practice, this means phasing out fossil f...",
        "full_text": "Role in addressing climate change\nAs part of the global effort to limit climate change, most countries have committed to net zero greenhouse gas emissions. In practice, this means phasing out fossil fuels and replacing them with low-emissions energy sources. This much needed process, coined as \"low-carbon substitutions\" in contrast to other transition processes including energy additions, needs to be accelerated multiple times in order to successfully mitigate climate change. At the 2023 United Nations Climate Change Conference, around three-quarters of the world's countries set a goal of tripling renewable energy capacity by 2030. The European Union aims to generate 40% of its electricity from renewables by the same year. Other benefits\nRenewable energy is more evenly distributed around the world than fossil fuels, which are concentrated in a limited number of countries. It also brings health benefits by reducing air pollution caused by the burning of fossil fuels. The potential worldwide savings in health care costs have been estimated at trillions of dollars annually. Intermittency\nThe two most important forms of renewable energy, solar and wind, are intermittent energy sources: they are not available constantly, resulting in lower capacity factors. In contrast, fossil fuel power plants, nuclear power plants and hydropower are usually able to produce precisely the amount of energy an electricity grid requires at a given time. Solar energy can only be captured during the day, and ideally in cloudless conditions. Wind power generation can vary significantly not only day-to-day, but even month-to-month. This poses a challenge when transitioning away from fossil fuels: energy demand will often be higher or lower than what renewables can provide. In the medium-term, this variability may require keeping some gas-fired power plants or other dispatchable generation on standby until there is enough energy storage, demand response, grid improvement, or base load power from non-intermittent sources. In the long-term, energy storage is an important way of dealing with intermittency. Using diversified renewable energy sources and smart grids can also help flatten supply and demand.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Renewable_energy",
        "title": "Renewable energy"
      }
    ]
  },
  {
    "question": "How do GANs create data?",
    "answer": "Generator and discriminator compete.",
    "source_id": "Machine learning",
    "url": "https://en.wikipedia.org/wiki/Machine_learning",
    "category": "multi-hop",
    "generated_answer": "realistic data synthesis",
    "response_time": "6.85s",
    "fused_chunks": [
      {
        "chunk_id": "Semantics_1",
        "text": "Semantics is relevant to the fields of formal logic, computer science, and psychology. Definition and related fields\nSemantics is the study of meaning in languages. It is a systematic inquiry that exa...",
        "full_text": "Semantics is relevant to the fields of formal logic, computer science, and psychology. Definition and related fields\nSemantics is the study of meaning in languages. It is a systematic inquiry that examines what linguistic meaning is and how it arises. It investigates how expressions are built up from different layers of constituents, like morphemes, words, clauses, sentences, and texts, and how the meanings of the constituents affect one another. Semantics can focus on a specific language, like English, but in its widest sense, it investigates meaning structures relevant to all languages. As a descriptive discipline, it aims to determine how meaning works without prescribing what meaning people should associate with particular expressions. Some of its key questions are \"How do the meanings of words combine to create the meanings of sentences? \", \"How do meanings relate to the minds of language users, and to the things words refer to? \", and \"What is the connection between what a word means, and the contexts in which it is used?\". The main disciplines engaged in semantics are linguistics, semiotics, and philosophy. Besides its meaning as a field of inquiry, semantics can also refer to theories within this field, like truth-conditional semantics, and to the meaning of particular expressions, like the semantics of the word fairy. As a field of inquiry, semantics has both an internal and an external side. The internal side is interested in the connection between words and the mental phenomena they evoke, like ideas and conceptual representations. The external side examines how words refer to objects in the world and under what conditions a sentence is true. Many related disciplines investigate language and meaning. Semantics contrasts with other subfields of linguistics focused on distinct aspects of language. Phonology studies the different types of sounds used in languages and how sounds are connected to form words while syntax examines the rules that dictate how to arrange words to create sentences.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 0,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Semantics",
        "title": "Semantics"
      },
      {
        "chunk_id": "Machine_learning_2",
        "text": "Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and associati...",
        "full_text": "Current Supervised Learning Algorithms have objectives of classification and regression. Current Unsupervised Learning Algorithms have objectives of clustering, dimensionality reduction, and association rule. Current Reinforcement Learning Algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods. In 2014 Ian Goodfellow and others introduced generative adversarial networks (GANs) with realistic data synthesis. By 2016 AlphaGo obtained victory against top human players using reinforcement learning techniques. Relationships to other fields\nArtificial intelligence\nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favour. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines, including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. Machine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature.",
        "rrf_score": 0.01639344262295082,
        "dense_rank": 1,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "title": "Machine learning"
      },
      {
        "chunk_id": "Deep_learning_10",
        "text": "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. ...",
        "full_text": "In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015, and the residual neural network (ResNet) in Dec 2015. ResNet behaves like an open-gated Highway Net. Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015), both of which were based on pretrained image classification neural networks, such as VGG-19. Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014) (based on  Jürgen Schmidhuber's principle of artificial curiosity)\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes. Diffusion models (2015) eclipsed GANs in generative modeling since then, with systems such as DALL·E 2 (2022) and Stable Diffusion (2022). In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone. Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by LSTM. but are more successful in computer vision. Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 2,
        "sparse_rank": 0,
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "title": "Deep learning"
      },
      {
        "chunk_id": "Internet_of_things_18",
        "text": "On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted ...",
        "full_text": "On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role on the Internet and the Web, geo-spatial standards will play a key role on the Internet of things. A solution to \"basket of remotes\"\nMany IoT devices have the potential to take a piece of this market. Jean-Louis Gassée (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note, where he predicts that the most likely problem will be what he calls the \"basket of remotes\" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another. For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, \"where collected data is used to predict and trigger actions on the specific devices\" while making them work together. Social Internet of things\nSocial Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices. SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services, and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering. Social Network for IoT Devices (Not Human)\nIoT defines a device with an identity like a citizen in a community and connect them to the Internet to provide services to its users. SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human. How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system.",
        "rrf_score": 0.016129032258064516,
        "dense_rank": 0,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.015873015873015872,
        "dense_rank": 0,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      }
    ]
  },
  {
    "question": "How does IoT intelligence work?",
    "answer": "Across device, edge, and cloud.",
    "source_id": "Internet of things",
    "url": "https://en.wikipedia.org/wiki/Internet_of_things",
    "category": "multi-hop",
    "generated_answer": "The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT",
    "response_time": "8.41s",
    "fused_chunks": [
      {
        "chunk_id": "Internet_of_things_13",
        "text": "IoT Analytics reported there were 16.6 billion IoT devices connected in 2023. In 2020, the same firm projected there would be 30 billion devices connected by 2025. As of October, 2024, there are aroun...",
        "full_text": "IoT Analytics reported there were 16.6 billion IoT devices connected in 2023. In 2020, the same firm projected there would be 30 billion devices connected by 2025. As of October, 2024, there are around 17 billion. Intelligence\nAmbient intelligence and autonomous control are not part of the original concept of the Internet of things. Ambient intelligence and autonomous control do not necessarily require Internet structures, either. However, there is a shift in research (by companies such as Intel) to integrate the concepts of the IoT and autonomous control, with initial outcomes towards this direction considering objects as the driving force for autonomous IoT. An approach in this context is deep reinforcement learning where most of IoT systems provide a dynamic and interactive environment. Training an agent (i.e., IoT device) to behave smartly in such an environment cannot be addressed by conventional machine learning algorithms such as supervised learning. By reinforcement learning approach, a learning agent can sense the environment's state (e.g., sensing home temperature), perform actions (e.g., turn HVAC on or off) and learn through the maximizing accumulated rewards it receives in long term. IoT intelligence can be offered at three levels: IoT devices, Edge/Fog nodes, and cloud computing. The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT application. For example, an autonomous vehicle's camera needs to make real-time obstacle detection to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including deep learning into IoT devices is an active research area to make smart objects closer to reality. Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions.",
        "rrf_score": 0.032266458495966696,
        "dense_rank": 1,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_19",
        "text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devi...",
        "full_text": "How is SIoT different from IoT? SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users. Function\nIoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, shares information, monitors, navigates and groups with other IoT devices in the same or nearby network realizing SIoT and facilitating useful service compositions in order to help its users proactively in every day's life especially during emergency. Examples\nIoT-based smart home technology monitors health data of patients or aging adults by analyzing their physiological parameters and prompt the nearby health facilities when emergency medical services needed. In case emergency, automatically, ambulance of a nearest available hospital will be called with pickup location provided, ward assigned, patient's health data will be transmitted to the emergency department, and display on the doctor's computer immediately for further action. IoT sensors on the vehicles, road and traffic lights monitor the conditions of the vehicles and drivers and alert when attention needed and also coordinate themselves automatically to ensure autonomous driving is working normally. Unfortunately if an accident happens, IoT camera will inform the nearest hospital and police station for help. Challenges\nInternet of things is multifaceted and complicated. One of the main factors that hindering people from adopting and use Internet of things (IoT) based products and services is its complexity. Installation and setup is a challenge to people, therefore, there is a need for IoT devices to mix match and configure themselves automatically to provide different services at different situation. System security always a concern for any technology, and it is more crucial for SIoT as not only security of oneself need to be considered but also the mutual trust mechanism between collaborative IoT devices from time to time, from place to place. Another critical challenge for SIoT is the accuracy and reliability of the sensors.",
        "rrf_score": 0.031754032258064516,
        "dense_rank": 2,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Big_data_13",
        "text": "It is controversial whether these predictions are currently being used for pricing. Internet of things (IoT)\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mappin...",
        "full_text": "It is controversial whether these predictions are currently being used for pricing. Internet of things (IoT)\nBig data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts. Kevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: \"If we had computers that knew everything there was to know about things—using data they gathered without any help from us—we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.\" Information technology\nEspecially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA). By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them. ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data. Survey science\nCompared to survey-based data collection, big data has low cost per data point, applies analysis techniques via machine learning and data mining, and includes diverse and new data sources, e.g., registers, social media, apps, and other forms digital data. Since 2018, survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality.",
        "rrf_score": 0.030309988518943745,
        "dense_rank": 7,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Big_data",
        "title": "Big data"
      },
      {
        "chunk_id": "Internet_of_things_14",
        "text": "Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning te...",
        "full_text": "Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as regression, support vector machine, and random forest to advanced ones such as convolutional neural networks, LSTM, and variational autoencoder. In the future, the Internet of things may be a non-deterministic and open network in which auto-organized or intelligent entities (web services, SOA components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend, clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments. Architecture\nIoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud. Devices include networked things, such as the sensors and actuators found in IoT equipment, particularly those that use protocols such as Modbus, Bluetooth, Zigbee, or proprietary protocols, to connect to an Edge Gateway. The Edge Gateway layer consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or fog computing. Edge Gateway layer is also required to give a common view of the devices to the upper layers to facilitate in easier management. The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth.",
        "rrf_score": 0.029910714285714284,
        "dense_rank": 4,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      },
      {
        "chunk_id": "Internet_of_things_34",
        "text": "A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics. According to Lonergan, the term IoT was coined before smart ...",
        "full_text": "A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics. According to Lonergan, the term IoT was coined before smart phones, tablets, and devices as we know them today existed, and there is a long list of terms with varying degrees of overlap and technological convergence: Internet of things, Internet of everything (IoE), Internet of goods (supply chain), industrial Internet, pervasive computing, pervasive sensing, ubiquitous computing, cyber-physical systems (CPS), wireless sensor networks (WSN), smart objects, digital twin, cyberobjects or avatars, cooperating objects, machine to machine (M2M), ambient intelligence (AmI), Operational technology (OT), and information technology (IT). Regarding IIoT, an industrial sub-field of IoT, the Industrial Internet Consortium's Vocabulary Task Group has created a \"common and reusable vocabulary of terms\" to ensure \"consistent terminology\" across publications issued by the Industrial Internet Consortium. IoT One has created an IoT Terms Database including a New Term Alert to be notified when a new term is published. As of March 2020, this database aggregates 807 IoT-related terms, while keeping material \"transparent and comprehensive\". Adoption barriers\nLack of interoperability and unclear value propositions\nDespite a shared belief in the potential of the IoT, industry leaders and consumers are facing barriers to adopt IoT technology more widely. Mike Farley argued in Forbes that while IoT solutions appeal to early adopters, they either lack interoperability or a clear use case for end-users. A study by Ericsson regarding the adoption of IoT among Danish companies suggests that many struggle \"to pinpoint exactly where the value of IoT lies for them\". Privacy and security concerns\nAs for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the \"things\" around the user can cooperate to provide better services that fulfill personal preference.",
        "rrf_score": 0.02919863597612958,
        "dense_rank": 8,
        "sparse_rank": 9,
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "title": "Internet of things"
      }
    ]
  },
  {
    "question": "How do qualitative and quantitative differ?",
    "answer": "One uses words, one numbers.",
    "source_id": "Research",
    "url": "https://en.wikipedia.org/wiki/Research",
    "category": "multi-hop",
    "generated_answer": "Both qualitative and quantitative approaches involve a systematic interaction between theory and data",
    "response_time": "7.37s",
    "fused_chunks": [
      {
        "chunk_id": "Sociology_20",
        "text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to est...",
        "full_text": "Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to establish valid and reliable general claims. Sociologists are often divided into camps of support for particular research techniques. These disputes relate to the epistemological debates at the historical core of social theory. While very different in many aspects, both qualitative and quantitative approaches involve a systematic interaction between theory and data. Quantitative methodologies hold the dominant position in sociology, especially in the United States. In the discipline's two most cited journals, quantitative articles have historically outnumbered qualitative ones by a factor of two. (Most articles published in the largest British journal, on the other hand, are qualitative.) Most textbooks on the methodology of social research are written from the quantitative perspective, and the very term \"methodology\" is often used synonymously with \"statistics\". Practically all sociology PhD programmes in the United States require training in statistical methods. The work produced by quantitative researchers is also deemed more 'trustworthy' and 'unbiased' by the general public, though this judgment continues to be challenged by antipositivists. The choice of method often depends largely on what the researcher intends to investigate. For example, a researcher concerned with drawing a statistical generalization across an entire population may administer a survey questionnaire to a representative sample population. By contrast, a researcher who seeks full contextual understanding of an individual's social actions may choose ethnographic participant observation or open-ended interviews. Studies will commonly combine, or 'triangulate', quantitative and qualitative methods as part of a 'multi-strategy' design. For instance, a quantitative study may be performed to obtain statistical patterns on a target sample, and then combined with a qualitative interview to determine the play of agency. Sampling\nQuantitative methods are often used to ask questions about a population that is very large, making a census or a complete enumeration of all the members in that population infeasible. A 'sample' then forms a manageable subset of a population.",
        "rrf_score": 0.032018442622950824,
        "dense_rank": 1,
        "sparse_rank": 4,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Sociology_18",
        "text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any...",
        "full_text": "In the former, the subjective is often equated (though not necessarily) with the individual, and the individual's intentions and interpretations of the objective. The objective is often considered any public or external action or outcome, on up to society writ large. A primary question for social theorists, then, is how knowledge reproduces along the chain of subjective-objective-subjective, that is to say: how is intersubjectivity achieved? While, historically, qualitative methods have attempted to tease out subjective interpretations, quantitative survey methods also attempt to capture individual subjectivities. Qualitative methods take an approach to objective description known as in situ, meaning that descriptions must have appropriate contextual information to understand the information. The latter concern with scientific knowledge results from the fact that a sociologist is part of the very object they seek to explain, as Bourdieu explains:\n\n How can the sociologist effect in practice this radical doubting which is indispensable for bracketing all the presuppositions inherent in the fact that she is a social being, that she is therefore socialised and led to feel \"like a fish in water\" within that social world whose structures she has internalised? How can she prevent the social world itself from carrying out the construction of the object, in a sense, through her, through these unself-conscious operations or operations unaware of themselves of which she is the apparent subject? Structure and agency\nStructure and agency, sometimes referred to as determinism versus voluntarism, form an enduring ontological debate in social theory: \"Do social structures determine an individual's behaviour or does human agency?\" In this context, agency refers to the capacity of individuals to act independently and make free choices, whereas structure relates to factors that limit or affect the choices and actions of individuals (e.g. social class, religion, gender, ethnicity, etc.). Discussions over the primacy of either structure or agency relate to the core of sociological epistemology (i.e. \"what is the social world made of? \", \"what is a cause in the social world, and what is an effect?\").",
        "rrf_score": 0.03200204813108039,
        "dense_rank": 3,
        "sparse_rank": 2,
        "url": "https://en.wikipedia.org/wiki/Sociology",
        "title": "Sociology"
      },
      {
        "chunk_id": "Research_7",
        "text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen und...",
        "full_text": "[i.e., you have already found it] If you don't know what you're searching for, what are you searching for?!\" Research methods\nThe goal of the research process is to produce new knowledge or deepen understanding of a topic or issue. This process takes three main forms (although, as previously discussed, the boundaries between them may be obscure):\n\nExploratory research, which helps to identify and define a problem or question. Constructive research, which tests theories and proposes solutions to a problem or question. Empirical research, which tests the feasibility of a solution using empirical evidence. There are two major types of empirical research design: qualitative research and quantitative research. Researchers choose qualitative or quantitative methods according to the nature of the research topic they want to investigate and the research questions they aim to answer:\nQualitative research\nQualitative research refers to much more subjective non-quantitative, use different methods of collecting data, analyzing data, interpreting data for meanings, definitions, characteristics, symbols metaphors of things. Qualitative research further classified into the following types: Ethnography: This research mainly focus on culture of group of people which includes share attributes, language, practices, structure, value, norms and material things, evaluate human lifestyle. Ethno: people, Grapho: to write, this disciple may include ethnic groups, ethno genesis, composition, resettlement and social welfare characteristics. Phenomenology: It is very powerful strategy for demonstrating methodology to health professions education as well as best suited for exploring challenging problems in health professions educations. In addition, PMP researcher Mandy Sha argued that a project management approach is necessary to control the scope, schedule, and cost related to qualitative research design, participant recruitment, data collection, reporting, as well as stakeholder engagement. Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive).",
        "rrf_score": 0.03177805800756621,
        "dense_rank": 5,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Research_8",
        "text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting num...",
        "full_text": "Quantitative research\nQuantitative research involves systematic empirical investigation of quantitative properties and phenomena and their relationships, by asking a narrow question and collecting numerical data to analyze it utilizing statistical methods. The quantitative research designs are experimental, correlational, and survey (or descriptive). Statistics derived from quantitative research can be used to establish the existence of associative or causal relationships between variables. Quantitative research is linked with the philosophical and theoretical stance of positivism. The quantitative data collection methods rely on random sampling and structured data collection instruments that fit diverse experiences into predetermined response categories. These methods produce results that can be summarized, compared, and generalized to larger populations if the data are collected using proper sampling and data collection strategies. Quantitative research is concerned with testing hypotheses derived from theory or being able to estimate the size of a phenomenon of interest. If the research question is about people, participants may be randomly assigned to different treatments (this is the only way that a quantitative study can be considered a true experiment). If this is not feasible, the researcher may collect data on participant and situational characteristics to statistically control for their influence on the dependent, or outcome, variable. If the intent is to generalize from the research participants to a larger population, the researcher will employ probability sampling to select participants. In either qualitative or quantitative research, the researcher(s) may collect primary or secondary data. Primary data is data collected specifically for the research, such as through interviews or questionnaires. Secondary data is data that already exists, such as census data, which can be re-used for the research. It is good ethical research practice to use secondary data wherever possible. Mixed-method research, i.e. research that includes qualitative and quantitative elements, using both primary and secondary data, is becoming more common. This method has benefits that using one method alone cannot offer. For example, a researcher may choose to conduct a qualitative study and follow it up with a quantitative study to gain additional insights.",
        "rrf_score": 0.0315136476426799,
        "dense_rank": 2,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Research",
        "title": "Research"
      },
      {
        "chunk_id": "Psychology_43",
        "text": "Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understa...",
        "full_text": "Qualitative research involving first-hand observation can help describe events as they occur, with the goal of capturing the richness of everyday behavior and with the hope of discovering and understanding phenomena that might have been missed if only more cursory examinations are made. Qualitative psychological research methods include interviews, first-hand observation, and participant observation. Creswell (2003) identified five main possibilities for qualitative research, including narrative, phenomenology, ethnography, case study, and grounded theory. Qualitative researchers sometimes aim to enrich our understanding of symbols, subjective experiences, or social structures. Sometimes hermeneutic and critical aims can give rise to quantitative research, as in Erich Fromm's application of psychological and sociological theories, in his book Escape from Freedom, to understanding why many ordinary Germans supported Hitler. Just as Jane Goodall studied chimpanzee social and family life by careful observation of chimpanzee behavior in the field, psychologists conduct naturalistic observation of ongoing human social, professional, and family life. Sometimes the participants are aware they are being observed, and other times the participants do not know they are being observed. Strict ethical guidelines must be followed when covert observation is being carried out. Program evaluation\nProgram evaluation involves the systematic collection, analysis, and application of information to answer questions about projects, policies and programs, particularly about their effectiveness. In both the public and private sectors, stakeholders often want to know the extent which the programs they are funding, implementing, voting for, receiving, or objecting to are producing the intended effects. While program evaluation first focuses on effectiveness, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful. Contemporary issues\nMetascience\nMetascience involves the application of scientific methodology to study science itself. The field of metascience has revealed problems in psychological research. Some psychological research has suffered from bias, problematic reproducibility, and misuse of statistics. These findings have led to calls for reform from within and from outside the scientific community.",
        "rrf_score": 0.030303030303030304,
        "dense_rank": 6,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Psychology",
        "title": "Psychology"
      }
    ]
  },
  {
    "question": "How does digital media fuel the digital revolution?",
    "answer": "Via interconnectivity and access.",
    "source_id": "Digital media",
    "url": "https://en.wikipedia.org/wiki/Digital_media",
    "category": "multi-hop",
    "generated_answer": "interconnectivity",
    "response_time": "5.45s",
    "fused_chunks": [
      {
        "chunk_id": "Digital_media_0",
        "text": "In mass communication, digital media is any communication media that operates in conjunction with various encoded machine-readable data formats. Digital content can be created, viewed, distributed, mo...",
        "full_text": "In mass communication, digital media is any communication media that operates in conjunction with various encoded machine-readable data formats. Digital content can be created, viewed, distributed, modified, listened to, and preserved on a digital electronic device, including digital data storage media (in contrast to analog electronic media) and digital broadcasting. Digital is defined as any data represented by a series of digits, and media refers to methods of broadcasting or communicating this information. Together, digital media refers to mediums of digitized information broadcast through a screen and/or a speaker. This also includes text, audio, video, and graphics  are transmitted over the internet for consumption on digital devices. Digital media platforms, such as YouTube, Kick, and Twitch, accounted for viewership rates of 27.9 billion hours in 2020. A contributing factor to its part in what is commonly referred to as the digital revolution can be attributed to the use of interconnectivity. Examples\nExamples of digital media include software, digital images,web pages and websites, social media, digital data and databases, digital audio such as MP3, electronic documents and electronic books. Digital media often contrasts with print media, such as printed books, newspapers and magazines, and other traditional or analog media, such as photographic film, audio tapes or video tapes. Digital media has had a significantly broad and complex impact on society and culture. Combined with the Internet and personal computing, digital media has caused disruptive innovation in publishing, journalism, public relations, entertainment, education, commerce and politics. Digital media has also posed new challenges to copyright and intellectual property laws, fostering an open content movement in which content creators voluntarily give up some or all of their legal rights to their work. The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history, called the Information Age, perhaps leading to a paperless society in which all media are produced and consumed on computers.",
        "rrf_score": 0.03278688524590164,
        "dense_rank": 1,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_5",
        "text": "The transition has created some uncertainty about definitions. Digital media, new media, multimedia, and similar terms all have a relationship to both the engineering innovations and cultural impact o...",
        "full_text": "The transition has created some uncertainty about definitions. Digital media, new media, multimedia, and similar terms all have a relationship to both the engineering innovations and cultural impact of digital media. The blending of digital media with other media, and with cultural and social factors, is sometimes known as new media or \"the new media.\" Similarly, digital media seems to demand a new set of communications skills, called transliteracy, media literacy, or digital literacy. These skills include not only the ability to read and write—traditional literacy—but the ability to navigate the Internet, evaluate sources, and create digital content. The idea that we are moving toward a fully digital, paperless society is accompanied by the fear that we may soon—or currently—be facing a digital dark age, in which older media are no longer accessible on modern devices or using modern methods of scholarship. Digital media has a significant, wide-ranging and complex effect on society and culture. A senior engineer at Motorola named Martin Cooper was the first person to make a phone call on April 3, 1973. He decided the first phone call should be to a rival telecommunications company saying \"I'm speaking via a mobile phone\". Ten years later, Motorola released the Motorola DynaTAC, the first commercially available mobile phone. In the early 1990s Nokia released the Nokia 1011, the first mass-produced mobile phone. The number of smartphone users has increased dramatically, as has the commercial landscape. Android and iOS dominate the smartphone market. A study by Gartner found that in 2016 about 88% of the worldwide smartphones were Android while iOS had a market share of about 12%. About 85% of the mobile market revenue came from mobile games. The impact of the digital revolution can also be assessed by exploring the amount of worldwide mobile smart device users there are. This can be split into 2 categories; smart phone users and smart tablet users. Worldwide there are currently 2.32 billion smartphone users across the world. This figure is to exceed 2.87 billion by 2020.",
        "rrf_score": 0.031746031746031744,
        "dense_rank": 3,
        "sparse_rank": 3,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_7",
        "text": "It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping a...",
        "full_text": "It has imposed painful costs on small movie theaters, some of which did not or will not survive the transition to digital. The effect of digital media on other media industries is similarly sweeping and complex. Between 2000 and 2015, the print newspaper advertising revenue has fallen from $60 billion to a nearly $20 billion. Even one of the most popular days for papers, Sunday, has seen a 9% circulation decrease the lowest since 1945. In journalism, digital media and citizen journalism have led to the loss of thousands of jobs in print media and the bankruptcy of many major newspapers. But the rise of digital journalism has also created thousands of new jobs and specializations. E-books and self-publishing are changing the book industry, and digital textbooks and other media-inclusive curricula are changing primary and secondary education. In academia, digital media has led to a new form of scholarship, also called digital scholarship, making open access and open science possible thanks to the low cost of distribution. New fields of study have grown, such as digital humanities and digital history. It has changed the way libraries are used and their role in society. Every major media, communications and academic endeavor is facing a period of transition and uncertainty related to digital media. Often time the magazine or publisher have a Digital edition which can be referred to an electronic formatted version identical to the print version. There is a huge benefit to the publisher and cost, as half of traditional publishers' costs come from production, including raw materials, technical processing, and distribution. Since 2004, there has been a decrease in newspaper industry employment, with only about 40,000 people working in the workforce currently. Alliance of Audited Media & Publishers information during the 2008 recession, over 10% of print sales are diminished for certain magazines, with a hardship coming from only 75% of the sales advertisements as before. However, in 2018, major newspapers advertising revenue was 35% from digital ads. In contrast, mobile versions of newspapers and magazines came in second with a huge growth of 135%.",
        "rrf_score": 0.031054405392392875,
        "dense_rank": 2,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_13",
        "text": "According to the Audit Bureau of Circulations report from March 2011, the definition of this medium is when a digital magazine involves the distribution of magazine content by electronic means; it may...",
        "full_text": "According to the Audit Bureau of Circulations report from March 2011, the definition of this medium is when a digital magazine involves the distribution of magazine content by electronic means; it may be a replica. This definition can be considered outdated now that PDF replicas of print magazines are no longer common practice. These days digital magazines refer to magazines specifically created to be interactive digital platforms such as the internet, mobile phones, private networks, iPad, or other devices. The barriers to digital magazine distribution are thus decreasing. However, these platforms are also broadening the scope of where digital magazines can be published; smartphones are an example. Thanks to the improvements in tablets and other personal electronic devices, digital magazines have become much more readable and enticing through the use of graphic art. The evolution of online magazines began to focus on becoming more of a social media and entertainment platform. Online piracy has become one of the larger issues that have occurred concerning digital media copyright. The piracy of digital media, such as film and television, directly impacts the copyright party (the owner of the copyright). This action can impact the \"health\" of the digital media industry. Piracy directly breaks the laws and morals of copyright. Along with piracy, digital media has contributed to the ability to spread false information or fake news. Due to the widespread use of digital media, fake news can receive more notoriety. This notoriety enhances the negative effects fake news creates. As a result, people's health and well-being can directly be affected. See also\nDigital media use and mental health\nElectronic media\nMedia psychology\nVirtual artifact\nDigital preservation\nDigital continuity\nContent creation\nDigital rhetoric\n\nReferences\nFurther reading\nRamón Reichert, Annika Richterich, Pablo Abend, Mathias Fuchs, Karin Wenz (eds. ), Digital Culture & Society. Schiffrin, Anya. Media Capture: How Money, Digital Platforms, and Governments Control the News. Edited by Anya Schiffrin. New York, New York State: Columbia University Press, 2021.",
        "rrf_score": 0.031009615384615385,
        "dense_rank": 4,
        "sparse_rank": 5,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      },
      {
        "chunk_id": "Digital_media_1",
        "text": "The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history, called the Information Age, perhaps leading to a paperless society in whic...",
        "full_text": "The ubiquity of digital media and its effects on society suggest that we are at the start of a new era in industrial history, called the Information Age, perhaps leading to a paperless society in which all media are produced and consumed on computers. However, challenges to a digital transition remain, including outdated copyright laws, censorship, the digital divide, and the spectre of a digital dark age, in which older media becomes inaccessible to new or upgraded information systems. Digital media has a significant, wide-ranging and complex impact on society and culture. Business model\nDigital media platforms like YouTube operate through a triple-product business model in which they provide information and entertainment (infotainment) to the public, often at no cost, while also capturing their attention and collecting user data to sell to advertisers. This business model aims to maximize consumer engagement on the platform. Paid Media\nPaid media refers to promotional channels that marketers pay to use, including traditional media (e.g., television, radio, print, and outdoor advertising) and online and digital media (e.g., paid search ads, web and social media display ads, mobile ads, and email marketing). This model compels businesses to develop sponsored media and then pay social media platforms like Instagram to show it to customers in their newsfeeds. These customers become exposed to paid media, sometimes referred to as promoted or sponsored posts. Owned Media\nOwned media refers to digital assets and channels that a company or individual controls and manages. This includes websites, social media profiles (e.g., Facebook), blogs, and any other content platforms owned and operated by the entity. An entity is the owner or controller of the channel, such as a business or an individual managing their online presence. Earned Media\nEarned media refers to public relations channels such as television, newspapers, blogs, or video sites that do not require direct payment or control by marketers but are included because viewers, readers, or users are interested in them.",
        "rrf_score": 0.030090497737556562,
        "dense_rank": 5,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Digital_media",
        "title": "Digital media"
      }
    ]
  },
  {
    "question": "How does computer vision help robotics?",
    "answer": "Provides environment perception.",
    "source_id": "Computer vision",
    "url": "https://en.wikipedia.org/wiki/Computer_vision",
    "category": "multi-hop",
    "generated_answer": "fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms",
    "response_time": "8.91s",
    "fused_chunks": [
      {
        "chunk_id": "Computer_vision_5",
        "text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. Thi...",
        "full_text": "This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, e.g., how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image. Machine vision is the process of applying a range of technologies and methods to provide imaging-based automatic inspection, process control, and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, e.g., vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasized by means of efficient implementations in hardware and software. It also implies that external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms. There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with the processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications. Progress in convolutional neural networks (CNNs) has improved the accurate detection of disease in medical images, particularly in cardiology, pathology, dermatology, and radiology. Finally, pattern recognition is a field that uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data. Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision. Applications\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap.",
        "rrf_score": 0.030679156908665108,
        "dense_rank": 1,
        "sparse_rank": 10,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_16",
        "text": "In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system ...",
        "full_text": "In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors. Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower). A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images. While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized. Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective. As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role. See also\nLists\nOutline of computer vision\nList of emerging technologies\nOutline of artificial intelligence\n\nReferences\nFurther reading\nJames E. Dobson (2023). The Birth of Computer Vision. University of Minnesota Press. ISBN 978-1-5179-1421-9. David Marr (1982). Vision. W. H. Freeman and Company. ISBN 978-0-7167-1284-8. Azriel Rosenfeld; Avinash Kak (1982). Digital Picture Processing. Academic Press. ISBN 978-0-12-597301-4. Barghout, Lauren; Lawrence W. Lee (2003). Perceptual information processing system.",
        "rrf_score": 0.030679156908665108,
        "dense_rank": 10,
        "sparse_rank": 1,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_0",
        "text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical ...",
        "full_text": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions. \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory. The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems. Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. Definition\nComputer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.",
        "rrf_score": 0.03057889822595705,
        "dense_rank": 3,
        "sparse_rank": 8,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_6",
        "text": "The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refe...",
        "full_text": "The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\n\nAutomatic inspection, e.g., in manufacturing applications;\nAssisting humans in identification tasks, e.g., a species identification system;\nControlling processes, e.g., an industrial robot;\nDetecting events, e.g., for visual surveillance or people counting, e.g., in the restaurant industry;\nInteraction, e.g., as the input to a device for computer-human interaction;\nMediaPipe, an open-source framework from Google for AI edge device computing, e.g., face detection, image classification, object detection;\nmonitoring agricultural crops, e.g. an open-source vision transformers model  has been developed to help farmers automatically detect strawberry diseases with 98.4% accuracy. Modeling objects or environments, e.g., medical image analysis or topographical modeling;\nNavigation, e.g., by an autonomous vehicle or mobile robot;\nOrganizing information, e.g., for indexing databases of images and image sequences. Tracking surfaces or planes in 3D coordinates for allowing Augmented Reality experiences. Analyzing the condition of facilities in industry or construction. Automatic real-time lip-reading for devices and apps to assist people with disabilities. For 2024, the leading areas of computer vision were industry (market size US$5.22 billion), medicine (market size US$2.6 billion), military (market size US$996.2 million). Medicine\nOne of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example.",
        "rrf_score": 0.03007688828584351,
        "dense_rank": 6,
        "sparse_rank": 7,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      },
      {
        "chunk_id": "Computer_vision_4",
        "text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how...",
        "full_text": "For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry. Distinctions\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input and output are both images, whereas in computer vision, the input is an image or video, and the output could be an enhanced image, an analysis of the image's content, or even a system's behavior based on that analysis. Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality. The following characterizations appear relevant but should not be taken as universally accepted:\n\nImage processing and image analysis tend to focus on 2D images, how to transform one image to another, e.g., by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither requires assumptions nor produces interpretations about the image content. Computer vision includes 3D analysis from 2D images.",
        "rrf_score": 0.02964426877470356,
        "dense_rank": 9,
        "sparse_rank": 6,
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "title": "Computer vision"
      }
    ]
  }
]